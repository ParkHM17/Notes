<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<title>笔杆检测报告单（全文标明引文）</title>
	<meta name="keywords" content="" />
	<meta name="description" content="" />
          <meta content="0" http-equiv="Expires"/>
          <meta content="no-cache" http-equiv="Pragma"/>
          <meta content="no-cache" http-equiv="Cache-Control"/>
          <meta content="no-cache" http-equiv="Cache"/>
	<link href="css/report.css?v20180524" type="text/css" rel="stylesheet" />
	<script src="js/jquery.tools.pack.js" type="text/javascript"></script>
	<script type="text/javascript">
    function $ShowMore(n) {
        if ($("#simMore_" + n + " a").text() == '收起相似文献') {//收起
            $("#reportTable_" + n + " .trLike").hide();
            for (var i = 0; i < 5; i++) {
                $("#reportTable_" + n + " .trLike:eq("+i+")").show();
            }
            $("#simMore_" + n + " a").html('查看更多相似文献<span class="icons inlineBlock simDown"></span>');
        } else {
            $("#reportTable_" + n + " .trLike").show();
            $("#simMore_" + n + " a").html('收起相似文献<span class="icons inlineBlock simUp"></span>');
        }
}</script>
<style>
    em.similar{color:Red; font-style:normal;}
</style>
</head>
<body>
<div class="report_bg2">
  <div class="report_bg3">
    <div class="report_top">
      
      <h1>笔杆检测报告单<span>（全文标明引文）</span></h1>
    </div>
    <div class="report_Wrap">
      <div class="report_tab" id="report_tab">
       <ul>
                                            <li class="rep_curr"><div><a href="全文标明引文" class="green">全文标明引文</a></div></li>
                                            <li><div><a href="全文对照.html" class="green">全文对照</a></div></li>
        </ul>
        <div class="report_priSav">
          <a href="javascript:window.print();" class="print inlineBlock"><span class="icons inlineBlock"></span>打印</a>
          <a target="_blank" href="https://www.bigan.net/report/explain.html" class="report_explain inlineBlock"><span class="icons inlineBlock"></span>检测说明</a>
        </div>
      </div>
      <div class="report_content">
        <div class="report_main">
          <a id="toTop" title="回到顶部"></a><!-- 回到顶部 -->
          <script>
              $(document).ready(function () {
                  $("#toTop").hide();
                  //检测屏幕高度
                  var height = $(window).height();
                  //scroll() 方法为滚动事件
                  $(window).scroll(function () {
                      if ($(window).scrollTop() > height) {
                          $("#toTop").fadeIn(500);
                      } else {
                          $("#toTop").fadeOut(500);
                      }
                  });
                  $("#toTop").click(function () {
                      $('body,html').animate({ scrollTop: 0 }, 100);
                      return false;
                  });
              });
          </script>
           <div class="report_Mtop"></div>
          <div class="report_Mbot">
            <div class="report_result">
              <div class="report_info">
                <p><span>标题：</span>查重版.pdf</p>
                <p><span>作者：</span>黄子恒</p>
                <p><span>报告编号：</span>BG202303191404576795</p>
                <p><span>提交时间：</span>2023-03-19 14:05:23</p>
              </div>
              <div class="report_ratio">
                <ul>
                  <li style="display:none;"><span class="icons ratioIcon ratio1"></span>总复制比：<span class="green">12.4%</span></li>
                  <li class="inlineBlock"><span class="icons ratioIcon ratio2"></span>去除引用文献复制比：<span class="green">12.2%</span></li>
                  <li class="inlineBlock"><span class="icons ratioIcon ratio3"></span>去除本人已发表文献复制比：<span class="green">12.4%</span></li>
                  <li class="inlineBlock"><span class="icons ratioIcon ratio4"></span>单篇最大文字复制比：<span class="green">0.7%</span></li>
                </ul>
              </div>
               <div class="clear"></div>
              <div class="seal">
                <div class="SealArea">
                  <div class="SealBg"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH0AAAB9CAIAAAAA4vtyAABpGUlEQVR42sx9B3xkVdn+uVMz6clMerK77LLLUpcmIAgiCCJFigU+BUUUBUFUQBAUBMUG+v+soIKgUqUsbEmfzEymZVJ3s733ljIzt/d7z/997ywhbHbXRT+/3ze/axyyyWTmOe953uc55z3vJdJ/+CHKzldRhK+KosATnufhuazC/zhBzElSVpE5+A4vqIJgcaas53hBUSeppeUVOiEqkqoIogQ/JPC8KIj4EpIqa4asCzl+n6Rypi1ppiwqmqGKtsRpOVnF1xSlvKxwiga/K8Kf1lVNlRWqaVTRNVEVNI03DUFTeFXMa5wuinBpEvw5GR7w/uDXOEme+YnkaY/CR5t6HDss5H8H98LbLbwzeAIocBx8Rg3hU2RdFhQxL4l5RYVB4U1B5hTlgKpwsi6qRk7XeVXlFEnSVMMy4aHrumlbOrVFS5+gFqvqumrZNlVtk9VkAM+yDFXX4K84Lw7jLcmmmrPk3UpOy42rQh4gprJmqyYMqsTxMNKqKBQuWZQKuMMFw/+fAP1/A3dBgCjl4V2q6nufgWc5+EiipAm8Ahd+ePhJiRUV1lYR1UlVPqBKom0ZlGq2Tk0NUKVwSZw1MWYd2EsnD9D8BOUn7fEJKgiU47V94/r+CYuXqW6bpi0YNmdYvGlxhgmhLcNkUFQYUVXIyoYomApOHR7nnajrsm7IkoCXeBBcmF4F6A9Gz7vITgd9+vcP/tb/HdzfCwoZYx8HQFacuTwuK3knxkxeo5xO4atoUnjAZxapblBN2bqFDcf4pcv0xUv3v/7qnuef3/TzJ9c+8IP19z689t4HV9334KrvPbz/Oz9U//LSjt88Hb3nwQ3PvED376OWwu7dDYEv2YZCqUyRhTTRgBGgrJEV8jBvVNtmTTNvGJxmsBpMBRUmx3TgCu92arLOxH16vE99//8Q7hBBSOuFz6DIEP4CzGtZycljrJrlFAFiSpcMM6/aYyzdNzmxcf2eeELds5Xu3bL/x0+lzrwkMW9RunFBdPaJ0Yb54ZKmHk9txB1qd1Ut81S1VzYuLa0dOeuC7uNOfs5XHfvktcJrL068+lLynu9kbvvm5p/+il2+XN+8wRaylq0B3JMw82DWSKYuGKJiChYVVU3OsmYOUoKCMQ5D7rzPKbink0mBIQ/BfeYk+L/B77xQmLa8LMElAP6iZGn6ODUEWzd4wdi8Te6O7v/Dn9bf+8DIrV9JfuLGntMvXfvlb4z98PHV5368g5S2uwJdxNdJinqZ0hQpHSClQ6Ssn5StKGnY2nJSet6CZMPsDlf5MlLSN2fh5osuGzjp7NdJWZQpi5c2pOecMnjBZau+9LWNP/vF2NK39dFBc2wCyB3py7JVSMRAQHmW5pH3eFnh3w1zB0bRSUni9KCejvsU0P8XcYfQBuiBTDkYAlmCxEhtClRLVY3u2iMt79rzgx+PXnFt/LgTOgLlrcTb5q1YSrydZQ2JeSenSpvjpCLtDg4xoc6iylhJKF4civtCcXcw5gn2l81a23jy3nMuXdl0Uq+/Ju6ryRTV9bmr4g7oPcQdJ0UJpriXKY+4qrp9dcn6hWtP++iGh3649+UXtc2rKTdJVZmahqHp1LJR3qgqK4O0EjE4QB6hkML8NJ3BZ0Isv//xfwh3Ls9CvKsmCD/NAhHCC5vWrNv8+z8O3n1v21kXvFMcCpOiFAmkXEUxxhcnZNAT6HUFOkgg7A6GvXVRT8Oge3aahPpddRlXQ4JpiLuaokxTJ2mAK1qxoK/m5IHKE/p8Tf2+upQ3lHCXD/mCXaHazKzjhhvnpEtqAfoYKYuQcrjeIiVvV7fEr/z02t/9fmx0hZDNshN5OS8LpsHpGugoVhQctcoj9LI4hfuR8C38p/Lu4z+F+8xRLUxDXprUJB5FrwzaAAUyx+ZYhYXpzImWzJoUJ/U4u3p471/+OvCNuwc/c1P4xLOixU39RU2j9QtH556RCM3v9DWlGIjZhn5vU8rTkHTXwwVPUh4AdFbC3dzLNMIFuMNz+ArPl7pKl3vK271VYV8w5q9JBOoS/vqEpzZWc8LogvNWLzg/XjG/h9T3MY2DnqY0U9PpJj2MK+KBcS1KLTp94y1fWP9fn994953cimFIAxo18oaSFxXQWsDnADokoby0nzUm8lSEDKzwIFSVnMLDP01pmOkDc4yykvxPKRaeZyE1Kaqe51heyGm6BJYkp+poZyiQi7xnfd+q3z2ZvOTK2JkXjz3xJGV3bf7lz1tnn9RFytc1n7Du+NMzZXMHXLMTpAagT7vq4WuS1OJXB/2Yuw6uhKsOvpNh6geZhiHSMEzqk6QxRZrgSjPNfa5Zfd45cKU9s2OBplRwXjq4IBZoieNv1aRcwbi7OuWrH/TVD7lr06Siy1W6LFC61BtYEggsP+f8LT97Qt200jBFHchHNkHgjkHog3syVdC4di5PORG82xjPQVYopNkC0NO56H8Vd+QTzZ4QQHQr++W8bghUE23bFik1qDW5Y9VE22srvvrF3rkLWgmzpCwkPfZzW9qz64/PRBeem3DVrA7O7atoCpOqfqYpTRrgSjENAHoB98LV7aqGq8cVjLqB32vgt+BKumv7fDgb4Al8M+ap6fXU9/oaEkVNkBUSnhCkAfj5pDuU8tQA6BFPRdrTDGQFrAXfjLsqokxJj6+kh/jafIHX3P7IOedn//QMndipUJ2lpqyYYzrNSZbIqSLLSVzOUERN07Lce/lWcB5Tauc/y+/yjIeqmFlFBVEsiLKlqVSXqaUrkwfEWHLdzXcOtZzW462KMH4g8TZvSeKkRdGrr42d9uFk8XHDTHO/q6GHlEfdVWl/rYN73cHLVV+4APcRgtcgqc0wtX2u2rSrtoB1glRC7oWvKVKdZoIpEoILfmCQqep3hxKkKkqq4IcH/PUpXw28h4ynGuDudVV1FlXHA3WZ0paessa0vyFDynpdxR2EedtbFL/2ymz3UrpnN9u3lpM1UTB4xcqZJtCokc+bnKSYVHz/Y0rt/Gf5XZnxECQe3hayyqRELXA/ujTSN3zffUvrT9989nVbFnw0VlzX4a9IBkLD7qo+4ku7ylOkvI9U9hdIwFM3EGiMk8qpMAew+tzA9fgErowr2MdUp0gVXq5qCOekB6+UtznpaYKvff45Sd/sXndzlGmEK0OqMhDmTE2chGAIB+BnPHUQ+/B9GKR0cUO4tC7MVPeSyiipXkkaVpKaXn8oWgqj6I+RksSCRYPX3DR849d3PP93ddtW0P75nACix5RVMNtgOBRFQwuIhlYpXBDxovgf0zMzEVedB1h8SeFVCwy9ZU+Oj//jrfgnb/gbKV3+8evo6KDy9xcGmk7pIRUd3vo+X91qpjwDcttVGiclYaa4y1PZU1SXKGqJk8aEKwhXnKmeetJLquCKeiuBJSLuchAnCVKeYiqAoDOkAkR9lBRHSDFox5irPO6pTPqq00WhtKsStE3SCyPa0O9uhLwKw9kLc4XMjrma08GFg/PPAL3fW1kf85YPkOK0rynMhMCRZfz1fZ6aTlK6lAks9Za82TJv1b33i0MDqsAKoN0VXHJA9fAu4lPo/8/gLh/hoRzhAaCbtmVbGr92xaYf/iBy/Cl/h6g59xJ+YidV9q1+9CFQ0BkS6i1q7PLWh70NPe4qAAU+JHyNeGu6PbU9rhoIW2BhIIEYUwlXlFRESGkBzQ5S0UkquzzBbl9NuLg+Ut4UqWiOVrZ0l5SEi4t7fEURjz/u6NEMCfSDcveURD0lMFSg6/u9DcA8SYKJIUmaou6mVONJa8+/ZOzzN2+7+pre2lmRQDW8OCTtNGnqJY0xb2O8qLbXVxHzBdqYon94ShI3fGayt0uUshD4kqZnVdFRO5Ijag5CX8D9GIX8B8b9SA8QLbpN96eH+2/8Uo8rECFk2RmLaCxCFbrq3keXljZ2kTKIxBhTFWeaM55TAOsIqekjLUNk3iCZM8TMcjikHMIWsA4zB68eV0ncX5kuCY1cePXoZTes/fQtG2+7Y9O992394SPbnvrJ9l//YuO9d2361p2bvvrVtZ+5afjiK5InndMWPP4tXz2IxU7G3cH4ul3FEPj9vlr4Cp6rj5QmPRW9wea+eafkb7sjd/PXI745va7j4qQaxGs/09JHgOub+5j6DKkeJcGhQEWPu2hpSXXq6k+PLW3lBHbCUEHVANwF3J0YVwrDwPPiB8B9KooPG9oFGpn6AUG2RNuSVE5n85Q3aR4Eo8hTmU5OZJ//2/CZ54HxSRDS5i/vv/4Ldrp/z/cffKki+BbxJSvqgTcgSXaRmp6ihrhnVsqNgjoNMsZdDbQA6RH4vZtAkFb1Fjf3zT599WWf3vnAo5N//Xu+o1Vct23flq10b07hJPD4mmpBFgG7z1MqWqY+aQmqsWty4kCsf/ydzL7n3lh/2/19H72qM9QEgd+HlO2HrDvkbe5hKlOBenCwYVflcGjuaP2CvqLGBAmCy4Vs0esFLoJM3ojZm0BGqRzyVCT9JVGXD6Zd31mXHnjxNVuTJg0ZzKwhGpB1WVOVZE4U8qaqmZPKMVpWMh3lY+ET3bDAEMkaUrkoqKDMqa6wG9Zs+e73uk879w1f2XJ/SdgdiBfXpk74EHzyTGX9clICyqHfA5+kAnQFpMEB0tJFSnr91cmiWhiMGAn1+JoTzacPn335us/dvv3uhw786ve5xW/zQ4PK3p2GzBuWiksL1BJMhRoGNWwOPrdpUBUUqwD8Nm5Tkeq6JoHvt6khKbuA36xN64xkdN+Lf4l/5ctvNy9YTsqAuwZIdZJUdpLiDk9FTyDUA1oTsq53DrxDUJaANeiiOBNC8+Wv7yVBsLhpd9VwoAaEELBczxkXbf7dM3TygDQxrvBALKbEylRSVE0YF3KmYhyJDw7FfXqkH5ItZ44EflPmLY6zFPBs8oSuQBblM/2Zr397aXkZOMAuoGNv7aC/KcOEepiKDnfpauIfZMpA0g2QIHwTUhYaH1dTxuNNM0U9pLiN1Ha2nJm+5oubf/5rvr1T273XzuVgLA0KGFMIa0WnoEs1Bbc+IIEplkZNDSSFpqJgnaBU5iTLVkVLh4jbu2H79nBM3rMD3DGuvOiqJnJ0fMwaHNj5q6eiN36ms2VhuLqlxw3Q+7shRRc39vlnJUkDCP8IqUoyIEZxYGASJCvmdAZa+t3NaQKsVd3pKYuSojAJ9C44e/uDj9KNGwxZyBq2lNdoHkiDHTc4XHw+Aksfivt0fA+L+9QLFb6pCQJVTTOP67sqNYSVQyu/dPviotpuQtL+8hW++hUENFxjH0ECifrK+wMh8CYpUgouETwkJM8k8Li3MkZcCaYU/OTKq27e/dzfuS0bgTZh7oDbsqhtmLhYKGmmqBq8pOd5RRpnVV4E8wIe3dBxxdziJF6QOM2gkkbxndrsls26rtuyIJkikIBuWyrP51ZuYFdtgkCB/wKhJXZ1rLn/e+GTzu3x1YOgTDJVfa7QgLs+7RjajLcW5iUwUtpbHy1qXOKt6SVAiS0RT0PEC1wP0qC0nQTC1U1r77mXH8jkNCkPcSHrpsiLmgTv+Vj96pFwn5oH03FHnlFsjlVFQaOgXrZuGL3zGx3FwSTxrXSH0r4aoEiwKuAqk/4mSFMxUhsrbljmLo94qsHmRH1oJpPodAJtNccPXPPZ7X94mh0ZsngWjK1ODZEqAK7i7KGCaIMQlmzcAs1BBpEMwBtkA4wGLmDpmqwbkm0ZpmIpBjzd8uyfQL3sWbxYkcHmW6qqywawoqQKeYgVy6KmDlPDVilQkqh096y45faOugVA9P2kEkxTxlsz5K0fAptGauMudLzJooaUrzYO/+ltiPkb4v5mXBRiwFtUJElRq7d65VfuyA8NKKqAU48X1KwAqeYY9/yOyDNH4ilWoTkeKJbSsb1bH36ku6qxh7hHApUZ92yw4L2uEMA65K5N+Bq7PY0pMivqq+0hoQHfLBDRIMPDpCIaOn7VJVdv+9sr/MCgLeQNhFvjJBHeuiUqhd01EaWxpIJPl1VbUCiLa/eWopm6xavO9psiUpGXbUHVLHbv9sFb7wBnEG08btfbiy3Lwg8AHKSJwPgwMMA2Ku7QClmL36uaE5YFJEbH9mVffiVzzQ1tlc1A4pDP4+5gwl2P2d4zCzQlJPk+VxBGAiZoD6mMuUBizu5zzVlZ0pLxgOQvebu8duCOO8V1I3kq7tdlSdS0vHKIuzwizxwprx7pkTVM3Iwbn9z+37+NzD21hxT1e2FW1qYCcyOeppi7bgVTPwqe013fU9QUJ/UJT3CQhIZd9YB4q68mdvr5Gx/9MR1dhYRAKahPUZdF1dmQAqw1E3C1uYMXPIcUCeirGL3OKpBu6qKcNwWa3x8551N8ulMcSvbOPaeb1PV+6gt0LC/v2TBww83SphWGJIHQUFFYK7KuQRJWbJkFxlJVVtXzhsEC9LZOd2zf9dRvFp95YVtJ6C2mbAlTHg80jARanMCHORqKedDBgakecjWnSGMaJKZ/FriQfn91K/G8WRna+Ogj+sQegVo4BdUjRvAH0JGHfXBUt2xt7yuLe06/uI2U4R4QLhzOHmaaw/7GsK+5z9UCigV0AhDOMFhwpmbIVR4m3nd8geHLrxGXLjOUnIzLZhTpm8eVblApnGXkNENQbQ3IG2DStUMeNmgHS4UnsqlaOjfyzQeWk9p2X+0yT0nytA+vfPz+Nm/DztffidTPBf7d/PzLkg5ow6AJCmRbQ4c0bKkmfMuWcjKX42Uta9JxkwL3UFDdq0azN96aWXhmq7cy7KrA5QdXNUiAfk/9YEk9sA2EUS+pAckQCdRHmUbwHFF/1WCgOkZ8vSefM/7qYsrKCifIEneksoPDxPvMYYEnOK/hh0Ermaog4ozXFJ2DALWF/a+92HvOh7uYADKdt67P3TRImsOkptNdCW6zz12Hb5RU9pMgDMCQB7VzZ9OJo999WFy3xqQqZ4p5vfA+wO0Jhe20qQudr4JxqmnK9EswDV1ziluoYoJj2LY2XF6/3NOw6pavSZO7t/zyqeWkuhs036WfZPtWcJAsZA0+hW5QyMwagA95wnJ2YTQVppeAJTuQCjnwOsDOmg0iVJ3sHdz4le93NJwMCTZCivsXntrtq+gluDKx0l8Hij7packwIH5qwr5gyh9cE2jpIf5XfEUjZ1+8N7WGVwF2Vs2zkKw0XteQIwVOzqu4f6Udnt9nxjskMSmfh+CbkHngUF3CihRqa2x3e+pTnwkXhfrcFWFPZRj0ePHsblIz4mkEHsQtIfcsMOJ9RfWD/pp2kMmkNHrK2Vuf+KmyYbUJhAsiW3HqkN4zxgexdqoMREC8cB2CO+5LQ/o08vA+d//6rx0Voe7QfH5sF6+bE6tXD172qVZS3uYLrvnNU5APQMcAvqqliJpggowxLZBfVFbgbyPXY07WEH2JB//JSTKv6ICNrarG+vX7n3k6ddbFS+tOyD/00OZPXtlTVAVyE+TZINMEWSrjaex3NaRIKEogu4bSxU2t3qIYE+j7r9v4tStAR8I4s7K8T+LfDXOIEUGR5KPh/r6HYbIsC3JhLA/JAt8wiDx5ZEX6uusWF1WnSdGqovowUw3ckiid08UA4jUDTHCFtwVyaS9pAjUGarezqLz9/Cu2Pf0nI7sHRCe8pg1yDhwv/74FCUehItaHjfTCZWG1jZDftib6sY8DEY9ccu2aW77c5avY+MiPYrWzw6dfsLezIxKc107KDwxlzIk9OY2jMJY6J6l5qmMqBmaHkYOXUnUFFQgmFVw6BzHKCiqwPUgdGByZ3ZVf3rb9uRfHXn/5nRMXxUhJxl0DiSrsqksGGhMe3G5cTVDUD7tqk67mDo8/TZhooGjrg/cZ2SwQhMxz+0H9Crg6awBdyIoosUfUM4fizkngjDTLhhcCNDTw5Ad2Ddx5/xv+onaPf6CocgQYhmlMulocVx3qIBV9nlAfUx0nVRD7Q6RyOSkavuCine8sNybHJGpNKBKoT13QBUGCYDzUFhx8KEe6RFvYG4m2gc/ylmz5xW+AQHZ1x1pJ8XISWHnXfVSVNj35VKyqvnfu2Zue/B1kb2XygIVhjQt2soVVU1Q2wEyByHGSraOb5EKZBvyfzurauMxPmjxHeWqaYA72d3aHz76k3RXoh0TlbgBGzbghk1WBw4p7m6KBOXFX2QBp6POH2lwlrYT0LTiZe+lN4BaeywKhafAxwVPrFlg3TuYPr2emZ9eDOp2TOV2BGLEFzbBMMDX7/vzcm40ndBEmU1KR8JXCRAO9lSLNMN0GXaF40eyVpY0R4uvxlXSTQIerfPDyq7S334RJAsIcuA+GHwu2NAVY1QDem+aBp6VP5ciXAaQ3eMmV4DNXfed7O577S7pqzjIX2MiaoQe+O3zZte2kctU3v7vr2Rci1XPamKKx1k5RV2UQjLoBOdk0NKwaUcFN4XxSNIfNFCzvwfUsQQU7ltOEnJSDt4dKH5TWgf3jb72WufBiyFt9pAxMEzgsIBwAvd1T1+c7LubygHaILzwzfcMtbWef8ypxx66+Vl+/mhMngFuAUyHkdVPDwj9FO+J65CECCGYHzkFRgHlPQe2l07FzLl3ClA55KkbdFSlS3O2t7vThvn7K0xD1Vg+4G0eY6u6yiuGPX7Rs0anLP3LR+OuvUSqBdIHI0jnRFPCDsrhdzOlsbvqiG+ANtIue3uHfw+KuqwYkJ2WSDx93BngWddto5LTLxIk98TMuaifurqraXc//YcM3H3zF3xA7fdESVxUbjfOKABEjC+P5wUHIppBUwE6ByClQDf5FGAAZt+R5TtbyOZxVWLQJMBkKJAdqyDSvLn6j9+LL3nGVtBFf0hvKMPUpEMdukMU1keJAmClOf/RSmh4Q44noBZc/y5SsvfMBKkzkJQFyBrwSJBjAXeNn4D6zAq2ACAw7rn+BmaWWPT6x+d7HlpJqCGdQ4kOkOuWtDhfXd3iRZzL+5m5vVZpUxEig78ILjba39y9fMrG8k+byE9TkTVwilTgAQYQpDk94noUPpb77KGA9Bff059MvyeCoKbDW5IF0EtT6stoWumMrGNXccKbvpMv2vb08etzp7RUVO//7D1u+/0uI0MmNqyUrN7HkrfCFH0+de43pZBdN57VC3tadkdakQshDdKHtlCRIjJwuQcpVDBuw06gMrm7Ha6+GP3rZUlI64KoYRWNY3etvRsvir1pNylZUzt7548ctMa/sW7f0sqsisy9QOsO8MMHilNKlwq4IKx4T7lhDqsDk4yVDEag22ZvpWPixKKkcAQ1OZkXctcsC1a2+IPDMKHNcP6mDQOj2epa6izJfvo3yeXSzMk7VnKqbCvJpVpPGUdepVNBBRfPW+1hlJu4zoQd5JQk2xC/Yn62/+GM7U5n57A0Ytwd2r/zOXUtIdcfxZ0rbNoBQHL7m8z3HzWWHRka+cHdXUe1SwvRWLZAV3hAN1eCdV9NQ7cBEhkkAWkZyKvMgB0BKlHTbMrQ8C0wE2stQIPRtg81t+90zPfNOTbmKR5nyIaamx924zFUKVjztqmx1ef9WFtj+3V9SnbVHRjYtunbFV+4V9m4B9FhJtVSw1RKWI87Mq9P9amEA4E3x3CTEAq69Ksrm7zwSBjnoCsSK61aQJlCyPZ7qdm8wwqCLGyD14OvCJPDa2efuT8aBlfIWhbcLklHQ4E8qACZqKVPlVA4krl2IcRPmNoXgMk1eUfJUxU+Y37S2/4bPrbvnm4ACzDUNfakEzy3gY0ME+wrjYWg4cslzP/EO8e9565XUGR99h/F1ukLtxB89/pT+cz/VE2xexpSFXWVtTHX7iR9e/+Rvtb17wOvBtDtkLA9Zm0K3qU7T044IZE0zS2V7Yu/Wn/935+yTw8Q9HKgbYmaFPaV9TCXYq65AsIuUtFU3bfret7SXn92+8COD88488PZLrA5CiSqyDfod2Obw62Iz9XtW5sBcYOBmBocv/0yUlKdIKbi1NMHt/MGihrivLupsGff66pZ7KsBrrP3OfTQ/blo0L5t5RctzrJO+8CGoYh6HQKKmZeCCFwRqDmumDSkvg85zFq14adXjjy0hFSAH+dWjWMIoCJZNQU8JRk61NFYWBfRygm5TIR7pCjR1LTh7oqNn27K31AObd/3t5dR1N0WCzUv81YPXfGH0sZ9OJKMG2FILDAy+DRmZ5vC4T+0by+o0KAriW9YmYaaYsrZ+7eg931nsKYuRMpjoQ0xopasu7Q6BU+nz1YFTiZe09NUu7Cir6SRla7/6tQPrhkBQaVjmrbKichjcDx12531kTS1vQwRObnr40WXVjXGgNqYOsO5laqOu2n5Sk/DU9pfNgr8arZ/TO//EHlfd+l/8jKpAoFZe1OHt4hKdo5RNVTEkydJUw2EPGAMeHI0NZtKyRPz82c0bRcrJeJ5AzXzsulZSNfTV22U5h+c3cCsDHKpsKmg4BZulhgq6GCJpzc13gI7c8/xfIT6AM0TF3N/xDsq+i65XNdBNKm7JUArzTZc0UDXO6pg8E/eCwZlCIENCU7hjStQMDhQwSH8QF6uHMlddD28PQOj0Y2FaryuUIcEhb33MWxsn1WBolheXxYk/MWv+ht8/pch74FOAu7ZF6/DrBNMZpnBAIA/vFt736IrURZe8TkjcXb3K2f2C6O724MZ8b0XLQP3xiZK6vuNPHP3Qh7ubz1z33LNgEMBgcfAxVYMKisMziD6aVAuXViDogGfgn2Aa2vDBTJ0d295ZvzDVtHDDI08Y+3ZLe9fGaha2Ef/u6DI826Gh+1AVE7CzDXivHLwU0I5g68royJLiUHzOyZQVbErHhjLttfPDtXP19Zt10xB12eAFSxDhrysWbgxSxFydBvp7H3yq+AtAh2uKZzDlggAWZSpR0Cc21YW33u499cM9JAQgJN21IOrTzo55T1EDXMD4MW9l0lv8NiHJq68a2zgEf10EaQqK9v0Lk+/DfWq64VtRDZih2b88l5o7v8ddhBVuWCBXmyJVEX/DYOMpq0/9cKpuTpgU99fNG553+orP3cmvXAHkDh5E1ExkYV6A0LZMFQQM8jJcuDSuYeGJoYAjk0xRF+Xcju3r77x3SfnsTlftUk9Z+rav7nvzdTD98bMvUUXJoDbYP1AjOD8U3lZNoB0DTIEKOkuOnXZBJ6nck4ofiESX182NksDu5/6M799EQ6obMghHxZSQXk0JEgz81jTQpZm11O/DHZCBlCuyQJjgZYWcqFAbiHTXj37aXTkn4+wXAr/HA8C0DXFXQ9SDFVejTEWPK9Dqr4yeev62P/5dwHJRM68eWqtNZu5vFB7UtsYSsfQVV0d95SPFWFXS46uPeYIrXFVdTFXXrJPXnHfp6tknDfhCqUBjcvbpEy+8YUNoUJud4CFAcPnbqWOGi9omkgxEH4o4MKuAvmLs27npj3+mugEkrm5eL+/avOV7TyyvmLPq1jtAb6SuuxWs6Y6/Pg8sAQlDlkB6ymB1QSOpOH0EHhmI3/LDX7QFylJ3f23HKy/3nX/N6L0Pg9cHrUIBbcNhGFmhCg65YIISBeTfW5AomNVDPnUB9+n+ESZlVhZwDUcUOBO4yzBWDPVd9dm4qyrhBR9b1euqiqB1x/KpBO7X+3oWnCw9/uTeX/525V0PrVjVB+Sqs+rR9j2mQh4+Oc1N9N919+u+sm5SNOip6y9ujgZqwR8lSeAdpvz1E07NnHPxQO3clD+4zFMzcPWNdMdezTLHBEnOY1EDMLhsgFsGJaUB1cAFNoQDTKhBOWHV3xe3kZL2kiZVHl/3w5+3heblejpxc25sTJzcqQKRrNnQ7a/ubVkAPpnXTTz0hBJeAbbBmFAkm8JEUnb2RJeTwOAV19ncGPCiLamSaDq7TxIMD3CyAhnEwhVhmGo2KCNnMhcWPp3V0PctE8HXqXh/zz/yuEQIbpZX0GODEzNkYc8Lfw+X10dJ0QioAFdl1IM7ximCBZcp4olfdx3ld2mTGzd99mtbf/uMKXL7qXV4v/q+ShhNsyyLHxhoPeWcJYQZKGmMkdq4vxEL/l1VnS5f/4ln7HngwbHb7mz3Vr1BPMuaTlBeeRmskGQYkxOcrQF7y1klDw4AdbciQ3rUDNx01tBDCOBgtP37u2saQHSPjY5ueuhHnd7K/mu/CGjyNuh9mBYyZ+j9X/wqTKwdL7xoAZhg7g0djz1BpsUtPM1ZV1eEjVu2P/yjTc+9CkIeKIVqnGqLMN6OOYBMIBqKCGIGMoql6TAPnH1nYdp16GMm7rxigKtSkXqzNi8bBlA9zY2sGLruUzGmMslUhH3VTp0hUE1V2lMGKPWedMHuP/1x5+//X/u8szbd9X1jbP/Mv0UkQdZyclYVs/lxiis5IDGAFnTxwe+/UlwOqXm9uxoEU4xg/X/UHYoRsulrXwe2Gx8Z7jr7wjbiSV/7aTqRFahCJzk6xoEKymqKzPIQZRCksgnog1HVbBCBVMNDH0DNqrzlkZ+0uiu2/fGPwL/pCz/e7qvc/fTzqPpNQ9i5VTZzwvYDbWCAzz0HPAHuy0GsmTCH8IJpBJQlUqARacptzbC4miMRpvM4TEbu6NueAPqM+i112hEyZHxcWTfkHS88mwyemiDliUAIa/ZdczKuhgzxx0nFcNnxCTDPDSfES+d3Lzo/37XcdPYz4AFA4G69rINflXVBlwyNF/MwE+Gbhmlru/eu/Pgn3vCUZNxl69y4S5dyN/Uy9UmmOuIt7jv/4q1P/37viy/ln/qV9szv97ct4WSex4oxO2uLBzTO5BQKygkABvEm4jIIZ4ro9cEVGiakVmDq8Z0bukhxZPYZoGeUbevDwYW9pS2TW9fBhNj1+JOZ6z4vs2z/BZ94izBdNfOXFVeMXP/ZXc/+gd04CkLG1ix4KdYQnJOt8nTfO/Uc7RrypzjF46LIF9ZjC0FduP4p7oXXeZeKVYeE0Qew/fEVV36h21MPPAOI97tmp72NYC37UGQ3gOrr8lX3kmB3acuuHz2hbdohSFP73YooKQTPkeICCs/JLMSIpFvU0A+0di2tDi1n/AOeKgj2PtLQ58UTAX0kOMAEe72hjvo5b889aeTrd6lrVysCjyhIwILqhKFnqW2AkFQMQwdbxGOZgC7LJos8K+mgDIEfBRvXQ5IfuuAd4k1e9RmT53YtWb7M7Ytd+gnImeu+9e2lrjLe5jc+9EhnccO6L34jccGFi4tKl7lCI9960BBzIMYpMoekG0JBnExfY3g3xtWpLa13o5WfjvhM6A87EvJBK4Vpr1CUimYQbLY0sfXpP3fMPiNMKvpddQOuWUl/S5evwtnZrx0tn7Vu1oJVdXPBZK365PV8V+K9LVYFl2YJWClQZsBfmgHBjnu+lMttfuLJZcQd8ZZgeYm7IebUkEQJbu+OkvpRUjPsrmglTPr6z8vb9+JqjKiPW4ZmU2BiIYe7zwCLoQqSyqk8EK6IR6RtS0WHIxWWxSEfrPrWvT3+0HJSvOs3f9RlYegzt3SSwMbvPZ6+8ur4CYskKm969PFuUi12D4AUFZct6bzsprHX3qG0sNyEJA6J9H1rlk68OzJYcIp1pek8fgjQM6Gfifv0n8FVBEVzDnHLoI9BIEysW5269hagXwxH0tjra+oM1CZcNd3e0GBV8/oFp61YcCoursw+YexPf8cD4ZoCghKX3hQJcYeQhxSFC/Zg6A1L37wh/cnruxlPoqgq4qrq9TR245YuZFcnezAhzCf+si5/2Zb7HoRQzSp4FgICG3Q6ZEUAFP2RCUSt5AZHd/zmGcUSIOvxXJ5akGDRmoHxgSS56rEfvUOKe8vqe6uPm8ikjQO5yHGntPlD4bpZKy+7CbAevvX2xcSV6+k1nEIaoEHFGUuUFqhATZidM5Zc5He1yj/B/UgjcVjQ3/tJWRVx91oEYwETbutPfpNsOKWXlGeYUNSFdc6g/TrdlRFS0Vc8J1mzIEr87UVl2x94RM2zWGUjS3lJgJEjABIkekHO59lx4GgIXi7Ss7RmVoTxJwMhUEhgCnqYajBmKW9jr7s5EahL+rCEYVltS/aFv0D0Abkj54LSBZbCJWfJBoeuiHtefakzeFxr6WxhzQrc4sRlAB68DPK9pkq2tfn7P10eaF77/e8vKa6OnXmRJdt7I+3LPVVgOka/+ZBqsLHZZ7T7q8X92zmDV5xZAi+CkW7rWP8lgJw0puA+Mu4Hp/dhOf1IuL8X5qIyHffCXAPodbRvlF0eHvrQ5RFShsdI3DUJdyPY16gPM22GzE54Z8dJ0VKvf+0tXzf2TVAN3q0GuEPIE2AqwAP3FSUWXk01rbEXXw77K4GYcDXGUw9uuN9dPeQODviao65ZMaZqhTfYQYoHL73WHhkBiaKJHDgmCGGs0gJ3Rk1+bMe6ex9u9dS2M2WdnmD4yhvo5AGwO4AIWBgQgPCuQVaqa9Zseu4vu0dXbL77viip2PDoz6nC91//haWkdNsPnhqPt7WR2sTpl4BGRLuig+HCJWWMfayNAVGhUKwVei/Spxmi/xnc8dckFV9vWm5QcPNIAR8LuOvrN6/93JfxjLKrLOmt6WPqI0wd7gh66tPM7BipjxPvOx7P8OU3Hti4heI7NsY1MWsqBOQo2HHW5lSFNxULBN/O3/wWdGjaHUq46xMMktdqT2glqRxwN8aYOcDIA94K4If133yQZrNjKspkSRc0k8P0bGvj7W2JE87sICWDn75FGN+dvuSidlK98Te/Bd0tQwYwTFbngOIVUwasDMMAO6/t2xGbs7DVV7OrOzLR1dpz7nn7X3un/2PXLCFlW37zSwhtiHQqayzYIANcP4+pWsP1YchuU4v100GfnkILoBeG4UgUf3Tc4fUEnn0v5CG2REVUWMhYgMCOu78b91R2E2+vN5gEKiaz+gmupgA3ZDyNI96SNpc3edLlWxL9lMOaZXg1kOoEhDXLTYLH2c+LugYjKG7/yRPhIiDxIKQIPFvE1MCVdOPmVsxdF61siblKXy+pHP/ds6ArBNUA64g1iJoiTO5b/c27WsHCVZ6w6a+/pzqVQOZMjLeTqtaS0ly6C2QlHh2WcKMZNKyqCYotU9mWDXtsKL3U3xidc7w6tk/UsPZib3d46y9/p47v1zRDMo6+9XqI5VaOpBSPXc9M/SuemVfBMbz3mhCizlkDAaQaZNexXz+dKG7udmGJa9pTEXXV9pKaDKRZkCHFDQlSubS4Yc3Xvq6MbYMZzwow2W0Q8URTLR4bvwgTEsBC7ey+VXfd3Y3rAXgoC89lMSGsB3fVJBzoB5jQO8SdXnSeuWoNoAO0C/kZ1CGVjQPpZEf93A5XRbazW7TlvMBzK4Z7zji73RtcSgLhMy6nyM8WjxUWqmojUwPd6yKOgCbmtj/2RCspH7z5NkPLZimYCp4qMnhgTYT4Vo+E+Lv08t4S01GQnfmvR+ecKSUzHXeLx4VoQeWB/WRqTb7y2uCc07vwOGAVQA+sADpyEM+B4uHNLlKenHWK9ujjdHy7QfVdsnJAVnKmSUB98RqvaiKv6BTYenRF7LIrulxYqQOggxoFjwoXylJ3LUycbnfJa4TsueMbksUDTcv9ychZl493deFRfkp3PPv8UlLVd+n1Cj+57dnfdZOSttq5k6++ufrLdy5zl4/e+wDMAMsClqBOflKwQl2TsGCIWvLerdEF5/e4G/dGOqkAIa476yMWpA0FHP4RI106ZGXx6EF9dLP6T/WMjptgWAvCYX24JVHK9/etveK6MHhAT1WcwXPJYTzdidANeOui3mC8pGXjhy/b8ZMn9W1bC9WJuqQRUO+C5Swcmha19PziN5fNPbGTFA8gSSHcEefcV9Q5kwj6PUHKooEaUJD5nduoIq3/+597SV300ss0XaR8fv2vnx66/Y4oKUrMP3s5qUp87GPinvWyRYV928LNs9o81WxXhyzkYMYBB8EAQI7FLigaHusWqTz22uIdzzyfU7NY06uJpoYFghLQuioeCfeZVZ//Du6HHYn3mN2p2tQdr8+bAnCsZFNt/+6d990fISVhT3nSOT0LnDxA6vGYHINavpNULHVXh0+7iOuJgugAs6rneCLlOfhgTrKyqc7v/cnPlpQGe0hgCveYF4RRLZ56Jlg90kaKh447e+yPz6+JxIA0VI3rvf2Oblf1+sd+mjj9vLc9ZdtffSW24MzlpGTVvfcDcQOCgpjf/OvftwbqepjK8Pyz5dxO3ESWNd2QeKzuwDIyNCQ6SBQN9JFEbSQfbCMhwnTGkzWKeFh6getIYfsvQH/Yn8EleOFgczTdeUCI2hJ4ZYGDcNUpEPbYn/4Y99V0M6VJ4rACUwvoRfwNuDvkCoFC6fFU9809S2hvFymuzYHdA9zzwLO47wyflt23+dbb21xY5QsyBtIp0DoetfLWOicQcTDBTw2fdaGcioyvXYMjb5l094a2eScucxW9Q8q3fOchYV92b0+k1R2MtpyiHpgQN6xNX3oFDEz4oqs33PXt5a7K1ZffyIOXpZDVWdnkKQ4AxSpfTVVUrPvgFfhHE1gIDJ4FwQBWWtNxs06XpuvFQzokHdYNHTv0R/qBKdxxpRZ8vYa1+eDP4T2wnIBq1jCyXe2ZhpO6QXkz1XF/fYoEIan2FjWmfS3D7qY+4AyI48bT+MVvQ1hBdjAFmYB9BZEE0ccDEHvWr7rsmggpTrnLYZTSDrcUEmyhHwAk2CjDxM47j25ZbWbH0SuvGklddgO4+VaXN3LiR1ReFDZvNrXxtT96OAIu7obrwqF5oHC2PfwDcFW2rvUt+gik/t1/eQE715myI3Ac+HSYglYhzEFlQoAjk5owjS0EGbK3g/thdy2OgvvRV8GOBfeDulTGFR6YlGA18xKkNtVUjDzH4mI/uMG1oyOLPtbNlEfc5RDpw6Qm6gEpiCfoBj1N4aJgO/Gm5pyWf2sx+JCchkNI4GML2Tx47hzVsptXjJx7cZQUgx5KOrg7J/7x0P9UP4AlpCjz+Vvo7l0qbuhI63/yVCupXXXz54c/ek2rp3T4nvvDx52QWnTB7mefj552Yau7Kn76BfsGk4YJbAI8oo7c8uXl3tL28vlc/yhqSkVjLUE3QZ7jyXPDVLD2GOvoROAiMNYwi3FGGIfH/bAnWo6C+9FXIo+EO1yFTlEKlhdIBdwtlbK8U5Oi6MKe7Ws+9uked2XEW4ZHpUh1my/onPBqGiINHUVgZYs7mhaMvfgipXhsCN42wSVcnsUNQGoKA5nkGReEmeIMpAim2mkMUJ9w1TkXxHsQBqON8a2//iZrfDeuvygK5bgtPXF4f1ufe6mTKUuUByc6u9pCs2NnXziZynSGmjor54sTe2zVzG4c6Tv7orf9oehpZ4XdVdFFV+rqJG7zqxZLsYRRAOmoq/9Uof/TA6JHWYf5F3A/bNMXQRBwvZNjgTBNU9f2TGz4r1s7mZKoFyAODpLaNKkLe+ti7oYUaRrw1/cRf6Rk1taf/dxW8jrMYUEhWD8n4AFAjVpSfyZx+vnTcS8038HDPqjiq+Fa5vauvfkrGjtm4sKgDMraPDBuUXNyJLnYVxMrm9Nz3CJl8yZ+3S4Izw2//k2r15+8/qaN/++pnuqmrsqWTS+8CNZs5V335friOsVdfA2iXZPhtQxLL5TtHTaFHnsjqaPA+j+FO0IvYoMIVseiXXNC2PCVOzu95TGmspdUgXhPMdh0At2+qyVBgkPusi4S2vDQI3p2Px5o4WUiQJ7A6lXsziP0pZOnfbiAO64TuGqcXkcNcabWsU6QLqrai32bvvOAZqg2HsFDu081Rw5q4mJf3chV179ZVTF0xZdAdFsaSyWx/+JPtJPqDldV9OJP6uvWyaB5DcMENUUxuarOUd9CyRzoGVUzZoJeUOjH/jgWm3qMYuYouGOvS5bNwTs2bZCVm++7v7M8FCdYv5UkNU6votoEqUt7ZsVJ9RAp6XaFNnzvYXN8Hy6NsCLBEyAiFnjotsUnk4lTzwMR2ecuw7Y4TKjQZqrX8VB9nhqYBG1+1+r7H1ZMCiLSVATewFNhgAsI28ypF2y+7a7YNTd0e3x73nwDKz4hfW/Y2FE7bykp5TauBVuN9WIqh91qRB4EjOSUdUAuBUIH0TId9+lUfpQs+q+R+7GjfKQHK2ksy2clPKABemzLoz/ormkAc5Nx10RJEBii34OVwzHSCDBmSPESpnzl/d9Vx3aZeKpWPoh7oQUsH4/HTzqngLsT3cEC7mB8sQOMtxZw7/R6Ru59UDFsGxIgD0IQlzDx5BzPZy6/vK3heHVCjJUv6A02c1s2WzrkS2njr5/sdDcM3fotCBHcsjZlpwTGFGwdSwFl3G61IFVgVtVnIj5z7/+QIoBjFJFTZRr/I7jnweezAkS9gqcy6dYnHgvXNiZJGajGhLsej7mSYJ+7qQNCtqh5ZWnt2+6K1Q9+z548gGvkkk4EB3eQpaZpctFY4sQPAe5pV6nT+wXjPeNphOyK/XewfVFlorRyx2NPgMGBeNcELGODBGtrAB5d/eiP47VzRWUy25le6ia9i87HVQFT1YWxsede4dVxkSqGgSIRBCXMMBYLyBTn/B4WB8C8wUK9dxH/pxF6WLo/UnRPL4+Z9oJHqy2YrmemX/Du4AKGUfN4PEDDXl50x+OP9ASBWErS2D7L6Y9GQpBXu0j9xllnbJp7UntRzZaHfkAnDwBQOcyrKuQIDnHXDTYciS88ewp3p4HXwU5TiDsDzFUVCVTufOQRU80CTWNxDFK7jGWbwNGyJcl7lvlbTI0d/NTn3imp3vDtb+GJARVrt0z0DDqWOeoC1i9CQjY0GDAQVKKF8HPgk9TDoHMk6I8R90OqVN4/GOK0KXWkBsfiTNwhNrIQcXlB4zUFD/fTHQ88ECmpiDFFKU9dhAT7nQ2QTldtvHTerpMvXLngxK7Kpm2PPmYe2J/T9QMF3ME3Ae5Ar/nunt4TzgLfBLinPNh7CuZLocEdtvRigjCJWj3lI3d8VRf3cNTKKTaWqEDS0wWqK7oswKQBfYVrABO5SEXzwIXXQcLEJhqmllcmbezuoOuGRZ3zY1joYsjAOvCnIXIMRVZF7iigHzbkj75OcLguDeJM0I+CO8g9UB7TL/A92GdXV+0cp3PYhRiiavu37unxFUddRdhCkdT0uxpioOK9df3Bk9bPPSvc0Lhi0bnsM3+iishTmjVsAp5Vs+ysgc0vpOHBxCc+EWZKhz214ariCCkbdM+JuZqwZZG7Ekgm7G/s8Pgzn7pG3LmaBdwnRCOP3bwL5Y/gOZElZOdQjyRwB8bAlB7pfN70OuSjQPaBvP70nz+kT8Ahp1lmnustAAqx7GhzrCLICzykH0XmsawTRkbgJnNjQMuCaghYskbFiUlsmEmpmJ/ceOudvb6KHrcPaLnXWWKBqAUNM1g5D8xmm6u8Z84ZG377a3Xl6AFTAMtKYLLA3M/JznzZuKb/05/u9GCflh7i6wk1dNTPAeuV9Nb0eoNhX7DLW9tFvIMXfVxbOQA+K89jXZLidGBAijAcQCFrmLhNoXCcoR9l3Vw9pPvcPxXXHygfHmJlj9Szawp6odCe9t1zByjGeOdokoEHEDmOoxR7UOS53Hiel3QqgJ/O5vAgB+RJPr/py98Asxp1+fAUPTpVTI2drlCmZDYMQDtT/jd3deZ7D5kbNkG8s4JKKAtKGvuwi/DCe3aM3nrbck95G1O8/ewLhfu/ufErn402znJO6tcmA439rrow8Q0tOp/GeqltsqrB6xaWI+gIdwF3vKmAib1fQCYe4j+nFhGnVrWmdw05urj+N3E/bD6YPgMgyeNhsEJfZgh0QVQ4QefESU7JK9q4KORVQcIzQ7h+mucVFg8n84C7bIPh5DfedkfYUxonRT2+5s6qOR3lTVFffdzbkETjGcqUNnYGWwbvuptu3wrxbSrAM7Ju87LAIe+Ayll35z2t3spuX8X6z31ee/KHO+64qaehEfu0uBp7fQ1RUhlxF8fnnKq8uYQahqBifxjM7yjiEXfHAeEiF4yErWsHjyu+h7U0jVLf173+6Obl2HE/loGZ2fQRT6RoinNyHtKohGXycEm4F8bpNKuonCnj4U+BxTo9yYALvm9IErZ9soBo2HVf+kqHuyhJivpqTs7dec/e27+WrluAK+egR/wgBauWujxdn7xMXNOPlRecSiRNV/DGGXwO4p1nt9z//e6S+kxZbbT5jN6a+R2eylY82VTbzzRlmHog+rirbGlZi1PmyiuqCRgjJ0p4Tg5XDXElq9BaEc94gxk+Sk+KQ0rvj3LLgGMH/Z/+2Mzm1gdL/gXR4Uj8Di6ya7i0CswDNMLmJ6jKaXu3ZTdvNljR2Yk3BcMG0s/rkB0pHd8/+pmbOl3+NCkKF8/Zfevt2790ayI0L+34HmwVTcrbCJP61JXG9tU21SngzuoGDwZAFSepbavqnp/9d7Rmbn9pMBNYGPXNHznzE+s/f1v/xz+ZaT4JDFga61L9iwMNwz/7hZKfBIBZDtfqwLVCyIMihK+G01oRPoheuFPD4Zo4T4+1Q6rv/2VAjzH9FvopT+9Xd/B8EydYmq7j58ANSNG5wE7vWDu65rWXcy/9beOvfrXpH28BN0OkTogcbteJYt6UcNNm/br+Sy7vYnz9pLinqKm75rhwdUuqtHm0akHaW49niyF8mdI1X/8GndiDHYh4iwiK7uh3eczEHSfur6/G5p0SdgcGL702+/TzdMd2ZWRg7cPfHTr/gkSwpcMd6iT+6Pwz1j39NOAOvD6BvyxhpwVRYkVsgKC8u0tQuAnNkajgA92L5IPG+1F+curOOtObBE4V/k8Nv+G0NoKfOfDG6y995MLnKoN/bp4z+MTPqSjyJq6hq4Ki5LNZWzZNm02mYovA93iGGeSGmGN0QIiny1r6i5v7fXVdpOytQGjnU09SrP6gkmgSmVNkLqeown4JphQ1lnclTj9vCWGWXnGFsG7EtK1Vv/1r4oSPrCxtHPCFEq661qLyVZ/41N43/qGyWVDrgmngdnveOXSLh7uxAhtiCvdFOX4m7lPusbCaCvw2s6H3v4n7B2rj/R7dQRQ6dXS4z8WhPgSi2bN5W+eV173gLVtKPP+oqBv50RMUcq+NGwloVsfHJijgbo61d3TOPSFCPCvc5QlPqNcbSjJVMVIZ9dUm/PVxprrLVZ445UPCstdNmxNMjVd0MmFxssLrLGeLpg0Wdvf2gZ889tf6xuRpH84+8v19d301NguYvajLW7ycMF0MSZz00c6FH9r6yOP6BFpeoHgWDyuZHyhs4cqJLAhkXA8DScxrIo9WUFUOr0D+/UXE99Zvp/HM9MEWDQhgXDHBHhGignfSyY4NvfX6201Nb7l9bxLP38uCL3300p0dbdSUIcXqEFWTHGtTeL7viZ90Ek878fYVz0l5jkuSpohvVszTkiKhhKe0m/jxYMX3fkZlDo8J5bJUkonoHE/E7jgc9r+Bf7C2rsz9+udbr7lm5KSzujyVSxgmyrjbXYFEzeyVH/pwquHEdyobV33hy+aatbj5qdmirh7LOu1M85njOWf/zMnJzg7Ssewf/TvK8pC8Mn2AUYkDt8Pwg/l2jv3ZIj+xft2OX/3krQWn/LWmOfPte7a99Q9+xWp+bHyc6kI+J2FHQaof2Df02S+0E0/MXTzkmeOYpiAe7HNjjWq/H5xQoLuiafUfXqLwJ01VzrKQRQivYK9PwC4LqQUUCjWNHWu3P/KDdYvO6SkPgntqd/veqahLXXHdjptuG5p7aqyk8h/EFb/go3IkikWslpVXpZzKflDQ4cJurzInySzoMlU72JLkkPvzHMv+0QcK+UPs0tQw5ESHFSEnFe6NJokOSho9sLP/Rz8Nf/v+/Eg/NlcXsanifsvIsfnCZpEw2N8695R2lz/jqR4hLUlXWZopiXkrYQAyruCAB3sdJ+aftTs+THULC1PysqiZxILJzuUmTHESOy/iGs+BtvbF88+FF2p3lbR5KyIXf2zdz36874Entp98eT+pSjPFXcSXmnc6/+Jr2KfDpnyWY3HT8QPj7pTXAqFykF0Kil5636GWw/ipf3kZfXryPGw73iyP3UqwPQdEoSwA12cVjTMhDDV2/abxlaM6m8OzOwaVLTScvCBlQSVbws5n/7QsEOpxBbD9Dmlymp9XJL1Y9pLAOwmUR73B/ouv4CayWCiD7eR1CFYCydWQBcB9XFURd5vuTyWXXXbD6y6m//iz9//w53LX0t2PP5qe/6GIpwlVPKnIMKHeqvnbHvkpns6mVBrnsSL1mGXGIcChVRbUwollFP5Hxv1fDvN/eh8aRxTg8RdnfQmr51QULcY+MCgGtQUN2FzUpCzHcyx4FFwAx18zFIPbP/jFL7dDOiUl2NjbBZZ+dp+7KQUX05D2N6RJRYerYsWXvqhQ7JGhcNivIWuZJMvnQAGymojNMkSsXBfHdm/8w9MD//VZ+Y1X8q+9svJTn4vXzoUUEYEBdLWkmZqMp7HNXZ+86jP5lYNANWD1DqsXj7EEjs0rsuBUZ+OH5o/CMx8U9JmEXrgx42Hv7AZwoFnVnCodVXZq9c1xFWt7QCezEoen5fFoEjYrhFwKoQbkoA+kO084o8NdnCKlINU7wJqS4xLe2b1MY4zUg4RPkLKlrorNTzyiQoArOUHMgy3IasDvIod6QuItEeSgImd5ftXqtT/9ZeazN6246KoVs89a5W9Muaozrqbe4jmtvuAIqe131eFtIern7vzVr0DFm+iYtQ+0WXMIiIUTMrgUddS8+i9k0UMWA6bfI/CQwLc0FVw3K2NcA82rPI+HKCESWDUvKhOgLQVeF/AoOoh3jcvzJt6lZ9fjv2wvquvxlqVxO7uuoyjYy9R2e0Nxf33S3xQpb4iQsvaK5uxbL0IO46VcXhWcczYKgSATZW5CBoqXIf5hrPXRtaseeDwaqOkndXGC/arDvmqnv1NDxB1awdSkfDVJfzkY16Gbv7B7yxrsNKOY0gd8HEr0iuTkNPkoOvJfUI1TuE+1GZrpkAsNn6iOZ68mJTYPb4IXtDxrKqKuiVpez7NizjncRCUrL8gHhDzFejc1v2976uLPhklForQq460ZJqFwUbDHVbKc8Q7XtKxtWthdWd9DypKzF5mpiI0nvjnO0lgs3VeJwzCy00jeWdfXFZXN7evNZFpOj3vKsV+lq6rXXxwjZUDrcXcw7WrsInVgxpLEs7y+ae/zr+BOh63haSuYfXi7KRmPq+LdTOWj3HYBtNrMo0N4p6rD9Rs9EuLT795zaP7E5UOZ51ChQlTk9BzYOg1vCaQBh3DwISEv4oIdnhPiTLGwe3fw5qvOqTvsDaRrBjs+QcW8plJWNvJ5VufBypqTCpX3D19/c9hb1+2uSDLVsao5iaoWAL01tKCz7pQVx523+8Rzu8tK3/b6tt74Rbp/i4jV/qqzVYVrtgTmFDalEZwFLAFrE8HKKHv2rb7qRniVHhfelyRCfF2kBG8Y46tJkeaYe3bCVTNAAp3e8rW3f8dYv9mwFXjdwougChYR+vc+w+EevKwcgrtTy//ebtzMe+AdY2gf7OuK5YtgP5HBZNDKCgsvLkyoeUE0FGxOnmfHBQkbVmgqSMjc1Mbpwdng3CPRoXJWwCo2w2DRZQO9YJcQycp1tEWCC7pJACK6xxWMn3hKtOn4kS/frPz3r3fd/s2t531yRXBur6fk7aLKvU/9StPyhbOA7+EOn5XnWc5Z8tekQjcU0RKlid8+naptaXdVdZPqzvK6SGh2wl3d6wZxOjtdNDfprh3xVMeJP7nwQ8Jbb2NyNSxewFO1TgUhLtRowlF54P0HX97dJDoiqxx26XwqCRduojQ9hYIvyXNs4QVFgcP+BBqVBTMHYcFy4M9hPqJ0gfQmcjqfn9o7Pfia795A05AkVC+iMgkvY+uYDrGRgD14420JUpr0l3Uz5cNNJ4/dcvOWj30y++oL1st/2/b1uwZPPAfmQQ8p7jn1XGEgA8ZewIYqzoU3/wO/Kgs40RTccMHKW1l3PoGo9g2suPjyVhKIlDauufXOjZ+9rR/Q9wTw/mCehoTTKh3+cLuvcvODD5n5/ZDdJ8ACiRK2rpAl3OHjeVxH/WcK79h3saUj3H92Ju7YwEHi8/ksx4EAFJzW2ljTzck6HqcT8TiDrloiL3BcXoIgdjrxTv8TB29BpsimZFo57Mt2wMK7KLK5vGXq0sR4pH5BH1PZU1zZ7g2ualq0/eNXrDn9gu133JM6/2PdTfOWl+A/tbor1t/7gM5N6hoV3n0U3j8BZYq7FqpaqPkDSgMLlKWmtu/Azh/8qJspTbfMNYeGpMVtfY2zIMBxs9tp0YkV8d5QFylKXnBJftkyEKd5Q8nK2I4RS8VlbG8qaMo/NTJHAv2w2uaQrvaH3H92+n/ifYxl7ACM26TOvSaB2bB2RZNzpjmBdyEz8qoCxgW7myhOv/h370lTuEmK83qqxoOVF+AnOUMHFwuChJri3hdeTbjLwSIBv7d76iDndRbVx8ua46VN2CSZFHd7fFFvcWdwdn7p2zJVcTdUkqeXgZCp4m7MJdgzRubhzWI3N0WO9KYbT2oL1tt7d9rjE4PnnN9HyhKBWmCbAQ9WRTm13uXh4vott3zN2LJRsxUee2lg70kcS5AnmnaUupTCdIaYOnZ5PvNuAoe9MyGMuMHj0ShJF50beqlOczyRTk7Q3H5KDZbSSQVvAMtTCVAVJgU8d+LclqZwX6CpAYAMkeM5TBSQGSZYiRr6xlV9F16TJEWDJDTgmR1lmvrxdEcwFaiPYQu2yjRTFPN4O0nRqis/R7dt5am6f0YdDsF5J787eXVNB57PcpIjK4x9e9feePfrpU17X3uZ6vzO27+V9DQAbcWLsT47QWoj7noUOSSQaDxu12//oE/uVqipOK0aCve3FY4saAq5BPLKzAKjo8+P6aBPvx/w9GZRgDt4FOzDaGLrffhX3ELaum3Fb5/Z+fTv7HVrcJMaO5DnhZUj+zIDwv4xPDXo3Bqo0IBnCnpntV0AS2+DjFd1PT+x9fEnOgK1fUw5JLw+0pBmmofdDYmiYJiUx10N6dK6MPF2EHfr3IU7/vQXyoqGZguWWagBwW1bp/M6Kdx0HHOgyBc0JUQK+AULzVl23zNvts8+Z+s3vkW5MfaJ37f5mlpJcaoKa1z73c1d7oaMv3HQXb6MIbGrPp0f7Yc5JQnOsQ3njJ3IH/k+ijP0DIa5Ih92V3o6h8zUkdNXd6e2yxVWOcCPY9GOLKh5lpqWOjjy4uXX/aqorOMb39i2YsA2RHtoeOCeh+I/eMKamDBMG3s384U/hesWhTvfjukitngGysRyTyrEM10nnNvqK0mT0PKKYIypSvtmR0hFsrgi6q1OMfNH8URZ8WjzvF2PPaKIExSobb+i5UTdOeKtO7JGw3XgI9xMF/snqzzdvnf09nszp1695Zv39196dW9ZbcpdPsg0pElL1D875Z01QBp6vbXt7tK2stC6b91rb1hrU7zZBWRqXcIdxIL9w0GVOOz1LelUxE6SMBMOreoq9FETkZeBnTXs9qM4Tgq7Z7JSHg8jYBkLD/Hi3OhV1FRxCminYxFoa0vHMhKDVXXw9JOqOikqYl6WqCXu3RG++Y7nXJ4/B0pjl1y2+/Y7uk790Cunnbv1H2/ZlOZtDc0Hy/Jc3mlvZWqTMpgmfeIAYDVu0klq0QM7193+9VanzVI/qelz1zndN0IrfA0r8ExTdRpvpFH0VtOCvc+9zI5P5kHnmdjGYr+tFurOCrVm8CBH0mqWCGQNgLETHa2Z8z+zrKhpuacqyQTS7pIBMLGBOTFv4wAekG0eKJoz7KoFJds1Z+GuJ39h8mM6tUHKwLzJiSzMJ5XH0Ybh5uWD3XO4d+90Pj2FFnAX8H5BSoH9Dp6ykEXsW+XMT5DkoJdkp1gE8rZZuHn3e7t3IPEMDbteY8WrMcmpgjIpylmON6hBd20d+uKdb7rcbYGKN6prnveXvsiUr/3Gt2kuq1kUxknjNcprTs9IvLUIDDlIdo0FrafnLZvaxt4//aW1YcFgSSNmNW9d1FUb99VFnBN3K111I3hYu3qxv6L/tq/QrZthLLOQM1gsSshZ6lRGLTzIEY80YHtRCSSKLU5sefhnbwFnMb5Y8f+v7kqg26jP/H+k0WX5kmzZsmznbJO03EcIYYFSYLnKli6U5WhLobRlS3m0C7SlLC2QNpDtAm3ZbnehvL5SytWSy3F865Z12JadOyHOHRIfkiXNjDQzmtFo9vtGjuI4cWB333ZbvXl5sq3Y0jff//t+3/mzRKrsOwxNcXMLHLFBo3NA7wJDD9582AhIn8QuXD629k/gXCGCzaXT6CeyuFYANVfA2gKLu8YgeuSnAfIZOkBBwHhjMjJuDdE2vHICl4HPjzoOv6kANluC18CpktlZ+S+xdMENSApMEUFt9jCfLoDQ+fT4O2+8vezc31FkPaV720T/gaa7zHW7vvHtzMHdEL5PAv7KwF8VUiwATWwa5AU2iXV73ASArrinx7/82h5SNWhs9OptXSYcCxjQKE3clD2qb4yZmwdcS4du/nyyr01Rs9iFlxPYDCczrCjN5vogc2IGEZOf2Aoo59KRyMDylQO0bZCYNxED/G3sFtZVDZq02oquATDskLEuSky9xOC9/kaI5QoKphzyafSymDXAej2cAba0T4/TJs3PLPccw3IpLeVQ6qPA9CxqfQ4Xn4FMAVtAgAYX2pssd2okNc0RiYovY4PJJPxlVVQPHty6avW7ly7/va6ivaKik9DrTNTGSvMGovt9ZaP76afUfIpVi1xBhuDoeCYNv1lh4LxyYGwZRcmrMjsYDtx8e7vOHjc5Q6TWY6gDTDFINW8lrSB6P+100/WempaRi1cm1rykZscyKpauAFGxsgj3TuaYWU2JZC6ghiMA6VQeggvMijMfrnquz+RCct/zL1m7aOlag71Xbw3orV5S4atuCdI46hCxOADgt+nN8dvvZKJhFRSTzSVFPi3nM3m+hHDAOEhsRmGYueSeUZKMMIWRJJuFgy/mChwPxlpOSDkAqXhYMzx8H05EhsH44NSmu5Nyh/8DHl5bx5jYseaVF12feFlf8S4xbSTWTp2lnVB/IqRDb1pLzG/XLY48/rgscoViHjDfFMQc4DimuFwah8ZlRSzs2h6668vrDXY/hcxCgzROe4VJU4xqjlDOiLHZb2zqMzq6zI7+pk9u/8mLcJtYrRE3LxXg3MNnKXW64755Yfqgz8lHmcYMHItd1KBTRUXYMth5xS29S1eo696c6H5r4G9v6YbowFzpJ5VeQx2A+iCxg7GLWxo9hO6y1G554B+lWBy3sak4X5DWDAvOgjKZXCKpJjNzyZ3LTwkispKInLaSGleWSxI4KFUBKcMJkHMQ78u4uxrsjDZLP014jAqVL8s9zQppqYA58rGJRHt37F9eCP/gsY3XXP8b57zfGyo3EuPaiqqO8y90r7im7fLrfI88rBw8qACgk7FlHBvFctiMD6ojjUS33/9Qd0VTLzLeY142pmvCJQR60PGGXrrBZ2r00Y6gydlFKvt0tVv/7beY8UX2Oo01Q6O0SGFMdlLo8ITMzW+bR0MBoT+TSHG8mmEOvfh6/2dvk3rfV5M79n/rEW3VBwSuDmTzRZodGw6U6Bt9hloAsJ21rt1f/7YSDgK6gTePSza1FjgwuBC7w9GbJtA8bVIUO5uxiiaktQ1c+YnJRHxkIhAsfHBoYnjn5OghbFzB2JLH5ZRalnzGptd8OfaZkuVxOT8hQqAASgwG83ghO6bu/WDqzdfaXEveIhWhL909vv7d7NZ4Zv9uMXNMHUthawaLXggMIq9KKpeRB4b3f+3rG4w1YcoKAapfZx8yL/ASB8QxoOB9eqxfB/QOnAzQ128y126/6mph70Ec+EIWO/T9YKvlnDApwUfRPrv2qeEekLlKYmpOgZfijkY5BbAPZKEeG488+1T0727f8Q8P+Fzn95tbcMCVNISNuHrMbaob0EaVPYbGIXNDF5xiW+OuBx4SDx4B8At6C4LGCjiYUkVj8plD7tlMcYopjGULaVEBFJYf2hp98rn3r78t8Mjq4E9+dSw8XCwogPnA7ODCYTZdDlBLSl8O8QWN1oTHMiiAaJA9l1Nxb4l6YMC3bIX3vMu5WC8u1ZVlRlXBBxbyapLLpxhRZiTUuWI2MxCJfesJONbYK0AbgnRlwNLcSzUFzPM76PoOGmkrh/WNcYI1CQ+p/mOLK/faC0jnCGcxLTB5GcIl0LAih52t4KvhArlzWppzTn3PaOqJwJtlS9qUSqWOHDkS/uJ9GyjzJpqAa91FPjGga4mQxoCuGWClh3IEjI6wuT6qq4kTZGPuqagP3X8/63cXVCWpqhk0EQpA+CSTHROSE1xC4bIqC6ZQUlgk6DsOyJvjEtn0ZJ6dFCCYl5Sg3/vZawGEvKc3+W+5VR3dLUPoLPNT2ak0n4L7KIxP5SZTioYsS5EqbsIWsb0RPgOiKTCyeUlDKbgiWzl86A9fedDz4x+r4HqKRZ6BaANiwkNyOg2vnFIUBru3RK6rfcvnPu/WWZHtWG/vN9TjRTtKM70RCFBJY8TUgqShplrwEx266h1feSCf2gOOEVskNBRRAgUzeYjLHnROuZfMEIpeUyWMswFWM5nJTeu8l13dQyoi5ka30Ykhq3lhP3K/u/qpxn7AOXo7XCEaYrnaHqrmj+BtvnB3urNLLUiClrNkGE7C3wvuQwRcCfaH51NZPpEQ02OKlOVSosDmJG6CTyaP7Q88/fSr1XXgA9cS6l3nwi1rXsyMH8sVZcCU8K4SBf7AyI7RLTv4DFsiJMHPhodG2+OHDeyYOhcwhtI2buLQYuagz5PcOiAA7FDkHFdUp8BxJxkZid4KRVllJo+/97b32ps2mmq9lNWvswVLcqcdIX3DSbnrHAP6hripwUcsHTrr9i/cKXncmkETOfzLuTIH90ze7TLknZvXXODLoSDmUbHYjx45pU7u/vXrQceF/cTqsTkiFQvipNVd2eIj9vJypn6jE7wNtqsRm5dY2/S17iuuPfqfr6tjY6BwPADzxDhWIAQxlVdAzYrg/1OTWWYcwnpcgQXWQ+SSfGps397Yy7/Y8DeffYeyvmkwvrf8so4nvj82MIxJJD4PgDIp5A/u2HNo916chtFqp5gBBvir0YngDdDUJ6fZH4Q3WSwPSAoPig4OP1FUs6IKhkVR5ClcFi8r+3Ye+dnzvgsubidmP7GMWJDKQbtwTQAO9CLRPco9VNEYomx+Yt1MLLErr81sWgeB1VRanplc03aTaSE3x8/K6J2NX3tmKbLsiLFVZmJi36qXg7b5vYQOmRvipKGHrgWf4zc0hEhDlDhj+havDgcwh+EwWhz9utpuUtnlWrbzHx/l3O4ikwBbAcE7WF6wZskikiAqrKTmBIXPKLzCANqRJDitSOmcSsm+QO+Nt7vvv+dYx/tHByP8KDgMsBgyfDCJk4QUU+JOLG02Ltl6bdrn5FwYEmEzaHbAUWEsg+xZ+YKgTPHKh5w4LoqMWgAvyvQHtj36qKd5gY/Qw7qquKk+hpN1JYk3lq/Sd7oM1u765raq5vZzLxt/7w9gtSRFHePUaaui5XZwqY7WmiWws/NO5OxFiXIZvlT6wkgwJ6mCVDy4/8APngrUt24m1KC1YYBUxnC4uxHnu4kzomstyX2EuII0VkjidGOU1HTrazovvHTHmjXCzh1qEbRLKshoBUBz07jBSEkUC1w2r5XXCnBUwTIIqqIK3IE/vv/hcBDpxFRF60tnuGxaAWfPpnDTuFzQVBs34uI20nKjq/aGsQUTizuIjnCpmaDAn0thGkhiC+BWVCQXOrD/w399IXLzrW3Wuj5ijpvtcYPdi2/YXhI6TqwT3HKnfVmHpTdb8+4HH9yzavW+N95S8hk4x1wym5LVmZhYWzCHoyMSL8zc8X82+3661pemrXJgDCGMVkVubHTyRz8J0g3txODW10IUB1AyiuSwTvCxbm0ZWZRyRnVNbp0DQY4FXL+1m1Cd9kXDN96d8HoyzAQIsigUQX9BYkyBHwe0k0fAC+CPBW2X8+OpJMB5VcTxNa08IIFFwU5W8KhiBqAfmnVm2vODoHGtM8A1ji3BtZzGgohMcBnAiDlkpcTxAgniGoxjIbRIHmE2bjzwwD8FbI3dlLWXWPuNiA48+nowld6KJm0TDJzjktC16XcQOlXtm3de6k9vFQGvKtpIHsdLU9nMLNo3rdaBkbYglue5SlWns9j37MyxWmSXKj3hc8k8q43WS/mxsQ9eeGnTZVcHms/ztyzu0tVEkEkNVzT16urdpB7JQEid39jcBQeT1MSM9SOGujCp8pOq9fbWbd//Ibt9a0HIKhDUKZwoZuDtC4qMC3+yGTicgOgmE1OY1FQFXpXRXKSmQ7+sxi3G5MqVMjQyyPuIeyb50qgGzg5IIjhcEAdYLU6zAEURN2iAWqrbd/Hvrx394ZOhFVf1ENsgsYR0VSFzYxg3wzZATOS1NPiMddpyu8Z+qgn5cA0u8K4BXY2HMnesuGkiHAADlVXVJBhxeI9aE1GJFEdLEYmnxnRCGfKeTe5cDrvvSjPqfPniswUmDZgUfFA2q6Y5dExgFrl339xx7529rgVRqn6AYL4MzgHASp/BGUWw1QBRdYhu7TO2uKucgWqbx6DbpKvoNtUFll+1d/Wa7NaRosyD0Zd5DlmWpHxRQWL2JFh4sQg/GBP440VMcch8AbPkqTQgRVANKVvEkWkpn2GZafSlhQVYWweV1wY2WFkEkC7iXH8R+Z5E9sDorj1vrt35pcdCy65ab7Z3ElPMgMUcMIkBUh+kmgeNCyEYdJNKj1Hbf4QYxhWmWyJGED2W29zEuH/VL9UMxHY5pF7Q2MABq2JLhqCtHC+ZOC1GnVWP/Ai5z2olnMm3M+ubpSejz6xaW9kCpiZmbUSqZ0OL3+zsxP1DjTGqOUbPj9KukLZJBYK9gHF+jxVebBok+l6bre3Sy7d+7bGxl14Vwv5c4nCOOy4cP1DAZXVKTlEBaaRUlVVzk4VMRsGwgsnwDJ+fEjNTeayli5g+YAF+pkQIT/ATg3bDeeVV/BVSUVUUwOQKRO6J7SNHXloTu+OOnqZ5bj0gFoNfZ5mulOqRDStKzdtK5g8TB9j3kK6iT2/x0IYhyhEn83uNTe5Ku4/oPPYFR556DnxMacv8DJRSKhCeoTmw3CLIao+z2fczyr1UXphJR1E+QQd+/8b6cy/dRAw+yhzW1/kIhNGOcEVzv6ElYHK6DTY3VRuhXAO6+Ugdq2sJmpxBE0IdcF9densbVbfB4uo+f/m2rz+47TsP73jk2wdefSUb9BSOHlD5jJrPFTGcUXEXs6LC0WbzKoONMAqr4NQDZgh4GVSPTSDFoYzlMsBLk+rEhLrjg1yHJ/vmHw//6AX35+4M2l0BujoMzpNY43pbRFen8SU7Q8ThN7QGzAtA+h6Do99UhxUFSx14VPgyUNHQb7AHSWWwcenok/8sj+3D7LSQO3Ub0Slynyn68oxDuaXgI+Q+89bNKr+VO9xKwFnNMZNvvhG95po2ovcS4xaTK2Zu7iLVUeM8wJfgeL16e4RyDpLWmH4enNl+Ugu3B1DQIAQgtKOfVHt1lg003Wa2dFqtXYTubmjqX3nF0P337fzxD/a+8Ny+P2xMjmyXVH5Smho/dkidTKk5rbpUSgHgXgqsqRUU3NIAX2RCwaGfvTj82A/i9z7gXnF1YNnFvvpFm5HK3goGGoM7IyazQgZc5wsBRxi8EXFg9Ec3Rawu+Gmb2RG79qY9X3u0c8k57RTtI4ZA63mjz/0s/+FheBslnzfTdp/Ihp5hPrbcl/mx8PusZpVZbbrleKr0h5NgCfIJtuNPA7fc3GuwhUhFFDTF1NxB13dCME0aRmjXMF0fpe3grDoNdR2kykOsMaoSJA7SH6GcIwZH0FjZS4xhumqAWHzE2EeMm3TmzXQF/LvWOS967z3cW29sfea5vr//0r6Hn9j11D8PrX5mxxPPbn3imZ3PvzjZ3l4c3Vk4vJffv0s6vn/q2dXrl170pr5mHV21HvvdKiDMiRkb4eo3uLyUy0NaQqABIGhiQ1dvbYxUtwDejRP7NktTJ6lYt3iZvOkdlT0MsWgXsfQuv2z3b16Tkklcd59JI5blmNKhPyHf/FxyR7t/onPmo/H7GeVeTteUy80l0WcEhQVNA1/m9e361jc3NTvXEarHUNle4QhWz49b5oNww3qbz1DthQNrcnrqFw6fd8mWFSvbbYB5arfrm+PYA4ugLWxwRkwN4OUGcBNF3QgBX127QUd6bM7jN9936N7HRr/71P7vPbH5vAvWkQqPpQXu7mZLY9fiT0X+9saBL9wWv+uuI9/9zugNt7jtrj7cpQMoqyZG7MjfSJy42Juq7wfVploipgV+jSgiZKztIlUbSIVPVw0+Nmqp7dNbAxesKK5/N9v2dviOu8N33TfRsRk3V6gqMncJOYZJ44SQdtxPqGCpEyR7xsn8stznrDfN1ak8S+4znSrOmSvqUaYwJauCkpcObZ387S+GbrhuE7H2zTt3z/krtzg/AdoNwCBmbvURO450WlsPP/iQ/PYbI1/9Spupxg8SNDjcFQvA9AMQwuc0Div3G+aBgDx0s9tm67tgxfjLr+Wjw1x//9FXfh668XMDl1w5sGSp1wxHRx8gJjfqtRHwSRtl6bHWeumqqK4GDtkI1RCnXHHd/DDV6jHDL0eqjH4jTvR6SYWbmALYwN7SaW4CuBWsqPXT5kEA8lST55yrwitvDv909cE9W8Axg/1iciLmU3lsNAOLUEpbnehmyJ8gt5jdSThLiT+W3OdaqFYeAS39VBJYlcHc+fFcJleUsW9iy57Rl3+3Z80Lx1et2nLzbWAuvUiY3IzklaZaD6kMXXeT8N5bqedf8DUtxlZCvQ2w5qBhHqg8YCHsItc19hmawqQBngBG2nbPferWwWzQG7/7/lDr+UNX3sD82y+Fxx8NLFnaTVmQw4dY+qmqiKk+YAQ4WOnX1/h19jDlGNK54ob5EQPccofX0ALoFncGkBq4Itam2KJzYxevnHzkudQza7Zd+7kOqqKX0BG9DZSg4/Ibjv78deXYMfDkUwKupy1q8sDKJcQRWNcQZ0hGmCn3s9iPj5b76TssSseqBIZmJnBYGdNbIpdmeFZilUxaGS8oaYDk4oTKjqfcXf0PPNDeNB+UK6LtFeqnrO3Olr3ffOjINx/tdXyqS1ulCxYGQEWQ1A/SjQPEPkzALtVG4PjrLZ3E7Lto+YGHHw5fsbKb6EKE9rrmHfzy17ZfdUOwfnE/qQe0N6THmQiIFQIEZyXcpvo+a7NX34ypC30TTpZS4EuawjjaiykjX8PiD+79RnbDRvX44WIxW0gdHnn8ifeNtRv0lvilK46vepb7YCCvKqKiFgRF4wRlhSI2YuREtUTBPnNm6kTjDX/2BYgflY880zT/6YxyJ7ENh1BOo6tmtNY4ZA+Z4nOpYpGROTWTlA7vT/37b3YuuGgTIV6LuYtQfVX18aXLRy+7cc+yK2N1i/wmQNA2r6FVI4qyY/e9sRZnHAyNPl1zl67Zo7N1EqoN/jtVNUjqgqR6Y5U1QKq0vCCuUtPiGtf0+nQDFiDBTA1SzTvxXtZombs6v6neQyq8xNBLdH22pp0PPcwFg/ljY8ff+c/dD38ncsENsYv/ft8PX+IGtuEOLm1k+1SlPgPe+8i559MfH40jZz1O/2a5ln8ic6k1G2thG/a65HAuWRWQx5A5uP/wqjW9nz4PRW+o6NRZui11g65P7Ft0wb6Wc4aq5vdRGA0GdNpCbwpJ7yKkFa4oaR0wtkT0jT7K5tHV+fSNAQMEX/U9enOvvipodQw6FsGd66Ubukmtz9CAJCQEb4avalF8yeVbllziocGFmEO0bTvlipL6iK4BouheYvdWtfY3L+lrWPCOdfH6JVcMffNxprtPZcBP5ZGgQZJnzYec7vD+rHIvnZdT0D12aJTa9bUyS3lNlAjRZVbIiAzGnCp/5OiB117ddee9XbamzXT1ZohXKaO/qjZQ4/BhGrk5BuASqR6dftLqIfM8xOWnMAUYJ9WDdNOAfsGwfmFM3+KjHYAIAepsAOzhaN15zmW7z1npty/uAUtldgVMLgh84ChEFnz64Je/HrvmJi1yrh00LvToXD36Rm3ZdF07sbQTM8ZErkW7vvjd9H+8pRzcn1XZJAQjBWx0g9B/rq01/1dy/5hziDMWW+RPeABEU6X1RXAzJgrihCBySYC8U1xegLgmn+EKe/Ym3n9n9/e+777w8jazrR1BiMVPbCOGVuzJMdZB2OI1Oj0GrVSvrwVv6aFNHtrWh6GNc8hcH6YrAWIO0/MgngzRTdvsn9rtOE/Lq7iGqHlxMs9vtEEEsGfFVflfvtJ/zz1rjVXweq9lAYQRYcruJWZAikHnwm1X33LsR89nurrVo2MqxyqqhEwNWezWQwYRuXAWdPe/efxP5D5L4tMPPj/d4q21j5+o4YqZnMjmJAlzdVMMO5Xl8hwulcSESTE5mQ2Fjv38le13fdWz7KL11Q3rLDVuQ41bZ/UQY5AYY3TVoMEe1WNGsKeyGsIxjHsNYC5IiBBwkh7rQsz7E7vGt+wKmZvA/gTBIlUs8Fc1BXRV+6+8Vv75S+E7b9tAkzChAPasJQQCt6GFF+364v2Tv3g1F4jIExO4ekdVE5KQ0OZekP4Sk5dZjUfx/+RBPv5LZ7rj09ek4SzZ9JZqzFKB3HEvjSgWsPEKc9Aa93Auy4gsIyQzXD4hFgTkhy8InDp2RImGEr/99Z5nn4hef3vgoqv7nAsh5gLvFyDmIJhsqs5L7FGqfouxAXE6RaLVtn6IlYjDbajymmrc5hqPpdZf6fAYa3upGvCoHmM1gPpIVfO2y67rm7+sk9BhS02geZH387fvfvLpY5s3ZcaOaN3YALrERIEFYM5KhUxBSuFeR1biUasYIfeXJffTAwGN4bZk41i8tDQR2BklC1Ymw3AZnKTnZVxxiCVD+FobY8EFnhKXR+ZKNZtSp46oRyfFSPzwa78beuS7/utv6vrkOW22hvVG63rK6a1fEqxb2mZpiX3m1vHvPbn9hlsh0O0iFX26ml6qslt74qPrAP9AdNZhtvVVtfhaL/RccF30xnu23/fQoad/evRXr050eLlde9EfqUWQ8pS2iwXC7EKaz6ZYhuE0TmLMMqY5Ni1l/9xyPyOCPKVRe8ZIEV+qq2h+tST30ko67GAB/yTAB2CzIjLT4PR/Lo1r3UUmlZ1McUnNIxTzvMrn1XRe4op8TpUkVUbe0NEPsps3HH/xp/v/6aHhr35jbPXzB773dPDuRybWu9Vcctcrq71NTeF5Fwx88pLIpy8JnntJ7PLP7LzpC/vu+NK+O75y6NHvJ15+Nd/mS3UHxR17CpOTEpvOppNqMpFjUoB6i2wBKUW4QoGVC6zEaONeuMoQYDD2YGITrciy///6fvbRxVK+v3yVeqO4aYRzcpC1PGgyXaOY7iyZ7vNH4FNyDLggUZKKCuZ48xgEMokpOc3lPpxMHj5WgGhM5I/v3bcvGBvt7D3Q6z0SjI4Nb01+MJo5/CE3Mckmk4JcEAvw34vaVYBfVbpmvsmZ15/5Qf6XQj+JI080/5WuEp4pN+F8TLkjOdiJ7p2siFaoRLWFK7dzuHtKLMhgc7EWKGn83VlOzXEqn1XFnCqLyA6FtEMFEDTym4r5E5dw8prxJmdef2VyL1uh0+Vebv34OHLXXodyx32CWK3kpg8+7uxDJwG+LpFlJhUhUcD52Ewmg1W0IhoxiJW1idzcjJo9XxqvKLdxl6+/VrmfZR3hGX8064OdbHA4Re58We4lG5XRCtOo4zmhwPEAirDLkMkkRRx7xOWOGeyPANdXGt/RHDuvbXwUCxxepZmW0jVTG3J/GY//ttzPyN5zemB1cgvyx5X79GwniFhisthll53ONsMrWbAnnICjC9qggSLkC5lsHoLJDE48MwJS97JaKypiWHgRw8+Ue9nH/Pn1eq7HfwE7ZtUgHnyphAAAAABJRU5ErkJggg==" /></div>
                  <div class="SealPercent"></div>
                </div>
              </div>
            </div>
            <div class="repeat_words">
              <div class="repWords_box1">
                <div class="repWords_row">
                  <div class="test_range"><a href="https://www.bigan.net/qa/?key=%E6%A3%80%E6%B5%8B%E8%8C%83%E5%9B%B4" target="_blank" class="green"><span class="icons inlineBlock"></span>检测范围</a></div>
                  <span>重复字数:<b class="red">6,152</b></span><span>总字数：<b>49,496</b></span> </div>
                <div class="clear"></div>
              </div>
              </div>
              <div class="clear"></div>
              <div class="similarLiter">

                <h2 style="width:100%;text-align:center;margin-top:15px;">中英文摘要等</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>摘要</p><p>智能语音交互已经成为汽车的标准化配置之一。作为其中的核心技术,<em class='similar'>自动语音识别和自然语言理解在深度学习的驱动下取得了重大进展。</em>然而,深度学习模型的高精度运行需消耗庞大的计算资源,而受制于车身结构和研发成本,当前车载语音对话系统均采用&quot;云—端&quot;方式运行,存在着一定的数据安全隐患。为解决这一问题,论文提出了基于残差分组线性变换解码器的自动语音识别模型和基于标签感知图交互的自然语言理解模型,在满足模型性能要求的前提下降低了运行所需的计算资源,并搭建了面向车载嵌入式设备的本地智能语音对话系统,</p><p>实现了数据安全、自然实时的离线智能语音交互。论文的主要工作及创新点如下:</p><p>1.针对基于深度编—解码器的自动语音识别模型参数量庞大的问题,提出了一种基于残差分组线性变换的解码器结构。该结构关键模块为&quot;钻石&quot;型缩放单元,其内部采用稀疏连接,同一组神经元共享相同的权重矩阵。所提模型在AISHELL-1数据集上的参数量和计算量分别为20.4M 和5.3B,错误率为6.67%;在 TED-LIUM2数据集上的参数量和计算量分别为20.3M 和4.6B,错误率为</p><p>11.86%,在模型识别性能和轻量化程度方面优于其他对比方法。</p><p>2.针对基于显式联合建模的自然语言理解模型交互能力不足的问题,提出了一种基于标签感知的图交互模型,其中标签映射模块可以获取原始话语与标签语义之间的相关性以提供丰富的先验知识,全局图交互模块可以对语句级别的意图—槽位交互过程进行建模以提供全局优化。所提模型在 MixATIS 和 MixSnips 数据集上的整体准确率分别为49.9%和77.3%,优于其他对比方法。</p><p>3.针对&quot;云—端&quot;方式运行的车载智能语音对话存在数据安全隐患的问题,搭建了面向车载嵌入式设备的本地智能语音对话系统。首先选取了 Nvidia Jetson TX2作为车载嵌入式设备,然后根据实际应用场景收集、创建了驾驶数据集</p><p>CQUPT-DS,<em class='similar'>接着将上述提出的两个模型在驾驶数据集上进行了训练,</em>最后集成、</p><p>移植网络模型至 TX2并围绕搭建了全套硬件平台,实现了数据安全、自然实时的离线智能语音对话。整套系统的对话通过率为97%,平均响应时间为0.87s。</p><p>关键词:<em class='similar'>车载语音交互,</em><em class='similar'>自动语音识别,</em><em class='similar'>自然语言理解,</em>嵌入式设备,<em class='similar'>对话系统</em></p><p>ABSTRACT</p><p>Intelligent voice interaction has become one of the standard configurations of cars. As the core technologies, automatic speech recognition and natural language understanding have made significant progress driven by deep learning. However, the high-precision operation of the deep learning model consumes vast computing resources. Due to the vehicle body&#39;s structure and the research cost, the current in-vehicle voice dialogue system is operated in a &quot;cloud-end&quot; mode, which has hidden dangers in data security. This thesis proposes an automatic speech recognition model based on a residual group linear transform decoder and a natural language understanding model based on label-aware graph interaction, reducing the computing resources required for operation while meeting the model performance requirements. This thesis also builds a local intelligent voice dialogue system based on in-vehicle embedded devices, realizing data security and natural real-time offline intelligent voice interaction. The main contributions</p><p>and innovations of the thesis are as follows:</p><p>1. Addressing the issue of numerous parameters in the automatic speech recognition model based on a deep encoder-decoder model, a decoder based on a residual group linear transform is proposed. The key module of this decoder is a diamond scaling unit, which uses sparse connections inside, and the same group of neurons shares the same weight matrix. The parameters and calculations of the proposed model on the AISHELL-1 dataset are 20.4M, and 5.3B, respectively, and the error rate is 6.67%; the parameters and calculations on the TED-LIUM2 dataset are 20.3M and 4.6B, respectively, with an error rate of 11.86%, outperforming other comparative methods in terms of model recognition performance and lightweight.</p><p>2. Considering the concern of insufficient interaction ability in natural language understanding models based on explicit joint modeling, a label-aware graph interaction model is proposed, which contains a label injection module and a global graph interaction module. The former can obtain the correlation between the original utterance and label semantics to provide rich prior knowledge. The latter can model the intent-slot interaction process at the sentence level to provide global optimization. The overall accuracy of the</p><p>proposed model on the MixATIS and MixSnips datasets is 49.9% and 77.3%, respectively, outperforming other comparison methods.</p><p>3. Assessing the crisis about the hidden dangers of data security in the in-vehicle intelligent voice dialogue operating in the &quot;cloud-end&quot; mode, a local intelligent voice dialogue system based on an in-vehicle embedded device is built. First, Nvidia Jetson TX2 is selected as the in-vehicle embedded device, and the driving dataset CQUPT-DS is created according to the actual application scenario. Then thesis trains the two models proposed above on the driving dataset. Finally, the network models are integrated and transplanted to TX2, and a complete hardware platform is built around it, realizing data security and natural real-time offline intelligent voice dialogue. The dialogue pass rate of the whole system is 97%, and the average response time is 0.87s.</p><p>Keywords: In-vehicle Voice Interaction, Automatic Speech Recognition, Natural</p><p>Language Understanding, Embedded Device, Dialogue System</p><p class='uncheck'>目 录 </p><p class='uncheck'>摘 要 I ABSTRACT II </p><p class='uncheck'>目 录 IV 第 1 章 绪论 1 </p><p class='uncheck'>1.1 研究背景及意义 1 </p><p class='uncheck'>1.2 国内外研究现状 2 </p><p class='uncheck'>1.2.1 自动语音识别技术 2 </p><p class='uncheck'>1.2.2 自然语言理解技术 4 </p><p class='uncheck'>1.3 论文研究内容 6 </p><p class='uncheck'>1.4 论文组织结构 7 </p><p class='uncheck'>第 2 章 对话系统基础理论 9 </p><p class='uncheck'>2.1 对话系统基本组成结构 9 </p><p class='uncheck'>2.2 神经网络基础理论 10 </p><p class='uncheck'>2.2.1 前馈神经网络 10 </p><p class='uncheck'>2.2.2 卷积神经网络 11 </p><p class='uncheck'>2.2.3 循环神经网络及其变种 12 </p><p class='uncheck'>2.3 基于编—解码器结构的自动语音识别 14 </p><p class='uncheck'>2.4 基于显式联合建模的自然语言理解 15 </p><p class='uncheck'>2.5 本章小结 16 </p><p class='uncheck'>第 3 章 基于残差分组线性变换解码器的自动语音识别 17 </p><p class='uncheck'>3.1 引言 17 </p><p class='uncheck'>3.2 Transformer 模型及其组件 17 </p><p class='uncheck'>3.2.1 自注意力机制 18 </p><p class='uncheck'>3.2.2 前馈网络层 19 </p><p class='uncheck'>3.2.3 位置编码 19 </p><p class='uncheck'>3.3 基于残差分组线性变换的"钻石"型缩放单元 20 </p><p class='uncheck'>3.3.1 分组线性变换 20 </p><p class='uncheck'>3.3.2 "钻石"型缩放单元 21 </p><p class='uncheck'>3.4 基于"钻石"型缩放单元的轻量级 Transformer 模型 22 </p><p class='uncheck'>3.4.1 轻量级前馈网络层 23 </p><p class='uncheck'>3.4.2 轻量级解码器 23 </p><p class='uncheck'>3.4.3 轻量级 Transformer 模型 24 </p><p class='uncheck'>3.5 实验结果及分析 25 </p><p class='uncheck'>3.5.1 实验环境、数据集和评价指标 25 </p><p class='uncheck'>3.5.2 数据预处理 27 </p><p class='uncheck'>3.5.3 训练配置信息及过程 28 </p><p class='uncheck'>3.5.4 对比实验结果及分析 30 </p><p class='uncheck'>3.5.5 消融实验结果及分析 32 </p><p class='uncheck'>3.6 本章小结 34 </p><p class='uncheck'>第 4 章 基于标签感知图交互的自然语言理解 35 </p><p class='uncheck'>4.1 引言 35 </p><p class='uncheck'>4.2 基于最佳线性逼近的标签映射模块 35 </p><p class='uncheck'>4.2.1 最佳线性逼近 35 </p><p class='uncheck'>4.2.2 标签映射模块 36 </p><p class='uncheck'>4.3 基于图注意力网络的全局图交互模块 37 </p><p class='uncheck'>4.3.1 图注意力网络 37 </p><p class='uncheck'>4.3.2 全局图交互模块 39 </p><p class='uncheck'>4.4 基于标签感知的图交互模型 41 </p><p class='uncheck'>4.4.1 共享编码器 41 </p><p class='uncheck'>4.4.2 意图解码器 41 </p><p class='uncheck'>4.4.3 槽位解码器 42 </p><p class='uncheck'>4.5 实验结果及分析 42 </p><p class='uncheck'>4.5.1 实验环境、数据集和评价指标 42 </p><p class='uncheck'>4.5.2 数据预处理 43 </p><p class='uncheck'>4.5.3 训练过程及配置信息 44 </p><p class='uncheck'>4.5.4 对比实验结果及分析 46 </p><p class='uncheck'>4.5.5 消融实验结果及分析 48 </p><p class='uncheck'>4.6 本章小结 50 </p><p class='uncheck'>第 5 章 面向车载嵌入式设备的本地智能语音对话系统 51 </p><p class='uncheck'>5.1 引言 51 </p><p class='uncheck'>5.2 嵌入式设备运行环境搭建 51 </p><p class='uncheck'>5.3 驾驶数据集收集 53 </p><p class='uncheck'>5.3.1 数据收集平台 53 </p><p class='uncheck'>5.3.2 数据收集过程 54 </p><p class='uncheck'>5.4 模型训练与移植 55 </p><p class='uncheck'>5.4.1 自动语音识别模型训练 55 </p><p class='uncheck'>5.4.2 自然语言理解模型训练 57 </p><p class='uncheck'>5.4.3 模型集成与移植 58 </p><p class='uncheck'>5.5 系统硬件平台搭建与测试 59 </p><p class='uncheck'>5.5.1 平台搭建 59 </p><p class='uncheck'>5.5.2 系统测试 60 </p><p class='uncheck'>5.6 本章小结 60 </p><p class='uncheck'>第 6 章 总结与展望 61 </p><p class='uncheck'>6.1 总结 61 </p><p class='uncheck'>6.2 展望 62 </p><p class='uncheck'>参考文献 63 </p><p class='uncheck'>作者简介 71 </p><p class='uncheck'>1. 基本情况 71 </p><p class='uncheck'>2. 教育和工作经历 71 </p><p class='uncheck'>3. 攻读学位期间的研究成果 71 </p><p class='uncheck'>3.1 发表的学术论文和著作 71 </p><p class='uncheck'>3.2 申请(授权)专利 71 </p><p class='uncheck'>3.3 参与的科研项目及获奖 71 </p><p class='uncheck'>致 谢 73 </p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第1章 绪论</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>1.1研究背景及意义</p><p><em class='similar'>语音交互是人机通信中最自然、</em><em class='similar'>直接的重要方式,</em>具有天然的优势[1]。随着智能汽车的兴起,车载语音交互成为汽车的标准化配置之一,是继通讯社交、智能家居之后的第三大应用场景。<em class='similar'>截止2020年9月,</em><em class='similar'>我国乘用车车载语音装配率为</em></p><p>64.8%,<em class='similar'>预计在2025年车载语音市场规模将达到32亿元</em>[2]。科大讯飞、亚马逊和谷歌等企业也相继推出了自主研发的车载语音对话系统,对车载语音交互市场的发展进程产生了显著影响[3]。作为智能语音交互的核心技术,自动语音识别</p><p><em class='similar'>(Automatic Speech Recognition,</em><em class='similar'>ASR)</em><em class='similar'>和自然语言理解</em><em class='similar'>(Natural Language Understanding,</em>NLU)在深度学习的驱动下取得了重大进展,模型性能得到了极</p><p>大提升[4]。</p><p>然而,<em class='similar'>高精度的深度学习模型需要消耗庞大的计算资源才能快速运行。</em>受限于车身结构和研发成本,当前主流的车载语音对话系统均采用&quot;云—端&quot;方式运行,即通过互联网将本地收集到的音频数据传输至配置有高性能图形处理器(Graphics Processing Unit,GPU)的云服务器进行处理,再将处理结果通过网络反馈到本地端,最后由汽车中控系统进行相应操作。事实上,此种运行方式在传输过程和云端处理过程中存在着一定的数据安全隐患,例如特斯拉间谍车、滴滴地图数据外泄等事件。为解决上述问题,实现深度学习下的 ASR 和 NLU 技术在车载平台上的高可靠性、强实时性应用,研发离线条件下的智能语音对话是有效的技术途径。</p><p>论文面向计算资源有限的车载嵌入式设备,立足于语音识别和自然语言处理领域中已有的先进成果,重点研究针对 ASR 和 NLU 的技术创新应用方法,开发相应的轻量级别神经网络模型,在满足性能要求的前提下降低其运行所需的计算资源,探索以离线方式运行的车载智能语音对话平台搭建,在本地实现数据安全、自然实时的语音对话。论文提出的语音对话方法可以推广到智能家居、智慧教育和智慧医疗等其他智能语音交互领域,丰富相关领域的技术理论,实现语音交互的价值。特别地,除了具体的 ASR 和 NLU 技术外,论文中涉及的一些关键共性技术,如模型轻量化、嵌入式移植等,亦可为研发其他依靠深度神经网络的设备提供成熟的案例支撑。</p><p>重庆邮电大学硕士学位论文</p><p>1.2国内外研究现状</p><p>1.2.1自动语音识别技术</p><p>自动语音识别技术的发展过程可以概括为三个阶段:第一阶段的主要建<em class='similar'>模方式为高斯混合-隐马尔可夫模型</em><em class='similar'>(Gaussian Mixture Model-Hidden Markov Model,</em><em class='similar'>GMM-HMM)</em>,在上世纪末应用广泛[5];在2009年,<em class='similar'>Hinton等人首次将深度信念网络</em><em class='similar'>(Deep Belief Network,</em><em class='similar'>DBN)</em>应用于声学建模并提出了深度信念网络-隐马尔可夫模型<em class='similar'>(DBN-HMM)</em>[6],在 TIMIT 的核心测试集上达到了23.0%的音素错</p><p>误率(Phoneme Error Rate,PER)。这种方法促进了深度学习在 ASR 领域的应用,</p><p>大量研究人员投入其中并取得了较大的成果[7]。至此,GMM-HMM 框架被打破,</p><p><em class='similar'>进入到以深度神经网络-隐马尔可夫模型</em><em class='similar'>(Deep Neural Network-Hidden Markov Model,</em><em class='similar'>DNN-HMM)</em>为主要方法的第二阶段;第三阶段是端到端(End-to-End,</p><p>E2E)的时代,基于 E2E 的 ASR 模型同样普遍采用深度学习的方法,特点在于不需要提前对语音帧进行对齐,可直接采用带标签的语音数据进行训练。与前两个阶段的方法相比,它更加简洁、<em class='similar'>直接且具有较强的通用性,</em><em class='similar'>能够减少研究人员对于语音、</em>语言等专业知识的依赖,<em class='similar'>大大降低了模型的建模难度。</em><em class='similar'>基于 E2E 的 ASR模型总体上可分为两类:</em><em class='similar'>一类是基于联结时序分类</em><em class='similar'>(Connectionist Temporal</em></p><p>Classification,CTC)[8]<em class='similar'>的 E2E 模型,</em><em class='similar'>另一类是基于注意力机制</em><em class='similar'>(Attention)</em><em class='similar'>的序列到序列</em><em class='similar'>(Sequence-to-Sequence,</em>S2S)<em class='similar'>模型</em>[9],二者模型结构如图1-1所示。</p><p>图1-1基于 CTC 的 E2E 模型(左)和基于 Attention 的 S2S 模型(右) Fig.1-1 CTC-based E2E model (left) and Attention-based S2S model (right)</p><p>基于 CTC 的 E2E 模型结构如图1-1左侧所示,主要由卷积神经网络</p><p><em class='similar'>(Convolutional Neural Network,</em><em class='similar'>CNN)</em><em class='similar'>或循环神经网络</em><em class='similar'>(Recurrent Neural</em></p><p>Network,RNN)编码模块和 CTC 损失函数模块组成。该模型的特点是:在无先验性语音数据对齐的情况下,能够度量输入序列和输出序列的相似性,<em class='similar'>并且能刻画语音特征序列和音素</em><em class='similar'>(字符)</em>序列的相关程度。<em class='similar'>2006年 Graves 等人利用空白字符对不同长度的序列进行对齐,</em><em class='similar'>首次提出 CTC 模型来解决 MINIST 数据集上的手</em></p><p>写数字识别和 TIMIT 数据集上的语料库音素分类问题[10],并在 TIMIT 数据集上实</p><p>现了30.51±0.19%的标签错误率(Label Error Rate,LER)。2012年,Graves 对基于 CTC 的 ASR 模型进一步改进,在 TIMIT 数据集上实现了23.2%的 PER[11]。</p><p><em class='similar'>2013年,</em><em class='similar'>Graves 等人利用多层长短时记忆</em><em class='similar'>(Long Short-Term Memory,</em><em class='similar'>LSTM)</em><em class='similar'>神经网络进行建模,</em><em class='similar'>进一步提升了模型在 TIMIT 语料库上的识别效果,</em>其 PER 为</p><p>17.7%[12]。作为第一个在 ASR 领域广泛使用的 E2E 技术,CTC 取得了较好的效果,然而其中的条件独立假设性太强,即认为每个语音(时间)片段都是相互独立的,这与实际情况不符,所以往往需要外部语言模型(Language Model,LM)来改善其条件依赖性,辅助提升模型性能。</p><p>基于 Attention 的 S2S 模型结构如图1-1右侧所示,主要由编码器(Encoder)、</p><p>注意力网络和解码器(Decoder)三部分组成。其中,Encoder 通过将输入的声学特征序列转换为更高维度的隐藏特征序列来实现与 CTC 相同的功能;Decoder 输出预测的字符序列,由于它本身是自回归式的,所以不存在条件独立假设;注意力网络则使用注意力机制将 Encoder 与 Decoder 的输出进行关联。<em class='similar'>基于 Attention的 S2S 模型于2013年由 Graves 首次提出,</em><em class='similar'>并被用于 MINIST 数据集的手写数字识别任务中</em>[13]。在此基础上,Bahdanau 等人在2014年进一步提出基于 Attention</p><p>的编—解码器模型[14]。受到上述两项研究工作的启发,Chorowski 等人将注意力</p><p>模型进一步应用于自动语音识别任务中[15],即使用注意力机制建立输入序列和输</p><p>出序列之间的对齐关系,并在目标函数中加入约束,最后在 TIMIT 的验证集和测试集上分别实现了16.88%和18.57%的 PER。至此,大量的研究人员开始投入到基于 Attention 的自动语音识别研究中,并提出多种改进模型。根据改进方法可分为三类:对 Encoder 的改进、对注意力机制的改进和对外部语言模型的改进。在对 Encoder 的改进方面,Bahar 等人采用更复杂的二维(Two-dimensional)LSTM</p><p>作为编码器模型,增加对语音时序信息的辨认能力[16]。近几年,研究者们热衷于</p><p>研究基于深度编—解码器(Transformer)的 ASR 模型。Transformer 最先由</p><p>Vaswani 等人提出[17],并被用于机器翻译领域。由于 Transformer 采用全连接网络,</p><p><em class='similar'>因而相较于 RNN 等结构,</em><em class='similar'>其训练效率高,</em><em class='similar'>模型收敛效果好</em>[18];<em class='similar'>在对注意力机制</em></p><p>重庆邮电大学硕士学位论文</p><p>的改进方面,Merboldt 等人提出局部注<em class='similar'>意力机制对注意力范围进行约束,</em><em class='similar'>同时考虑最大注意力得分并采用启发式搜索的方式对模型进行训练,</em>在 SwitchBoard 数</p><p>据集和 LibriSpeech 数据集上均取得了较好的效果[19];在对外部语言模型的改进方</p><p>面,Zeyer等人引入了以LSTM为骨干网络的语言模型,并采用字节对编码(Byte Pair Encoding)方式输出识别单元,在 LibriSpeech 数据集上取得了较好的效果[20]。</p><p>综上所述,基于 Attention 的 S2S 模型克服了在 CTC 方法中存在的问题,在ASR 领域内实现了技术突破,是未来重要的研究方向。然而该类模型,尤其是Transformer 模型,<em class='similar'>存在参数量庞大、</em><em class='similar'>计算复杂、</em><em class='similar'>难以部署等缺点。</em>例如,谷歌提</p><p><em class='similar'>出的 Conformer 模型参数量达到了1.19亿</em>[21]。<em class='similar'>这些因素为其在实际工程中的应用带来了巨大的挑战。</em><em class='similar'>因此,</em><em class='similar'>如何降低模型的参数量和计算复杂度、</em>提高模型运行速度,在计算资源有限的车载平台上实现高效的语音识别是亟待解决的问题,也是学术界当前研究的热点。</p><p>1.2.2自然语言理解技术</p><p>自然语言理解通常包含意图检测(Intent <em class='similar'>Detection,</em>ID)<em class='similar'>和槽位填充</em><em class='similar'>(Slot Filling,</em>SF)<em class='similar'>两个子任务,</em>前者属于文本分类领域,而后者属于序列标注领域。自然语言理解技术的发展过程从建模方法上可以分为两个类别:基于独立建模的方法和基于联合建模的方法[22],如图1-2所示。</p><p>图1-2 NLU 模型的三种建模方式。(a)独立建模;(b)隐式联合建模;(c)</p><p>显式联合建模</p><p>Fig.1-2 NLU modeling methods.(a) Independent modeling;(b) Implicit joint</p><p>modeling;(c) Explicit joint modeling</p><p>基于独立建模的方法如图1-2(a)所示。在 ID 子任务方面,Ravuri 和 Stolcke 在</p><p>2015年成功地将 RNN 和 LSTM 引入,表明序列特征有利于提升模型的预测准确</p><p>率[23];在SF子任务方面,常用的方法包括条件随机场(Conditional Random Fields,</p><p>CRF)、RNN 和基于 RNN 的衍生模型。2013年,Yao 采用基于 RNN 的语言模型来预测槽位标签,此外还研究了命名实体、句法特征和词类信息等其他自然语言处理的下游任务[24]。2013年,Mesnil 研究了用于 NLU 的各类 RNN 衍生模型,包括 Elman RNN、Jordan RNN 和双向 Jordan RNN 等[25]。次年,Mesnil 进一步地利</p><p>用 Viterbi 编码方式和循环 CRF 层消除了对于槽位标签的预测偏差问题[26]。上述</p><p>基于独立建模的方法虽然在 NLU 领域中取得了不错的效果,但由于单独训练的模式,ID 和 SF 两个子任务之间没有交互作用,存在共享信息泄漏的问题,导致模型性能不足。</p><p>基于联合建模的方法按两个子任务间的交互方式可分为隐式交互和显式交互,</p><p>二者分别如图1-2(b)和(c)所示。隐式联合建模的特点是采用共享编码器来捕获特征。例如,Zhang 和 Wang 在2016年引入了共享 RNN 模型来获取意图和插槽之</p><p>间的相关性[27]。同年,Hakkani-Tur 等人提出了一种用于联合建模的共享 RNN-</p><p>LSTM 架构[28]。相较于独立建模的方式,隐式联合建模对共享信息进行了整合,但这种方式较为简单,因为后续并未对真正的交互过程进行干预,导致模型可解释性不足,性能无法达到预期;而显式联合建模在捕获特征后设计了专属的交互模块,该模块可以充分共享信息,同时具有控制交互过程的特性,所以近年来越来越多的研究人员提出基于显式联合建模的方法。例如,Goo 等人提出了 Slot-</p><p>Gated 模型[29],该模型允许槽位填充可以根据学习到的意图设置相应的特殊条件;</p><p>Li 等人提出了一种新的 Self-Attention 模型[30],旨在通过意图的检测来引导槽位标签的填充;Qin 等人提出 Stack-Propagation 模型[31],更加直接地使用意图的检测结果来指导槽位标签的预测,并使用字符级别的信息来缓解二者间的误差传递;Niu 等人提出了 SF-ID 模型[32],为意图和槽位提供了一种新颖的双向关联机制;Zhang等人引入动态路径规划思想,提出了胶囊网络(Capsule-NLU)[33]并考虑到</p><p><em class='similar'>了两个子任务之间的相关性;</em><em class='similar'>Liu 等人提出了一种新型的协同记忆网络</em><em class='similar'>(CM-Net)</em>,以协作的方式从历史信息中捕获意图和槽位特征,增强局部的上下文表</p><p>示[34];Zhang 等人创新性地将 Graph(图)-LSTM 结构引入 NLU 任务中[35],不仅</p><p>克服了模型的局限性,而且可以利用意图和槽位之间的语义相关性;Qin 等人提出了 Co-Interactive Transformer 模型,该模型没有采用经典 Transformer 模型中的自注意力机制,而是额外包含了一个协同交互模块,可在两个子任务之间建立双向连接通道[36]。随着计算机计算资源的不断增加,以 BERT(Bidirectional Encoder Representation from Transformers)[37]为代表的各种预训练模型(Pre-</p><p>trained Language Models,PLMs)在 NLP 任务中取得了振奋人心的结果。例如,Chen 等人使用 BERT 提取共享文本特征用于意图检测和槽位填充,在原始模型的</p><p>基础上取得了显著的性能提升[38]。</p><p>重庆邮电大学硕士学位论文</p><p>基于隐式联合建模的方法只是通过共享字符编码的方式来隐式地考虑两个子任务之间的相关性,而基于显式联合建模的方法构建了专属的交互模块,在子任务间存在信息交互通道,使得模型性能有了进一步的提高。但是该类模型在交互模块中没有高效的交互信息融合手段,导致模型交互能力不足。虽然可以在特征提取阶段引入 PLMs 来补充丰富的语义特征,但 PLMs 的参数量往往非常惊人。</p><p>以谷歌推出的基础预训练模型 Bert-base-uncased[37]为例,该模型的网络层数为12、</p><p><em class='similar'>隐藏单元数为768、</em><em class='similar'>注意力头数为12,</em><em class='similar'>总参数量达到1.1亿,</em>如此庞大的模型难以部署于车载平台(嵌入式设备)上。因此,如何在不引入 PLMs 的前提下,构建意图和槽位的双向交互通道,高效融合所有的交互信息,深入挖掘跨任务的语义特征,提升模型的性能,在计算资源有限的车载平台上实现高效的语言理解是亟待解决的问题,也是学术界当前研究的热点。</p><p>1.3论文研究内容</p><p>针对上述问题,论文面向计算资源有限的车载嵌入式设备,重点研究针对ASR 和 NLU 的技术创新应用方法。首先提出了一种基于残差分组线性变换解码器的 ASR 模型,通过在解码器中引入残差分组线性变换,实现模型参数量和计算复杂度的大幅降低;然后提出了一种基于标签感知图交互的 NLU 模型,通过在交互模块中引入标签映射模块和全局图交互模块,实现模型交互能力和预测精度的提高;最后将 ASR 模型和 NLU 模型集成、移植至车载嵌入式设备上,根据实际</p><p>用车环境围绕搭建全套硬件平台,实现数据安全、自然实时的离线智能语音对话。</p><p>具体涉及以下研究内容:</p><p>(1)基于残差分组线性变换解码器的自动语音识别</p><p>针对基于深度编—解码器的自动语音识别模型参数量庞大的问题,提出了一种基于残差分组线性变换的解码器结构,该结构关键模块为&quot;钻石&quot;型缩放单元,其内部采用稀疏连接,同一组神经元共享相同的权重矩阵,实现模型参数量和计算复杂度的降低。</p><p>(2)基于标签感知图交互的自然语言理解</p><p>针对基于显式联合建模的自然语言理解模型交互能力不足的问题,提出了一种基于标签感知的图交互模型,其中标签映射模块可以获取原始话语与标签语义之间的相关性以提供丰富的先验知识,全局图交互模块可以对语句级别的意图—槽位交互过程进行建模以提供全局优化,实现模型交互能力和预测精度的提高。</p><p>(3)面向车载嵌入式设备的本地智能语音对话系统</p><p>针对&quot;云—端&quot;方式运行的车载智能语音对话存在数据安全隐患的问题,搭建了面向车载嵌入式设备的本地智能语音对话系统。具体地,首先选取 Nvidia Jetson TX2作为车载嵌入式设备并进行刷机、配置环境等操作,然后根据实际应用场景收集、创建驾驶数据集,接着将研究内容(1)和(2)的模型在驾驶数据集上进行训练,最后集成、移植网络模型至 TX2并围绕搭建全套硬件平台,实现数据安全、自然实时的离线智能语音对话。</p><p>1.4论文组织结构</p><p><em class='similar'>论文共分为6个章节,</em>组织结构如图1-3所示,<em class='similar'>各章节安排如下:</em></p><p>图1-3论文组织结构</p><p>Fig.1-3 Structure of the thesis</p><p>第1章,绪论。首先阐述论文的研究背景及意义,然后介绍国内外对于自动语音识别和自然语言理解领域的研究现状,针对当前存在的一些关键科学问题,</p><p>列出论文的研究内容并展示组织结构;</p><p>重庆邮电大学硕士学位论文</p><p>第2章,对话系统基础理论。首先介绍对话系统基本组成结构,同时引入深度学习领域内相关基础理论,然后在此基础上详细叙述自动语音识别和自然语言</p><p>理解的主流建模方法,为后续章节内容进行铺垫;</p><p>第3章,基于残差分组线性变换解码器的自动语音识别。首先基于残差分组线性变换构建&quot;钻石&quot;型缩放单元,然后基于缩放单元搭建改进的编—解码器模型,实现参数量和计算复杂度的大幅度降低,<em class='similar'>最后通过一系列对比实验和消融实验论证所提模型的有效性。</em></p><p>第4章,基于标签感知图交互的自然语言理解。首先基于最佳线性逼近思想构建标签映射模块,然后基于图注意力网络搭建全局图交互模块,接着融合上述两个模块形成基于标签感知的图交互模型,实现交互能力和预测精度的提高,<em class='similar'>最后通过一系列对比实验和消融实验论证所提模型的有效性。</em></p><p>第5章,面向车载嵌入式设备的本地智能语音对话系统。首先对车载嵌入式设备进行选型,然后根据实际应用场景收集、创建驾驶数据集,接着将前文提出的自动语音识别模型和自然语言理解模型在驾驶数据集上进行训练,最后将训练好的网络模型集成、移植至车载嵌入式设备上并围绕搭建硬件平台,并完成系统测试以论证其有效性。</p><p><em class='similar'>第6章,</em><em class='similar'>总结与展望。</em><em class='similar'>总结论文研究内容,</em><em class='similar'>并对后续研究工作做出展望。</em></p><p>Equation Chapter 2 Section (Next)</p><p>第2章对话系统基础理论</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第2章 对话系统基础理论</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>2.1对话系统基本组成结构</p><p>由于能提供人类与机器之间自然、简洁的交流方式,对话系统引起了众多研</p><p>究人员的广泛关注。<em class='similar'>现有对话系统按照应用场景和目标的不同,</em><em class='similar'>主要可分为两类:</em></p><p><em class='reference'>任务驱动型和非任务驱动型[39]。前者主要用于辅助用户完成一些特定的任务,后者也可被称为开放域对话系统或聊天机器人,主要用于与用户进行闲聊。</em><em class='reference'>此外,按照对话系统在给出确定的回复话语前需考虑的轮数还可以将其分为单轮对话系统和多轮对话系统。前者简化了对话过程,</em>后者综合考虑了历史对话中的上下文信息,<em class='reference'>能给出更加符合逻辑的回复话语。值得注意的是,不管是闲聊对话系统还是任务驱动型对话系统都可以考虑单轮或多轮对话,</em>而且在实际应用中,二者经常混合出现,并非是截然分离的[40]。</p><p>早年的任务驱动型对话系统大多基于人工设计的模板和规则来实现,这种方法不仅耗时耗力,而且限制了应用领域的扩展。近年来,由于深度学习的不断迭代更新,研究人员热衷于研究基于深度学习的任务驱动型对话系统,这种方法可以通过捕获高维特征来缓解这些问题。目前,<em class='similar'>基于深度学习的任务驱动型对话系统有两种构建方式:</em><em class='similar'>Pipeline</em>(流水线)<em class='similar'>方式和端到端方式</em>[41]。前者是业界主流方</p><p>法,而后者由于在可控性方面存在问题,当前还处于探索阶段[42]。</p><p>基于 Pipeline 方法的对话系统如图2-1所示,主要包含自动语音识别、<em class='similar'>自然语言理解、</em><em class='similar'>对话状态跟踪、</em><em class='similar'>对话策略学习、</em><em class='similar'>自然语言生成和语音合成等6个模块。</em>其中,ASR 将用户语音转换为系统可接收的文本输入,NLU 将文本输入解析为包含意图和槽位的结构化数据。它们为下游模块提供了必要的信息,同时也是对话系统的重要支撑,故论文主要研究针对 ASR 和 NLU 的技术创新应用方法。</p><p>图2-1基于 Pipeline 方式的对话系统</p><p>Fig.2-1 Dialogue system based on pipeline mode</p><p>重庆邮电大学硕士学位论文</p><p>2.2神经网络基础理论</p><p>基于深度学习的 ASR 和 NLU 模型广泛采用各种不同类型的神经网络拓扑结</p><p>构,以提高模型的性能。在这方面,前馈神经网络(Feedforward Neural Network,</p><p>FNN)、卷积神经网络和循环神经网络及其各种变体都是常见的选择。因此,在介绍常用的 ASR 和 NLU 建模方法之前,本节将先介绍这些神经网络的基本概念和原理。</p><p>2.2.1前馈神经网络</p><p>FNN[43]是最早被设计提出的人工神经网络之一,也是卷积神经网络和循环神</p><p>经网络的基本组成单元。以经典的三层 FNN 为例,其中含有众多按层排列的神经元,共分为三层,按照输入—输出顺序分别为<em class='similar'>输入层</em>、<em class='similar'>隐藏层和输出层,</em><em class='similar'>前一层的每个神经元会向后一层中所有与之相连的神经元传递信号,</em><em class='similar'>如图2-2所示。</em><em class='similar'>由于这些神经元都是全连接的,</em><em class='similar'>故前馈神经网络也被称为全连接神经网络。</em></p><p>图2-2三层前馈神经网络结构</p><p>Fig.2-2 Structure of three-layer FNN</p><p>FNN 中信息前向传播的一般形式如下:</p><p>(2-1)</p><p>式中,表示当前层数;<em class='similar'>表示第层神经元的输出向量;</em><em class='similar'>表示第层到第层的权重矩阵;</em><em class='similar'>表示第层神经元的激活函数;</em><em class='similar'>表示第层的偏置向量。</em>如果加入更多神经元构造更多层数,FNN 就能更好地对输入—输出关系进行建模,从而获得更好的性能表现。本质上,训练神经网络就是通过一些特定的算</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 2 章 对话系统基础理论</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>法不断迭代更新参数和,使得模型在某一组参数配置下对输入数据达到最优的拟合性。目前最常见的参数更新算法是误差反向传播算法[44],具体步骤是:首先根据预测结果与实际结果的偏差计算损失函数值,<em class='similar'>然后使用链式法则从输出层往输入层方向</em><em class='similar'>(反向)</em><em class='similar'>逐层求取各参数梯度,</em><em class='similar'>最后将学习率</em>(百分比,影响训练过程的速度和效果)与所求梯度相乘以更新参数。这些步骤可以反复循环迭代,<em class='similar'>直至网络输出的误差减少到可接受的程度或达到设定的最大训练次数。</em></p><p>2.2.2卷积神经网络</p><p><em class='similar'>卷积神经网络</em>[45]<em class='similar'>是一种包含卷积操作的 FNN,</em><em class='similar'>一般包含卷积层、</em>池化层和全</p><p>连接层三部分,相较于 FNN,<em class='similar'>它能对提取的特征进行有效的降维,</em><em class='similar'>从而减少模型的计算量。</em><em class='similar'>卷积核</em><em class='similar'>(卷积滤波器)</em>是CNN的关键部件,<em class='similar'>它是一个可定义权值的函数,</em><em class='similar'>主要作用是将输入图像上某个区域中的每一个像素进行加权平均并作为输出图像中的对应像素。</em>一个简单的卷积操作如图2-3所示。</p><p>图2-3卷积神经网络中的卷积操作</p><p>Fig.2-3 Convolution operation in CNN</p><p>具体地,首先CNN通过将卷积核在输入图像(三通道)<em class='similar'>上按特定顺序移动,</em><em class='similar'>扫描视野范围内的像素信息来提取特征,</em>这些特征是较小尺寸的单通道图片,被</p><p>称作特征图;然后在非线性激活函数的作用下,将所有特征图转化为激活特征图;<em class='similar'>最后对激活特征图在通道维度上按顺序进行堆叠,</em><em class='similar'>从而得到卷积操作的输出结果。</em><em class='similar'>通过对卷积核数、</em><em class='similar'>尺寸等超参数进行合适的设置,</em>可实现CNN对图像信息特征的扩展与丰富。</p><p>重庆邮电大学硕士学位论文</p><p>2.2.3循环神经网络及其变种</p><p>循环神经网络[46]<em class='similar'>是一种擅于处理时间序列数据的人工神经网络,</em>其网络拓扑</p><p>结构在时间维度上以递归形式展开,如图2-4所示。</p><p>图2-4循环神经网络拓扑结构</p><p>Fig.2-4 Topology of RNN</p><p>RNN 的一个主要特点是:当前时刻的输入包含数据输入和前一时刻计算得到的隐藏状态两部分。在时刻,RNN 接收的输入由当前时<em class='similar'>刻的数据输入和前一时刻的隐藏状态两部分组成,</em><em class='similar'>产生当前时刻的隐藏状态和输出,</em>计算过</p><p>程如下:</p><p>(2-2)</p><p>(2-3)</p><p><em class='similar'>上式中,</em><em class='similar'>、和均为权重矩阵;</em><em class='similar'>和均为非线性激活函数。</em></p><p><em class='similar'>RNN 因其网络拓扑结构在处理时序信息方面具有天然的优势,</em>但在反向传播过程中,随着时间推移,所产生的梯度可能会逐渐放大(或缩小)。当RNN在处理具有长时间依赖性的序列数据时,这种现象尤为明显,会使得梯度放大(或缩小)的程度趋近于指数级,即梯度爆炸(或梯度消失)现象,导致网络模型权重溢出(或无法更新)。</p><p>为了解决这一问题并提高RNN的性能,近年来许多研究人员已经提出了改进</p><p>措施,并取得了显著的成果。以长短时记忆神经网络[47]为例,它是一种改进的</p><p>RNN,其基本结构如图2-5所示。<em class='similar'>LSTM 引入了门控机制,</em><em class='similar'>包括遗忘门、</em><em class='similar'>输入门和输出门,</em><em class='similar'>这些门控单元能够有效地缓解梯度爆炸和梯度消失问题。</em><em class='similar'>下面结合LSMT 的前向计算过程,</em>阐述这些门控单元的作用。</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 2 章 对话系统基础理论</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p><em class='similar'>图2-5长短时记忆神经网络基本结构</em></p><p>Fig.2-5 Basic structure of LSTM</p><p>遗忘门决定当前时刻哪些输入会被丢弃。具体地,与RNN类似,它将前一时刻的隐藏状态和当前时刻的数据输入拼接作为当前时刻的真正输入,利用非线性激活函数将输出的范围固定为,这个值越大说明遗忘门越倾向于</p><p>保留该输出,计算过程如下:</p><p>(2-4)</p><p><em class='similar'>输入门产生当前时刻的输入映射,</em><em class='similar'>计算过程如下:</em></p><p>(2-5)</p><p>随后进行状态更新操作,即将与候选状态更新到网络状态中,计算过程</p><p>如下:</p><p>(2-6)</p><p>(2-7)</p><p>输出门产生输出,并将其与前一时刻的单元状态交互得到当前时刻的</p><p>隐藏状态,计算过程如下:</p><p>(2-8)</p><p>(2-9)</p><p>上式中,表示 Sigmoid 函数;<em class='similar'>和分别表示权重矩阵和偏置向量;</em><em class='similar'>表示按元素相乘。</em></p><p>LSTM 通过上述三个门控单元来控制网络状态,即&quot;记住&quot;重要信息而&quot;忘记&quot;不重要的信息,适用于需要处理较长间隔和延迟的时间序列数据的场景。</p><p>重庆邮电大学硕士学位论文</p><p>2.3基于编—解码器结构的自动语音识别</p><p>编—解码器结构最初用于机器翻译领域[48],该结构同样也适用于自动语音识</p><p>别领域,主要有以下两点原因:</p><p>(1)自动语音识别和机器翻译本质上都属于&quot;序列—序列&quot;任务,即将输</p><p>入序列转化为输出序列;</p><p>(2)编码器输出的向量序列不再局限于固定长度,因此可以处理较长的语音输入序列。</p><p>编—解码器结构包含<em class='similar'>编码器</em>、<em class='similar'>注意力网络和解码器三部分,</em><em class='similar'>如图2-6所示。</em><em class='similar'>其中,</em>编码器将输入特征序列映射为更高维度的隐藏特征序列,解码器接收这个序列并输出预测序列,<em class='similar'>注意力机制则通过计算输入帧和的匹配分数来完成输入序列和输出序列的对齐,</em><em class='similar'>匹配分数的高低代表这个输入帧和的相关程度。</em>通常,输出特定时所需的信息仅依赖于少数关键输入帧,因此在解码阶段不需要考虑每一个输入帧。</p><p>图2-6编—解码器结构</p><p>Fig.2-6 Structure of Encoder-Decoder</p><p><em class='similar'>以使用加性注意力机制的注意力网络为例,</em><em class='similar'>首先计算解码器前一时刻的状态和编码器某一时刻的状态之间的相关程度,</em><em class='similar'>即注意力得分,</em>过程如下:</p><p>(2-10)</p><p>式中,和分别为可训练的权值向量和偏置向量;和均为可训练的权重矩阵。</p><p><em class='similar'>然后使用 Softmax 函数进行归一化,</em><em class='similar'>并进行加权求和得到向量,</em>计算过程</p><p>如下:</p><p>(2-11)</p><p>(2-12)</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 2 章 对话系统基础理论</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p><em class='similar'>最后解码器根据、</em><em class='similar'>前一时刻的输出向量和状态向量计算得到当前时刻的状态向量和输出向量,</em>再经过 Softmax函数之后得到输出序列,过程如</p><p>下:</p><p>(2-13)</p><p>(2-14)</p><p>式中,表示与节2.2.3中类似的计算方式。</p><p>通常,为了从不同角度获取关于输入的各种层次的信息,注意力网络中会使</p><p>用不同<em class='similar'>的注意力机制</em>,<em class='similar'>目前使用较广泛的注意力机制还有缩放点积注意力机制</em>[49]、</p><p><em class='similar'>多头注意力</em><em class='similar'>(Multi-Head Attention,</em><em class='similar'>MHA)</em>机制[17]<em class='similar'>和局部注意力机制</em>[50]等。基于</p><p>这些不同的注意力机制,衍生出了各种类型的编—解码器结构,如 LAS 模型[51]、</p><p>Transformer 模型等。其中,Transformer 模型因极高的识别性能和便捷的建模方式</p><p>受到了广泛关注[52]。<em class='similar'>论文第三章将对Transformer模型进行详细探讨,</em><em class='similar'>并在此基础上进行改进。</em></p><p>2.4基于显式联合建模的自然语言理解</p><p><em class='similar'>在自然语言理解领域,</em><em class='similar'>考虑到意图检测和槽位填充两个子任务之间的密切相关性,</em>主要采用联合建模的方式来利用两个子任务间的共享特征,主要可分为隐式联合建模和显式联合建模两种。前者仅采用共享编码器直接整合共享信息,而后者捕获特征后依旧通过交互模块进行交互,可以充分共享信息,从而提升模型性能,同时明确其交互过程有助于提高模型可解释性[22]。</p><p>以文献[53]提出的模型为例,其结构如图2-7所示,主要包含两个相互连接的双向 LSTM(Bi-LSTM),分别用于意图检测和槽位填充。每个 Bi-LSTM 双向读取输入序列并生成一系列隐藏状态,其中对应用于意图检测的网络,对应用于槽位填充的网络。</p><p>具体地,在意图检测的网络中,隐藏状态与来自另一个 Bi-LSTM()</p><p>产生的隐藏状态连接,计算时刻上的状态,最后输出预测的意图标签</p><p>,其计算过程如下:</p><p>(2-15)</p><p>(2-16)</p><p>式中,表示所有意图标签在最后一个时刻上的预测概率。</p><p>重庆邮电大学硕士学位论文</p><p>图2-7简单的显式联合建模</p><p>Fig.2-7 Simple explicit joint modeling</p><p>在槽位填充的网络中,计算过程与上式类似。区别之处在于,由于属于序列</p><p>标注领域,在每个时刻都有一个输出,计算过程如下:</p><p>(2-17)</p><p>(2-18)</p><p>式中,表示在时刻上的预测槽位标签。上述例子是一个经典的显式联合建模方法实例,论文第四章将在此方法基础上进行改进。</p><p>2.5本章小结</p><p>本章作为论文的基础理论章节,<em class='similar'>主要包含三部分。</em><em class='similar'>第一部分主要介绍了对话系统的基本组成结构,</em>然后阐述了常用的几种深度神经网络;第二部分主要介绍了基于编—解码器结构的自动语音识别方法;第三部分主要介绍了基于显式联合建模的自然语言理解方法。<em class='similar'>这一章节为后续第3章、</em><em class='similar'>第4章的内容做下铺垫。</em></p><p>Equation Chapter (Next) Section (Next)</p><p>第3章基于残差分组线性变换解码器的自动语音识别</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第3章 基于残差分组线性变换解码器的自动语音识别</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>3.1引言</p><p>近年来,因具有出色的语音序列建模能力和强大的识别能力,Transformer 模型在自动语音识别任务中表现出了优秀的性能并受到了广泛的关注。需要注意的是,<em class='similar'>Transformer 模型的性能通常与编码器块和解码器块的深度呈正相关。</em><em class='similar'>此外,</em><em class='similar'>更大的模型维度也会带来更高的模型性能和更强的泛化能力。</em>然而这些都会使得模型中权重矩阵的维度的增加,进而导致模型参数量和计算复杂度的剧烈增长。例如,Pham 等人提出的 Transformer 变种模型堆叠了48个编码器块和解码器块,</p><p>其参数量达到252M[54];Synnaeve 等人构建了一<em class='similar'>个含有24个编码器块的Transformer 模型,</em><em class='similar'>在模型维度为1024的情况下参数量达到了270M</em>[55]。<em class='similar'>为了降低Transformer 模型的参数量和计算复杂度,</em>研究人员做出了各种尝试。Luo 等人提</p><p>出了一种简化的自注意力机制[56],使用前馈顺序记忆网络[57]代替映射层以实现状</p><p>态序列的转换,使得模型参数量下降了20%。但是,这种简单的替换并不能使得模型表现出良好的性能,而且很少有相关研究将这种轻量级网络应用于实际。为解决上述问题,本章工作在原始 Transformer 模型的基础上,侧重于以识别性能的小幅度降低为代价,实现模型参数量和计算复杂度的降低,具体地,通过将机器翻译领域提出的 DeLighT 模块[58]改进、嵌入至解码器中,使得模型在训练过程中可以适应向量维度和深度的变化,从而在增加宽度和深度的同时能减少参数量和计算复杂度。</p><p>3.2 Transformer 模型及其组件</p><p><em class='similar'>Transformer 模型是一个典型的编—解码器结构,</em><em class='similar'>如图3-1所示。</em><em class='similar'>左侧为编码器,</em><em class='similar'>由若干个块堆叠而成,</em><em class='similar'>每个块中包含两个子层,</em><em class='similar'>分别为多头注意力</em><em class='similar'>(Multi-head Attention,</em>MHA)<em class='similar'>层和前馈网络层</em>(Feedforward Network Layer,FNL);<em class='similar'>右侧为解码器,</em><em class='similar'>其结构与编码器类似。</em>不同之处在于,<em class='similar'>解码器将多头注意力层替换为掩膜多头注意力层,</em><em class='similar'>并在之后额外级联了一个多头注意力层以计算预测字符与语音特征间的相关性。</em><em class='similar'>所有子层之间都使用残差连接进行耦合,</em><em class='similar'>并在各子层之后使用归一化对该层输出进行调整。</em>下面对 Transformer 模型中的几个重要组件进行</p><p>简介。<em class='similar'>值得注意的是,</em><em class='similar'>在较新的文献中,</em><em class='similar'>常用&quot;线性层&quot;指代&quot;全连接层&quot;</em>[59]。<em class='similar'>因此,</em><em class='similar'>后续对这两种说法不加区分。</em></p><p>重庆邮电大学硕士学位论文</p><p>图3-1 Transformer 模型结构</p><p>Fig.3-1 Structure of Transformer</p><p>3.2.1自注意力机制</p><p><em class='similar'>Transformer 模型频繁地使用自注意力</em><em class='similar'>(Self Attention,</em>SA)<em class='similar'>机制,</em><em class='similar'>从不同的表示子空间中获取丰富的信息。</em>SA则使用点积运算操作来获取序列中任意位置上元素之间的相关性。</p><p><em class='similar'>在 Transformer 模型中,</em><em class='similar'>模型维度通常指各模块输出向量的维度,</em>取值通常为128、256或512等2的整数幂。假设一个长度为、模型维度为的序列</p><p>为 SA 在表示子空间上的输入。在中,SA 首先使用由三个不同的权重矩阵将映射到向量维度为的三个序列,计算过程如下:</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 3 章 基于残差分组线性变换解码器的自动语音识别</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>(3-1)</p><p><em class='similar'>式中,</em><em class='similar'>,分别表示查询</em><em class='similar'>(Query)</em><em class='similar'>向量序列、</em><em class='similar'>键(</em><em class='similar'>Key)</em><em class='similar'>向量序列和值</em><em class='similar'>( Value )</em><em class='similar'>向量序列;</em>,分别为对应的权重矩阵。</p><p>然后将和的转置进行点积运算并使用进行缩放,便于产生的梯度能顺利地进行反向传播,接着使用 Softmax 函数进行概率化表示,<em class='similar'>进而得到二者之间的注意力权重矩阵,</em><em class='similar'>最后将该矩阵作用于即可得到的输出结果,</em></p><p>计算过程如下:</p><p>(3-2)</p><p>在此基础上,将至中的各个输出结果按维度进行拼接,通过一个线性层</p><p>即可得到 MHA 的输出结果,计算过程如下:</p><p>(3-3)</p><p>式中,表示拼接操作;,表示输出权重矩阵。</p><p>3.2.2前馈网络层</p><p>FNL 的主要作用就是增强模型的拟合能力,<em class='similar'>本质上它包含两层全连接网络,</em><em class='similar'>第一层将扩张到更大的维度,</em><em class='similar'>第二层再将维度进行还原,</em>其前向传播的计算过</p><p>程如下:</p><p>(3-4)</p><p>式中,<em class='similar'>表示激活函数;</em><em class='similar'>,分别表示两个全连接网络的权重矩阵;</em><em class='similar'>,分别表示两个全连接网络的偏置向量。</em></p><p>3.2.3位置编码</p><p><em class='similar'>由于 Transformer 模型内部没有循环结构,</em><em class='similar'>无法像 RNN 一样利用天然包含的时序位置信息,</em><em class='similar'>注意力机制会在不同的上下文环境中输出相同的状态序列,</em><em class='similar'>导致位置信息完全丢失。</em>而输<em class='similar'>入的语音序列和输出的字符序列都属于时间序列数据,</em><em class='similar'>具有特定的排列顺序且富含上下文语义信息,</em><em class='similar'>所以在序列中加入位置信息有助于增强模型表示序列内部关系的能力,</em>且利于模型的训练和学习。</p><p>基于以上原因,Transformer 模型中引入了位置编码(Positional Encoding,PE),<em class='similar'>将 PE 与输入语音序列进行加性组合,</em><em class='similar'>使得序列中附带关于序列内部的位</em></p><p>重庆邮电大学硕士学位论文</p><p>置信息。具体地,假设输入序列长度为,表示第个时刻维度为的 PE</p><p><em class='similar'>向量,</em><em class='similar'>其中每个元素的计算方式如下:</em></p><p>(3-5)</p><p>对于输入序列中的每个向量,<em class='similar'>线性加上对应的 PE 向量即可得到含有位置信息的向量。</em></p><p>3.3基于残差分组线性变换的&quot;钻石&quot;型缩放单元</p><p>本节首先介绍分组线性变换的特点,给出其计算过程,然后在此基础上加入残差连接,构建包含维度扩张和维度收缩两个阶段的&quot;钻石&quot;型缩放单元,最后给出其数学定义和拓扑描述。</p><p>3.3.1分组线性变换</p><p>分组线性变换(Grouped Linear Transformation,GLT)是一种特殊的线性变换方式,<em class='similar'>它是将后一层中的部分神经元与前一层中的部分神经元全连接,</em>如图</p><p>3-2所示。GLT 通常被用于对全连接层权重进行分组处理,以减少模型参数量和计算量,同时也可以增强模型对局部特征的提取能力。</p><p>图3-2分组线性变换</p><p>Fig.3-2 Group linear transformation</p><p>具体地,<em class='similar'>通过在不同的网络层中设定神经元被划分的组数,</em><em class='similar'>可以控制整个网络模型的参数量。</em>例如,<em class='similar'>对于一个层全连接网络,</em>假设与分别表示第</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 3 章 基于残差分组线性变换解码器的自动语音识别</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>层的输入和输出,表示引入 GLT 时第层的划分组数,那么采用普通线性变换和 GLT 时模型参数量分别为和。可见,引入分组线性变换后,参数量可下降至原始的。</p><p>采用 GLT 时,每个神经元都能通过多种途径到达输入向量中的某个输入神经元,以降低流动过程中的信息损失,使得模型能够以相对较少的参数量进行有效地学习。具体地,<em class='similar'>假设输入向量和输出向量分别为和,</em><em class='similar'>GLT 首先根据维度和构建个维度逐渐增加的中间层,</em><em class='similar'>若需要进行变换的向量维度</em></p><p>能被最大组数整除,某一层的输出向量的计算方式如下:</p><p>(3-6)</p><p>式中,表示分组线性变换操作;<em class='similar'>表示第层的可训练的权重矩阵;</em>表示第层的划分组数,值为。</p><p>3.3.2&quot;钻石&quot;型缩放单元</p><p>为了使得网络更容易训练和学习,在 GLT 中加入残差连接,可以形成如图</p><p>3-3(a)所示的残差分组线性变换( Residual Grouped Linear Transformation,RGLT),<em class='similar'>在此基础上可以构建包含维度扩张和维度收缩两个阶段的&quot;钻石&quot;型缩放单元,</em>如图3-3(b)所示。<em class='similar'>缩放单元由5个配置参数决定:</em><em class='similar'>深度</em><em class='similar'>(层数)</em><em class='similar'>、宽度因子、</em><em class='similar'>输入维度、</em><em class='similar'>输出维度和分组线性变换的最大组数。</em></p><p>图3-3&quot;钻石&quot;型缩放单元的结构。(a)残差分组线性变换;(b)拓扑结</p><p>构;(c)逐块缩放策略</p><p>Fig.3-3 Structure of the &quot;diamond&quot; scaling unit.(a) Residual grouped linear</p><p>transformation;(b) Topology;(c) Block-wise scaling strategy</p><p>重庆邮电大学硕士学位论文</p><p>RGLT 包含切分混合和 GLT 两步操作,GLT 的输入为原始输入序列或中间层输出结果,表示切分混合函数。<em class='similar'>以第2层为例,</em><em class='similar'>首先将上一层的输出和原始输入根据该层组数分别按相同规律切分,</em>得到、、和</p><p>等4个向量;<em class='similar'>然后按特定顺序合并和、</em>和,<em class='similar'>组合得到的两个向量即为的输出;</em>最后经过 GLT 得到第2层的输出。第层的输出的计算过程如</p><p>下:</p><p>(3-7)</p><p>(3-8)</p><p>上式中,表示 GLT;和分别表示第层的权重矩阵和偏置向量。</p><p>基于 RGLT 构建的&quot;钻石&quot;型缩放单元在扩张阶段,<em class='similar'>将维度为的输入向量经过前层映射到最高维度为的向量,</em>使网络模型具有比较强大的学习能力;<em class='similar'>在收缩阶段,</em><em class='similar'>将最高维度向量经过剩下的层变换为输出维度,</em>进而减少整个模型的参数量。</p><p><em class='similar'>对于某些层数较深的网络,</em>可以基于逐块缩放策略(如图3-3<em class='similar'>(c)</em>所示)<em class='similar'>在其组件中嵌入缩放单元,</em><em class='similar'>以适应其训练过程中向量维度的变化。</em>每一个六边形表示一个缩放单元,它们的形状各不相同,对于处于位置上的缩放单元,其配置参</p><p>数和的计算方式如下:</p><p>(3-9)</p><p>(3-10)</p><p>上式中,和均<em class='similar'>为超参数</em>,<em class='similar'>分别表示最小深度和最大深度;</em>表示缩放单元总个数。</p><p>3.4基于&quot;钻石&quot;型缩放单元的轻量级 Transformer 模型</p><p>本节在原始 Transformer 模型基础上,首先将普通前馈网络层改进为轻量级前馈网络层,然后将上节所述&quot;钻石&quot;型缩放单元嵌入到解码器中,形成轻量级解码器,最终得到改进的、轻量级 Transformer 模型。</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 3 章 基于残差分组线性变换解码器的自动语音识别</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>3.4.1轻量级前馈网络层</p><p>轻量级 FNL 由两层线性层构成,与普通 FNL 类似,区别之处在于它不需要对输入序列的维度进行扩张,其结构如图3-4右侧所示。普通 FNL 需要先将序列维度扩张倍,而轻量级 FNL 首先将序列维度从转变为,然后将序列维度还原为。那么在同等条件下,轻量级 FNL 的参数量是普通 FNL 的。另</p><p>外,轻量级 FNL 中选用的激活函数为最近提出的 MISH 函数[60],相比于普通 FNL</p><p><em class='similar'>中的 ReLU 函数,</em><em class='similar'>它所求得的梯度更加平滑,</em><em class='similar'>更有利于模型进行训练。</em></p><p>图3-4普通前馈网络层(左侧)和轻量级前馈网络层(右侧)的结构 Fig.3-4 Structure of the normal FNL (left) and the lightweight FNL (right)</p><p>3.4.2轻量级解码器</p><p>通常,提升Transformer模型的性能可以考虑宽度扩张和深度扩张两种方法[61],</p><p>前者是增加向量维度及映射维度,后者是增加模型的深度。然而,这些方法在某些体量较小的数据集上的表现差强人意。例如,针对 THCHS-30数据集[62],将</p><p>Transformer 模型的模型维度扩大一倍,其参数量扩大了4倍,但识别性能变化不大。而如果在各模块中均匀地增加模型深度,同样会导致参数量的急剧增加。为了有效地减少 Transformer 模型的参数量,使用逐块缩放策略在不同的解码器块中嵌入深度和宽度不均匀的缩放单元,并将普通 FNL 替换为改进的轻量级FNL,形成轻量级解码器块,可以使得模型在适应宽度和深度增加的同时进一步减少参数量,其结构如图3-5所示。</p><p>图3-5轻量级解码器结构</p><p>Fig.3-5 Structure of the lightweight decoder</p><p>重庆邮电大学硕士学位论文</p><p><em class='similar'>在每个轻量级解码器块中,</em><em class='similar'>首先使用缩放单元完成输入序列的深度和宽度缩放,</em><em class='similar'>而深度因子和宽度因子将逐块增加,</em>所以缩放单元的深度和宽度会随着其靠近输出端的程度而增加;<em class='similar'>此外其中的注意力机制与普通解码器中的注意力机制有所不同,</em><em class='similar'>普通注意力机制使用三个权重矩阵来减小输入序列的维度,</em><em class='similar'>而在轻量级解码器块中,</em><em class='similar'>由于缩放单元的输出向量维度较低,</em><em class='similar'>所以其注意力机制不需要再次减小输入序列的维度,</em><em class='similar'>只需通过线性层将序列维度映射至模型维度,</em>这种机制同样适用于之后的编—解码器注意力。</p><p>3.4.3轻量级 Transformer 模型</p><p><em class='similar'>在原始 Transformer 模型的基础上,</em><em class='similar'>采用上述内容对解码器部分进行优化,</em>即</p><p>用若干个轻量级解码器块堆叠形成解码器,得到改进的、轻量级 Transformer 模型,其结构如图3-6所示。</p><p>图3-6轻量级 Transformer 模型结构</p><p>Fig.3-6 Structure of the lightweight Transformer</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 3 章 基于残差分组线性变换解码器的自动语音识别</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>3.5实验结果及分析</p><p>本节首先介绍实验环境、使用的数据集和评价指标,然后阐述语音信号的频域特征提取方法,并给出详细的语音信号及标签文本预处理流程,接着描述网络模型的训练配置信息及过程,最后通过对比实验结果分析改进模型的性能,同时设计一系列消融实验论证所提方法的有效性。</p><p>3.5.1实验环境、数据集和评价指标</p><p>1)实验环境</p><p>实验环境包含硬件和软件两部分。<em class='similar'>在硬件方面,</em><em class='similar'>主要使用一台高性能服务器来完成所有模型的训练;</em>在软件方面,所有实验都在 Python 3.6.9及 PyTorch 1.6环境下进行。实验环境的详细信息见表3-1。</p><p>表3-1实验环境信息</p><p>Table3-1 Experimental environment</p><p>类别类型型号规格</p><p>硬件</p><p>CPU Intel(R) Xeon(R) CPU E5-2680 v4@2.40GHz 2</p><p>GPU NVIDIA Tesla P10016GB×4</p><p>内存 ECC Registered DDR4@32GB 32GB×4硬盘西部数据 SSD 阵列21TB</p><p>麦克风 Actins ATS3605D</p><p>软件</p><p>操作系统 Ubuntu 18.04</p><p>编程语言 Python 3.6.9</p><p>深度学习框架 PyTorch 1.6</p><p>2)数据集</p><p>实验使用在自动语音识别领域内应用较为广泛的 AISHELL-1数据集[63](开源,中文)和 TED-LIUM2数据集[64](开源,英文)。其中,AISHELL-1包含近</p><p>170小时的语音数据,语音段数约14万,来自于400个来自中国不同口音区域的发言人,<em class='similar'>内容涉及智能家居、</em><em class='similar'>无人驾驶和工业生产等11个领域;</em>TED-LIUM2包含约207小时的语音数据,语音段数约9万,来自于1242个发言人的1495段演讲的音频和文字稿,演讲视频全部来自于 TED 网站。</p><p>AISHELL-1和 TED-<em class='similar'>LIUM2都按照一定的比例被划分为训练集</em><em class='similar'>(Train)</em><em class='similar'>、验证集</em>(Val)<em class='similar'>和测试集</em><em class='similar'>(Test)</em>三个子集,<em class='similar'>前者的划分比例参照文献</em>[63],后者的划分比例参照文献[65],具体信息见表3-2。另外,AISHELL-1中发言人的年龄、性别和方言信息见表3-3。</p><p>重庆邮电大学硕士学位论文</p><p>表3-2 AISHELL-1和 TED-LIUM2的详细信息 Table3-2 Detail of AISHELL-1 and TED-LIUM2</p><p>数据集子集语音段数时长(小时)</p><p>AISHELL-1</p><p>Train 120098150</p><p>Val 1432610</p><p>Test 72005</p><p>TED-LIUM2</p><p>Train 91262203</p><p>Val 5801.5</p><p>Test 11342.5</p><p>表3-3 AISHELL-1中发言人的年龄、性别和方言信息</p><p>Table3-3 The age, gender, and accent of speakers in AISHELL-1</p><p>年龄、性别信息方言信息</p><p>年龄段人数</p><p>比例</p><p>(%)</p><p>男性人数女性人数区域人数</p><p>比例</p><p>(%)</p><p>16—25岁31679140176北方33383</p><p>26—40岁71183635南方3810</p><p>40岁以上133103闽、贵、粤及其他297</p><p>合计400100186214400100</p><p>3)评价指标</p><p>在模型识别性能方面,论文选用常用的标签错误率作为评价指标[66]。对于采用不同建模单位的数据集,评价指标的称呼不同,但计算方式类似。对于以单字</p><p>符作为基本建模单位的 AISHELL-1,其评价指标为字错误率(Character Error Rate,</p><p>CER);对于以一元模型(Unigram)作为基本建模单位的 TED-LUM2,其评价</p><p><em class='similar'>指标为词错误率</em><em class='similar'>(Word Error Rate,</em><em class='similar'>WER)。</em><em class='similar'>CER 和 WER 的计算方式如下:</em></p><p>(3-11)</p><p>式中,、和分别表示相较于正确文本所替换、增加和删除<em class='similar'>的字符</em><em class='similar'>(单词)</em><em class='similar'>数;</em><em class='similar'>表示正确文本的总字符</em><em class='similar'>(单词)</em>数。</p><p>在模型轻量化程度方面,论文选用参数量(Parameters)和计算量作为评价指标。对于前者,深度学习框架 PyTorch 中集成了成熟的计算方式,只需加载模型即可得到具体结果;对于后者,论文使用常用的乘加累积操作数(Multiplication and Accumulations,MACs)[67]进行表示。同一个网络的 MACs 值</p><p>近似为另一个反映网络计算复杂度的指标——浮点运算数(Floating Point</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 3 章 基于残差分组线性变换解码器的自动语音识别</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>Operations,<em class='similar'>FLOPs)</em>的。<em class='similar'>对于将输入维度映射为输出维度的单层</em></p><p>全连接网络,其 MACs 值的计算过程如下:</p><p>(3-12)</p><p><em class='similar'>对于较为复杂的卷积操作,</em><em class='similar'>MACs 值的计算过程如下:</em></p><p>(3-13)</p><p>式中,<em class='similar'>和分别表示卷积核的高度和宽度;</em><em class='similar'>和分别表示输入和输出的通道数;</em><em class='similar'>和分别表示输出图像的高度和宽度。</em></p><p>在模型实际应用(推理)方面,论文使用 GPU 内存占用率(GPU Memory-Usage Rate,GMUR)和实时因子(Real Time Factor,RTF)[68]分别反映模型的资源占用和识别速度。对于前者,可使用 Nvidia 官方提供的显卡驱动自带工具</p><p>(nvidia-smi,NVSMI)监控 GPU 状态得到,为了保证不受某些特殊因素的影响,</p><p>对于每个模型的 GMUR 值,论文共设计5轮测试并计算其均值作为最终的结果;</p><p>对于后者,其定义如下:</p><p>(3-14)</p><p>式中,表示模型识别所有音频的处理时间;表示所有音频的总时间。</p><p>RTF 值越小表示模型的识别速度越快,小于1才能达到实时效果,通常范围在。同样地,对于每个模型的 RTF 值,论文共设计5轮测试并计算其均值作为最终的结果。</p><p>3.5.2数据预处理</p><p>1)声学特征提取</p><p><em class='similar'>论文使用应用较为广泛的频域 FBank 特征作为语音信号的声学特征</em>[66]。</p><p>具体地,首先对语音信号进行分帧、加窗处理,设每帧长度为,帧移为,</p><p>计算过程如下:</p><p>(3-15)</p><p>(3-16)</p><p>上式中,表示汉明窗;表示原始语音信号的第帧的第个样本点。</p><p><em class='similar'>然后对每一帧进行快速傅里叶变换将时域信号转变为频域信号并计算</em></p><p>其能量谱,计算过程如下:</p><p>重庆邮电大学硕士学位论文</p><p>(3-17)</p><p>(3-18)</p><p>接着将实际频率转化为梅尔(Mel)频率,以便模拟人耳对声音的感知,其</p><p>计算过程如下:</p><p>(3-19)</p><p>最后经过 Mel 滤波器(一系列三角滤波器)、对数运算及离散余弦变换即可得到 FBank 特征。</p><p>对于 AISHELL-1,<em class='similar'>论文设置25ms 帧长和10ms 帧移来提取 FBank 特征,</em>其维度为80;<em class='similar'>对于 TED-LIUM2,</em><em class='similar'>在80维 FBank 特征的基础上额外增加了3维 Pitch</em></p><p>特征[69]。论文还使用卷积模块对 FBank 特征进一步处理。其中,卷积核为3×3、</p><p><em class='similar'>步长为2、</em><em class='similar'>输出通道数为256、</em><em class='similar'>激活函数为 ReLU。</em>在卷积模块的作用下,原始特征的时刻长度被压缩至。</p><p>2)标签文本处理</p><p><em class='similar'>对于 AISHELL-1,</em><em class='similar'>论文使用4336个不同的字符来构成词汇表</em><em class='similar'>(字典)</em><em class='similar'>,其中4333个字符来自于原始标注文本,</em>另外3个为辅助模型<em class='similar'>训练额外增加的特殊字符,</em>分别是、和。用于指示模型何时开始或停止输出字符序列;可以在训练阶段将一个批处理(Batch)<em class='similar'>大小的数据中不同长度的语音特征序列或字符序列填充到相同的长度;</em><em class='similar'>可在模型输出时替代某些在字典中未出现的字符,</em>以避免模型在训练时出现错误;对于</p><p>TED-LUM2,论文使用 SentencePiece 工具[70]生成了建模单位为一元模型的词汇表,共包含500个字符。</p><p>3.5.3训练配置信息及过程</p><p>论文选择领域内近年来一些性能表现优秀的同类模型作为对比组,具体为:TDNN-Chain[71]、LAS[72]、SSAN[56]、Speech-Transformer(非官方开源)[73]、HA-</p><p>Transformer(非官方开源)[74]和 STBD(非官方开源)[75]。由于提出这些方法的</p><p>文献中未涉及评价指标中的 MACs、GMUR 和 RTF 值,且只在单一语种的数据集上进行训练,所以论文根据这些开源方法,逐一搭建了网络模型并在两个目标数据集上完成了训练。表3-4展示了各个模型主要超参数的配置信息,包含编码器激活函数(E-AF)、解码器激活函数(D-AF)、优化器(OP)、热身步数</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 3 章 基于残差分组线性变换解码器的自动语音识别</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>(WU)、学习率(LR)、Dropout 概率(DR)、批处理大小(BS)和迭代次数(EP)。</p><p><em class='similar'>改进的轻量级 Transformer 模型由12个编码器块和6个解码器块堆叠而成,</em></p><p>表示子空间的个数均为4,模型维度为256,其中缩放单元的参数、、、、和 DR 分别为4、12、6、2、8和0.03。</p><p><em class='similar'>论文使用Kullback-Leibler散度作为模型训练的损失函数,</em>标签平滑度(Label</p><p>Smoothing)设为1,其定义如下:</p><p>(3-20)</p><p>式中,和分别表示第个字符的真实分布和预测分布。</p><p>表3-4各个模型主要超参数的配置信息</p><p>Table3-4 Configuration of the main hyper-parameters of each model</p><p>模型数据集 E-AF D-AF OP WU LR DR BS EP</p><p>Speech-</p><p>Transformer[73]</p><p>AISHELL-1 ReLU ReLU</p><p>Adam</p><p>2.5e41.00.10128100</p><p>TED-LIUM2 GLU GLU 1.2e40.8e-30.15 DBS 150</p><p>HA-</p><p>Transformer[74]</p><p>AISHELL-1</p><p>GLU GLU Adam</p><p>2.5e41.00.10128100</p><p>TED-LIUM21.2e40.2e-20.20 DBS 150</p><p>STBD[75] AISHELL-1 ReLU ReLU Adam 1.6e41.00.1096100</p><p>Proposed</p><p>AISHELL-1</p><p>Swish ReLU Adam</p><p>2.5e41.00.10128100</p><p>TED-LIUM21.2e40.8e-30.10 DBS 150注:STBD 模型在 TED-LIUM2等英文数据集上无法收敛。 DBS 表示动态批处理大小。</p><p>与在 AISHELL-1上不同,在 TED-LIUM2上的批处理大小设置为动态的(Dynamic Batch Size,DBS)。在 AISHELL-1/TED-LIUM2上,模型在迭代训练</p><p>100/150次后,将最后30/50次迭代的参数值进行平均形成最终训练好的模型[74]。<em class='similar'>论文使用了近期提出的 SpecAugment</em>[76]<em class='similar'>对训练集数据进行必要的增强。</em>另外,为</p><p>了保证实验结果对比的公平性,在模型构建过程中没有使用任何例如以 C++语言进行编程的加速技巧。训练过程中各个模型在验证集上的损失(Loss)曲线如图</p><p>3-7所示(从第10次迭代开始)。</p><p>参照大多数自动语音识别系统,论文构建了外部语言模型(Language Model,</p><p>LM)。<em class='similar'>在推理阶段,</em><em class='similar'>将原始声学模型和语言模型的预测结果以浅融合</em>[77]的方式</p><p>综合,可以进一步提高识别精度。论文针对 AISHELL-1和 TED-LIUM2构建了两</p><p>重庆邮电大学硕士学位论文</p><p>个相同的具有1024个隐藏单元的两层 LSTM 网络模型作为 LM,BS 分别为512和128,训练语料来自于各自训练集中的标签文本。使用单个 GPU 进行训练,LMs 在迭代训练60次后,将最后10次迭代的参数值进行平均形成最终训练好的模型。</p><p>图3-7训练过程中 AISHELL-1(左侧)和 TED-LIUM2(右侧)验证集</p><p>上各个模型的损失曲线</p><p>Fig.3-7 Loss curves for each model on the validation subset of AISHELL-1</p><p>(left) and TED-LIUM2(right) during training</p><p>3.5.4对比实验结果及分析</p><p>将改进的轻量级 Transformer 模型和其他基于 Transformer 的同类模型在AISHELL-1和 TED-LIUM2上的实验结果进行对比,具体信息分别见表3-5和表</p><p>3-6。表中标&quot;*&quot;表示未开源的方法,MACs 值是在编码序列长度为500、解码序列长度为30的条件下计算得出,&quot;↓&quot;表示该项指标越低越好,<em class='similar'>粗体数字表示最优结果,</em><em class='similar'>下划线数字表示次优结果。</em></p><p>从表3-5中可以看出,在 AISHELL-1上,改进模型在参数量、计算量和 GPU内存使用率 GMUR 等方面均达到了最优。具体地,与对比组中识别性能最好的HA-Transformer 相比,改进模型在测试集上的 CER 上升了0.71%,RTF 值上升了</p><p>0.015(在实际应用中影响较小),但参数量和计算量分别相对下降了48.35%和</p><p>77.16%,同时 GMUR 也下降了24.94%,在引入语言模型后 CER 可进一步降低至</p><p>6.38%;在引入语言模型的情况下,与对比组中参数量和计算量都最小的 Speech-Transformer 相比,改进模型在验证集和测试集上的 CER 分别下降了0.97%和</p><p>1.09%,参数量和计算量分别相对下降了27.66%和70.72%,同时 GMUR 也下降了25.06%。</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 3 章 基于残差分组线性变换解码器的自动语音识别</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>表3-5 AISHELL-1上各个模型的实验结果</p><p>Table3-5 Experimental results of each model on AISHELL-1</p><p>模型</p><p>CER (%)↓ Parameters (M)</p><p>↓</p><p>MACs (B)</p><p>↓</p><p>GMUR (%)</p><p>↓</p><p>RTF</p><p>↓ Val Test</p><p>TDNN-Chain (Kaldi)*[71]7.45</p><p>LAS*[72]10.56</p><p>SSAN*[56]6.8436.0</p><p>Speech-Transformer[73]6.917.4728.218.175.470.059</p><p>HA-Transformer[74]5.585.9639.523.275.350.067</p><p>STBD[75]7.438.0353.859.364.340.083</p><p>Proposed 6.186.6720.45.350.410.082</p><p>Proposed (With LM)5.946.38</p><p>表3-6 TED-LIUM2上各个模型的实验结果</p><p>Table3-6 Experimental results of each model on TED-LIUM2</p><p>模型</p><p>WER (%)↓ Parameters (M)</p><p>↓</p><p>MACs (B)</p><p>↓</p><p>GMUR (%)</p><p>↓</p><p>RTF</p><p>↓ Val Test</p><p>Speech-Transformer[73]10.9511.4527.316.740.460.090</p><p>HA-Transformer[74]10.8111.4139.421.542.390.091</p><p>Proposed 11.8011.8620.34.634.070.115</p><p>Proposed (With LM)10.7611.11</p><p>从表3-6中可以看出,在引入语言模型的情况下,改进模型在 TED-LIUM2上的对比实验结果与在 AISHELL-1上类似,在识别性能、参数量、计算量和 GPU内存使用率 GMUR 等方面均达到了最优。具体地,与对比组中识别性能最好的HA-Transformer 相比,改进模型在测试集上的 WER 下降了0.3%;与对比组中轻量化程度最高的 Speech-Transformer 相比,改进模型的参数量和计算量分别相对下降了25.64%和72.46%,同时 GMUR 也下降了6.39%%。</p><p>为了更加直观地看出所提方法的效果,论文将上述结果进行可视化处理。具体地,选择在测试集上的 CER/WER、参数量 Parameters、计算量 MACs 和 GPU内存占用率 GMUR 等指标,将 CER/WER 作为纵轴、其他三项分别作为横轴绘制</p><p>散点图,如图3-8所示。可以看出,改进模型的对应点在三幅图上都位于最左侧,除了在 AISHELL-1上的 CER 略高于 HA-Transformer,在参数量、计算量和 GPU内存占用率等方面都达到了最优,进一步证明了所提方法的有效性,即在保证一定识别性能的情况下,实现模型参数量和计算复杂度的降低。</p><p>重庆邮电大学硕士学位论文</p><p>图3-8各个模型实验结果的可视化处理。(a) CER/WER 与 Parameters 的关系;(b) CER/WER 与 MACs 的关系;(c) CER/WER 与 GMUR 的关系</p><p>Fig.3-8 Visualization of experimental results for each model.(a) Relation between CER/WER and parameters;(b) Relation between CER/WER and</p><p>MACs;(c) Relation between CER/WER and GMUR</p><p>3.5.5消融实验结果及分析</p><p>论文围绕解码器类型和注意力表示子空间个数设计消融实验,探究它们对于整个改进模型性能提升的贡献程度。具体地,<em class='similar'>在提出的轻量级 Transformer 模型的基础上,</em><em class='similar'>分别将轻量级解码器替换为普通解码器、</em><em class='similar'>将轻量级解码器的注意力表示子空间个数从4降低为1,</em>从而得到两个变种模型,用 ND(Normal Decoder)和SH(Single Head)表示,观察它们在 AISHELL-1和 TED-LIUM2上的评价指标变</p><p>化情况,具体信息见表3-7。表中,Base 指提出的轻量级 Transformer 模型,&quot;↓&quot;</p><p>表示该项指标越低越好,括号内数字表示变种模型的该项指标值与 Base 模型的差</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 3 章 基于残差分组线性变换解码器的自动语音识别</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>异,红色代表结果变好,绿色代表结果变差。除了变化的模块,所有变种模型的其他部分(包括网络结构、实验环境和训练配置等)均与 Base 模型一致。所有实验均在不考虑外部语言模型的情况下进行。</p><p>表3-7消融实验结果</p><p>Table3-7 Ablation study results</p><p>数据集</p><p>模</p><p>型</p><p>CER/WER (%)↓ Parameters</p><p>(M)↓</p><p>MACs (B)</p><p>↓</p><p>GMUR (%)</p><p>↓</p><p>RTF</p><p>↓ Val Test</p><p>AISHELL-1</p><p>Base 6.186.6720.45.350.410.082</p><p>ND</p><p>6.00</p><p>(-0.18)</p><p>6.47</p><p>(-0.20)</p><p>25.5</p><p>(+5.1)</p><p>15.6</p><p>(+10.3)</p><p>63.24</p><p>(+12.83)</p><p>0.064</p><p>(-0.018)</p><p>SH</p><p>6.36</p><p>(+0.18)</p><p>6.79</p><p>(+0.12)</p><p>20.4</p><p>(0)</p><p>5.3</p><p>(0)</p><p>52.37</p><p>(+1.96)</p><p>0.073</p><p>(-0.009)</p><p>TED-LIUM2</p><p>Base 11.8011.8620.34.634.070.115</p><p>ND</p><p>11.05</p><p>(-0.75)</p><p>11.45</p><p>(-0.41)</p><p>24.9</p><p>(+4.6)</p><p>14.2</p><p>(+9.6)</p><p>42.73</p><p>(+8.66)</p><p>0.087</p><p>(-0.028)</p><p>SH</p><p>11.41</p><p>(-0.39)</p><p>12.07</p><p>(+0.21)</p><p>20.3</p><p>(0)</p><p>4.6</p><p>(0)</p><p>35.43</p><p>(+1.36)</p><p>0.109</p><p>(-0.006)</p><p>从上表中可以看出,所有变种模型的RTF值相较于Base模型都有略微降低,均在0.03以内,在实际应用中影响较小。在 AISHELL-1上,若使用普通解码器(ND),相较于 Base 模型,<em class='similar'>ND 在验证集和测试集上的 CER 分别下降了0.18%和0.20%,</em>但参数量和计算量分别相对上升了25.00%和194.34%,同时 GPU 内存占用率也上升了12.83%;<em class='similar'>若使用含有单个注意力表示子空间的轻量级解码器</em><em class='similar'>(SH),</em>相较于 Base 模型,SH 在验证集和测试集上的 CER 绝对值分别上升了</p><p>0.18%和0.12%,参数量和计算量没有变化,GPU 内存占用率上升了1.96%。在TED-LIUM2上的消融实验结果与在 AISHELL-1上类似。</p><p>为了更加直观地看出各变种模型和 Base 模型的区别,论文将上述结果进行可视化处理。具体地,选择在测试集上的 CER/WER、参数量 Parameters、计算量MACs 和 GPU 内存占用率 GMUR 等指标,将 CER/WER 作为纵轴、<em class='similar'>其他三项分别作为横轴绘制散点图,</em><em class='similar'>如图3-9所示。</em>可以看出,Base 模型的对应点在三幅图上都位于最左侧,表明引入轻量级解码器,可以实现模型参数量、计算复杂度和GPU 内存占用率的显著降低,而只牺牲小部分的识别性能,达到识别性能和模型轻量化的平衡状态,进一步论证了所提方法的有效性。此外,引入轻量级解码器时,<em class='similar'>需要考虑对于其中注意力表示子空间的个数选取,</em><em class='similar'>因为这在很大程度上会影响到模型的识别性能。</em>从下图中可以看出,采用单一表示子空间时,模型的轻量化程度几乎没有变化,但在识别性能上表现较差。</p><p>重庆邮电大学硕士学位论文</p><p>图3-9各个变种模型实验结果的可视化。(a) CER/WER 与 Parameters 的关系;(b) CER/WER 与 MACs 的关系;(c) CER/WER 与 GMUR 的关系</p><p>Fig.3-9 Visualization of experimental results for each variant model.(a) Relation between CER/WER and parameters;(b) Relation between CER/WER</p><p>and MACs;(c) Relation between CER/WER and GMUR</p><p>3.6本章小结</p><p>本章工作在原始 Transformer 模型的基础上,侧重于以识别性能的小幅度降低为代价,实现模型参数量和计算复杂度的降低。首先提出基于残差分组线性变换的&quot;钻石&quot;型缩放单元,然后将缩放单元嵌入至原始 Transformer 模型的解码器中形成轻量级解码器,最终得到改进的、轻量级 Transformer 模型。在 AISHELL-1和 TED-LIUM2数据集上的实验结果论证了本章所提方法的有效性。</p><p>Equation Chapter (Next) Section (Next)</p><p>第4章基于标签感知图交互的自然语言理解</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第4章 基于标签感知图交互的自然语言理解</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>4.1引言</p><p>基于显式联合建模的自然语言理解是近年来较为流行的一种建模方式,因其交互模块能充分共享捕获到的两个子任务间的共享特征信息,且能明确两个子任务间的交互过程以提高模型可解释性,在自然语言理解任务中展现了良好的性能,</p><p>受到了广泛的关注。然而,当前的研究方法需要进一步讨论以下问题:</p><p>(1)过于简化的标签特征。捕获特征只关注于原始话语和特征(One-hot)编码之间的相关性,而忽略了直观的原始标签特征。而标签语义可以通过评估话语中的字符和标签中的字符之间的语义相似性来提高自然语言理解模型的性能;</p><p>(2)维度较低的交互模块。交互模块只考虑了在字符级别(Token-level)上的意图—槽位交互,缺少了在语句级别(Sentence-level)上的全局优化,导致上下文特征泄露,从而影响模型性能。</p><p>为解决上述问题,本章工作在经典显式联合建模方法的基础上,侧重于对特征捕获和交互方式进行优化,实现模型交互能力和预测精度的提高。具体地,首先提出标签映射模块获取原始话语和标签语义之间的相关性以提供丰富的先验知识,然后提出全局图交互模块对语句级别的意图—槽位交互过程进行建模以提供全局优化,从而提升模型性能。</p><p>4.2基于最佳线性逼近的标签映射模块</p><p>受文献[78]的启发,论文利用最佳线性逼近(Best Linear Approximation)[79]</p><p>思想辅助检测原始话语的意图。具体地,基于最佳线性逼近构建标签映射模块,将意图标签特征自适应地融合到话语特征中,以增强模型的表征能力。</p><p>4.2.1最佳线性逼近</p><p>线性逼近是一种数学方法,它可以将一个函数在某一点处近似地表示为一个线性函数。具体来说,对于一个可导函数,可以在其某一点处,通过求取其在该点的导数来构造一个线性函数,使得在该点附近可以近似地代替。最佳线性逼近是在线性逼近的基础上,使得可以最优地代替,即在所有一次函数中,与的误差最小。</p><p>在多维空间上,最佳线性逼近指的是:通过一个线性函数来最好地拟合给定的数据点集合。具体来说,假设有一组数据点,它们位于多维空间中的某个子空</p><p>重庆邮电大学硕士学位论文</p><p>间上。需要想要找到一个线性函数,能够最好地拟合这些数据点。这个线性函数可以用一个向量表示,即最佳线性逼近向量。</p><p>上述过程用数学符号可描述为:假设表示 Hilbert 空间、<em class='similar'>表示的一个子空间,</em><em class='similar'>对于一个给定向量,</em><em class='similar'>需要找到一个离最近的向量,</em></p><p>的解是维空间的一组基向量的线性组合,其</p><p>中系数满足,为 Gram 矩阵,。</p><p>4.2.2标签映射模块</p><p>将上述思想迁移应用至自然语言理解领域,利用意图标签数据构建标签映射模块,其结构如图4-1所示。下面对模块的构建细节进行介绍。</p><p>图4-1标签映射模块结构</p><p>Fig.4-1 Structure of label injection module</p><p>1)标签编码器</p><p>构造一个以为基向量的标签空间,其中表示意图标签的个数。为了得到这一组基向量,设计标签编码器,通过堆叠的 Bi-LSTM 和自注意力机制对原始标签数据进行编码。具体地,首先使用 Bi-LSTM 双向读取输入序列</p><p>以生成隐藏状态,其中</p><p>;然后使用与式(3-2)相同的计算步骤得到自注意力的输出;接着将与连接形成编码向量;最后使用</p><p>Softmax 函数得到最终的输出,计算过程如下:</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 4 章 基于标签感知图交互的自然语言理解</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>(4-1)</p><p>(4-2)</p><p>式中,为可训练的权重矩阵;表示偏置向量。</p><p>2)标签映射</p><p>通过上述编码方式构造标签空间后,对于一个给定的原始话语向量,可</p><p>将其映射到上以获取最佳线性逼近向量,计算过程如下:</p><p>(4-3)</p><p>式中,系数,Gram 矩阵和定义如下:</p><p>(4-4)</p><p>需要注意的是,每个向量代表一个意图,且是线性独立的,因此 Gram 矩阵是正定的并且有逆。</p><p>4.3基于图注意力网络的全局图交互模块</p><p>为了高效融合所有的交互信息,深入挖掘两个子任务间的语义特征,论文提出全局图交互模块提供双向交互通道。具体地,基于图注意力网络搭建全局图交互模块,对语句级别的意图—槽位交互过程进行建模以提供全局优化,从而提升模型的交互能力。</p><p>4.3.1图注意力网络</p><p><em class='similar'>图注意力网络</em><em class='similar'>(Graph Attention Network,</em><em class='similar'>GAT)[80]</em><em class='similar'>是一种图神经网络的变体,</em></p><p>它融合了图的结构信息和节点特征,并且其中的掩膜自注意力机制能让节点专注于邻域特征且学习不同的注意力权重。<em class='similar'>与传统的图卷积神经网络</em><em class='similar'>(Graph Convolutional Network,</em><em class='similar'>GCN)</em>不同,GAT 不仅考虑节点之间的邻接关系,而且</p><p>还考虑节点之间的关联性,并根据关联性对邻节点进行不同程度的加权。具体地,<em class='similar'>GAT 接收节点特征作为输入,</em><em class='similar'>其中,</em><em class='similar'>表示节点个数,</em><em class='similar'>表示每个节点的特征个数;</em><em class='similar'>输出一组新的节点特征,</em><em class='similar'>其中</em><em class='similar'>(可能具有不同的基数)</em>。</p><p>重庆邮电大学硕士学位论文</p><p><em class='similar'>为了将输入特征转化为更高层次的特征,</em><em class='similar'>首先将由权重矩阵参数化的共享线性变换应用于每个节点;</em><em class='similar'>然后在节点上使用自注意力机制计算系数</em></p><p>,计算过程如下:</p><p>(4-5)</p><p>式中,表示节点的特征与节点的关联性;由权重矩阵得到,并使用 LeakyReLU 激活函数。</p><p><em class='similar'>接着使用 Softmax 函数对进行归一化处理得到,</em><em class='similar'>计算过程如下:</em></p><p>(4-6)</p><p>式中,表示节点的所有一阶邻节点(包括本身)的集合。</p><p>将上两式综合,自注意力机制计算出的系数可以表示为:</p><p>(4-7)</p><p>式中,表示转置操作。上述过程如图4-2所示。</p><p>图4-2系数的计算过程</p><p>Fig.4-2 Calculation process of coefficient</p><p>最后使用非线性函数作用于,并引入多头注意力机制得到最终的输出特</p><p>征,计算过程如下:</p><p>(4-8)</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 4 章 基于标签感知图交互的自然语言理解</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>式中,表示多头注意力机制中表示子空间的个数;表示由第个表示子空间计算得到的归一化系数;表示相对应的权重矩阵。上述过程如所示(以为例),其中不同颜色、形式的箭头表示各自独立的注意力计算方式。</p><p>图4-3多头注意力机制下的特征融合过程</p><p>Fig.4-3 Feature aggregation based on multi-head attention mechanism</p><p>4.3.2全局图交互模块</p><p>基于上述思想,为了实现语句级别的意图—槽位交互,搭建全局图交互模块,</p><p>其中所有的意图预测结果和槽位序列都相互连接,如图4-4所示,橙色方框中的</p><p>表示与槽位相关的隐藏状态序列,绿色圆框中的</p><p>表示意图预测结果。在数学意义上,图(Graph)可以表示为,其中顶点指的是意图和槽位,边指的是它们之间的相关性。下面将从这些方面对模块的构建细节进行介绍。</p><p>图4-4全局图交互模块结构</p><p>Fig.4-4 Structure of global graph interaction module</p><p>重庆邮电大学硕士学位论文</p><p>1)顶点</p><p>图中共有个顶点,其中表示原始话语的长度,表示的个数,即</p><p>预测出的意图个数。输入的槽位字符特征定义如下:</p><p>(4-9)</p><p>(4-10)</p><p>式中,表示层数;表示可训练的权重矩阵。初始状态。</p><p>输入的意图特征定义如下:</p><p>(4-11)</p><p>式中,表示用于映射的可训练的嵌入参数。</p><p>将上述两种特征(节点)组合,形成图的第1层状态向量。</p><p>2)边</p><p>图中有3种连接关系,分别为:槽位—槽位连接:构造槽位—槽位连接关系,</p><p>其中每个槽位节点通过滑动窗口(超参数)连接其他槽位节点,以进一步获取槽位之间的依赖性并合并上下文信息;意图—意图连接:考虑到所有意图节点都出自于同一个原始话语,构造意图—意图连接关系,获取所有意图节点之间的关联性;意图—槽位连接:由于意图和槽位之间的密切联系,构造意图—槽位连接关系,对两个子任务之间的全局交互过程进行建模。具体地,引入缩放点积注意力</p><p>机制计算意图和槽位之间的相关程度,计算过程如下:</p><p>(4-12)</p><p>式中,表示第个状态向量(槽位)和第个意图预测结果的相关程度;表示隐藏单元的维度。</p><p>若(超参数),表明该槽位节点和意图节点有足够的关联性。在这种情况下,将该槽位节点与预测意图节点进行连接。</p><p>3)特征融合</p><p>图中在第层的特征融合过程如下:</p><p>(4-13)</p><p>式中,<em class='similar'>和分别表示槽位节点和意图节点的集合;</em><em class='similar'>表示可训练的权重矩阵。</em></p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 4 章 基于标签感知图交互的自然语言理解</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>4.4基于标签感知的图交互模型</p><p>在经典显式联合建模方法(如图2-7所示)的基础上,采用上述模块对交互过程进行优化,得到如图4-5所示的基于标签感知的图交互模型。下面对该模型的一些重要组件进行简介。</p><p>图4-5基于标签感知的图交互模型</p><p>Fig.4-5 Structure of graph interaction model based on label-aware</p><p>4.4.1共享编码器</p><p>对于给定的长度为的输入序列,使用与标签编码器(节</p><p>4.2.2)相同的编码方式得到和,其中共享向量作为意图检测子网络和槽位填充子网络的输入。此外,在槽位填充子网络中使用 Bi-LSTM 对共享向量进一步编码,得到与槽位相关的隐藏状态序列,其中第个时刻的隐藏状态。</p><p>4.4.2意图解码器</p><p>受文献[81]的启发,论文引入阈值(超参数)辅助意图的解码预测,的值可以根据不同的数据集进行调整。具体地,在经过标签映射模块(节4.2.2)</p><p>重庆邮电大学硕士学位论文</p><p>得到最佳线性逼近向量之后,首先将其输入至意图解码器,获得关于所有意图</p><p>的概率分布序列,计算过程如下:</p><p>(4-14)</p><p><em class='similar'>式中,</em><em class='similar'>表示 Sigmoid 函数;</em><em class='similar'>和表示可训练的权重矩阵和偏置向量。</em></p><p>然后逐一比较和,若,则将其作为最终的意图之一。例如,当且时,。</p><p>4.4.3槽位解码器</p><p>在经过全局图交互模块(节4.3.2)中层的特征融合后,使用 argmax 和</p><p>Softmax 函数得到槽位预测结果,计算过程如下:</p><p>(4-15)</p><p>式中,表示第个字符的槽位;表示可训练的权重矩阵。</p><p>4.5实验结果及分析</p><p>本节首先介绍实验环境、使用的数据集和评价指标,然后给出对于标签文本的预处理流程及文本特征提取方法,接着描述网络模型的训练配置信息及过程,最后通过对比实验结果分析改进模型的性能,同时设计一系列消融实验论证所提方法的有效性。</p><p>4.5.1实验环境、数据集和评价指标</p><p>1)实验环境</p><p>实验环境与节3.5.1相同,具体信息可见表3-1。</p><p>2)数据集</p><p>实验使用在自然语言理解领域内应用较为广泛的开源多意图(Multiple Intent)</p><p>数据集:MixATIS[81]和 MixSnips[81],二者分别由单意图(Single Intent)数据集ATIS(Airline Travel Information Systems)[82]和 Snips[83]扩充而来,使得每句话语</p><p>包含1~3个意图。其中,MixATIS 包含14746句话语,涵盖关于航班的编号</p><p>(atis_flight_no)、时间(atis_flight_time)和餐食(atis_meal)等17个意图类别,</p><p>由录音和相应的人工记录组成,内容涉及用户在航空公司旅行查询系统上的航班询问信息;MixSnips 包含44173句话语,涵盖查询天气(GetWeather)、预定餐厅(BookRestaruant)和播放歌曲(PlayMusic)等7个意图类别。MixATIS 和MixSnips 的子集划分见表4-1,二者均已完成数据清洗,去除了重复的语句。</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 4 章 基于标签感知图交互的自然语言理解</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>表4-1 MixATIS 和 MixSnips 的划分信息 Table4-1 Split of MixATIS and MixSnips</p><p>数据集训练集(句)验证集(句)测试集(句)</p><p>MixATIS 13162756828</p><p>MixSnips 3977621982199</p><p>3)评价指标</p><p>在模型准确度方面,论文选用常用的意图准确率(Intent Accuracy,IAcc)、槽位 F1值和整体准确率(Overall Acc,OAcc)分别作为意图检测、槽位填充和</p><p>整体语句的评价指标[52]。其中,OAcc 表示意图和槽位均预测正确的句子所占的</p><p>比例,IAcc 和 F1值的定义如下:</p><p>(4-16)</p><p>(4-17)</p><p>式中,<em class='similar'>TP(</em><em class='similar'>True Positive)</em><em class='similar'>和 TN</em><em class='similar'>(True Negative)</em><em class='similar'>分别表示预测正样本和负样本正确的个数;</em><em class='similar'>FP(</em><em class='similar'>False Positive)</em><em class='similar'>和 FN</em><em class='similar'>(False Negative)</em><em class='similar'>分别表示预测正样本和负样本错误的个数。</em></p><p>在模型实际应用(推理)方面,论文使用 GPU 内存占用率(GPU Memory-Usage Rate,GMUR)和推理时间(Latency)分别反映模型的资源占用和预测速度。对于前者,与节3.5.1中计算方式一致;对于后者,定义为模型推理一个批次数据的时间,可使用 Python 中自带的工具库 Time 计算得到,为了保证不受某些特殊因素的影响,对于每个模型的 Latency 值,论文共设计5轮测试并计算其均值和标准差作为最终的结果。</p><p>4.5.2数据预处理</p><p>1)标签文本处理</p><p>和节3.5.2类似,在自然语言理解任务中,同样需要建立词汇表(字典)。对于 MixATIS 和 MixSnips,论文分别使用不同的554/1435个字符来构成词汇表,其中551/1432个字符来自于本身的标注文本,另外包含3个用于辅助模型训练的特殊字符,即&lt;EOS/BOS&gt;、&lt;PAD&gt;和&lt;UNK&gt;。</p><p>2)文本特征提取</p><p>论文使用应用较为广泛的 Word2Vec 中的 Skip-gram 模型提取文本特征[84]。</p><p>Skip-gram 是一种用于获取文本特征的模型,它的输入是一个中心字符,而输出是</p><p>重庆邮电大学硕士学位论文</p><p>与该中心字符相关的字符。具体地,给定一个长度为的文本序列</p><p>,其中表示文本中的第个字符。对于任意一个中心字符,Skip-gram 模型的目标是预测它周围的上下文字符。具体地,Skip-gram 模型的损</p><p>失函数基于交叉熵损失,定义如下:</p><p>(4-18)</p><p>式中,表示上下文窗口的大小;表示在给定中心字符的条件下生成上下文字符的概率。</p><p>这个概率可以通过对它们之间的向量(One-hot编码)进行点积操作,然后将</p><p>结果送入 Softmax 函数得到,计算过程如下:</p><p>(4-19)</p><p>式中,表示上下文字符对应的向量;表示中心字符对应的向量;表示词汇</p><p>表的字符数;</p><p>在训练结束后,对于词汇表中的任一字符,可得到以该字符的中心字符向量和上下文字符向量,使用前者作为提取的文本特征。</p><p>4.5.3训练过程及配置信息</p><p>论文选择领域内近年来一些性能表现优秀的同类模型作为对比组,具体为:</p><p>Attention BiRNN[85],Slot-Gated(官方开源)[29]、Bi-Model[53]、SF-ID(官方开源)</p><p>[32]、Stack-Propagation(官方开源)[31]、Joint Multiple ID-SF[86]、AGIF(官方开源)</p><p>[81]和 SDJN[87]。由于提出这些方法的文献中未涉及评价指标中的 GMUR 和 RTF值,所以论文根据这些开源方法,逐一搭建了网络模型并在两个目标数据集上完成了训练。表4-2展示了本章所提模型主要超参数的配置信息。</p><p>对于本章所提模型,论文采用联合训练的模式来综合考虑意图检测和槽位填充两个子任务,联合训练的损失函数由它们各自的损失函数按一定的比例组合而</p><p>成,定义如下:</p><p>(4-20)</p><p>(4-21)</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 4 章 基于标签感知图交互的自然语言理解</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>(4-22)</p><p>式中,、和分别表示联合训练任务、意图检测任务和槽位填充任务的损失函数;和均为超参数;表示输入话语序列的长度;表示意图标签个数;表示槽位标签个数;和分别表示真实的和预测的意图结果。在本章所提模型的训练过程中,和分别设置为0.85和0.15。训练过程中各个模型在验证集上的损失(Loss)曲线如图4-6所示(从第10次迭代开始)。在所有对比组实验中,<em class='similar'>论文选择在验证集上表现最佳的模型作为训练好的模型,</em>然后在测试集上对其进行测试以进行公平比较。此外,在模型构建过程中没有使用任何例如以 C++语言进行编程的加速技巧。</p><p>表4-2本章所提模型主要超参数的配置信息</p><p>Table4-2 Configuration of the main hyper-parameters of the model proposed in this chapter</p><p>名称 MixATIS MixSnips 名称 MixATIS MixSnips 隐藏单元维度256256优化器 Adam Adam 字符向量维度12864学习率 Learning Rate 1e-31e-3</p><p>标签向量维度256128权重衰减1e-61e-6</p><p>表示子空间个数48 Dropout 概率0.40.4</p><p>图层数22滑动窗口大小21</p><p>批处理大小 Batch Size 64128阈值0.50.5</p><p>迭代次数 Epoch 200200阈值0.50.5</p><p>图4-6训练过程中 MixATIS(左侧)和 MixSnips(右侧)验证集上各个</p><p>模型的损失曲线</p><p>Fig.4-6 Loss curves for each model on the validation subset of MixATIS (left)</p><p>and MixSnips (right) during training</p><p>重庆邮电大学硕士学位论文</p><p>4.5.4对比实验结果及分析</p><p>将本章提出的模型和其他同类模型在 MixATIS 和 MixSnips 测试集上的实验</p><p>结果进行对比,具体信息分别见表4-3和表4-4。表中标&quot;*&quot;表示未开源的方法,</p><p>&quot;↑&quot;/&quot;↓&quot;分别表示该项指标越高/低越好,<em class='similar'>粗体数字表示最优结果,</em><em class='similar'>下划线数字表示次优结果。</em></p><p>表4-3 MixATIS 上各个模型的实验结果</p><p>Table4-3 Experimental results of each model on MixATIS</p><p>模型</p><p>OAcc (%)</p><p>↑</p><p>IAcc (%)</p><p>↑</p><p>F1(%)</p><p>↑</p><p>GMUR (%)</p><p>↓</p><p>Latency (ms)</p><p>↓</p><p>Attention BiRNN*[85]39.174.686.4</p><p>Slot-Gated[29]35.563.987.746.2411.7±1.1</p><p>Bi-Model*[53]34.470.383.9</p><p>SF-ID[32]34.966.287.448.3514.2±1.3</p><p>Stack-Propagation[31]39.676.286.555.7336.6±2.1</p><p>Joint Multiple ID-SF*[86]36.173.484.6</p><p>AGIF[81]40.874.486.753.787.3±0.5</p><p>SDJN*[87]44.677.188.2</p><p>Proposed 49.977.888.344.629.5±0.3</p><p>表4-4 MixSnips 上各个模型的实验结果</p><p>Table4-4 Experimental results of each model on MixSnips</p><p>模型</p><p>OAcc (%)</p><p>↑</p><p>IAcc (%)</p><p>↑</p><p>F1(%)</p><p>↑</p><p>GMUR (%)</p><p>↓</p><p>Latency (ms)</p><p>↓</p><p>Attention BiRNN*[85]59.595.489.4</p><p>Slot-Gated[29]55.494.687.948.2512.3±1.7</p><p>Bi-Model*[53]63.495.690.7</p><p>SF-ID[32]59.995.090.645.7714.6±1.2</p><p>Stack-Propagation[31]72.496.293.744.5437.7±2.3</p><p>Joint Multiple ID-SF*[86]62.995.190.6</p><p>AGIF[81]74.295.194.243.147.2±0.4</p><p>SDJN*[87]75.796.594.4</p><p>Proposed 77.397.194.840.6610.2±0.3</p><p>从表4-3中可以看出,在 MixATIS 上,与对比组中预测效果最好的 SDJN 相比,本章所提模型的整体准确率 OAcc、意图准确率 IAcc 和槽位 F1值分别上升了</p><p>5.3%、0.7%和0.1%;与对比组中 GMUR 最小的 Slot-Gated 相比,GMUR 下降了</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 4 章 基于标签感知图交互的自然语言理解</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>1.62%;与对比组中 Latency 值最小的 AGIF 相比,Latency 的均值上升了3ms(在实际应用中影响较小)。本章所提模型在 MixSnips 上的结果与在 MixATIS 上类似。</p><p>为了更加直观地看出所提方法的效果,论文将上述结果进行可视化处理。具体地,选择整体准确率 OAcc、GPU 内存占用率和推理时间 Latency 均值等指标,将 OAcc 作为纵轴、其他两项分别作为横轴绘制散点图,如图4-7所示。可以看出,本章所提模型的对应点在两幅图上都位于最高点,除了在 Latency 值方面略高于 AGIF,在整体准确率和 GPU 内存占用率两方面都达到了最优,进一步证明了所提方法的有效性,即优化了特征捕获和交互方式,实现模型交互能力和预测精度的提高。</p><p>图4-7各个模型实验结果的可视化处理。(a) OAcc与GMUR的关系;(b)</p><p>OAcc 与 Latency 的关系</p><p>Fig.4-7 Visualization of experimental results for each model.(a) Relation</p><p>between OAcc and GMUR;(b) Relation between OAcc and Latency</p><p>另外,为了更好地理解本章所提模型在双向交互过程中捕获的语义特征,分别使用来自 MixATIS 和 MixSnips 数据集的话语输入至模型并将其注意力权重可视化,纵轴为预测意图,横轴为输入话语,标*表示含有槽位的字符,如图4-8所</p><p>示。对于每个权重块,颜色越深,表示意图和槽位的关联性越强。在 MixATIS 上,</p><p>例句具有两个意图 atis_flight 和 atis_meal,可以看到注意力权重成功地集中在正确的位置上,具体地,在槽位 delta 处正确聚合了 atis_flight 意图信息,在槽位meal 处正确聚合了 atis_meal 意图信息,表明模型能很好地关注每个槽位并合并相关意图信息。在 MixSnips 上的结果与在 MixATIS 上类似,在槽位 song 和 siesta</p><p>重庆邮电大学硕士学位论文</p><p>处聚合了 AddToPlaylist 意图,在槽位 shadow、of、suribachi、five 和 starts 处聚合了 RateBook 意图。</p><p>图4-8输入话语在本章所提模型上的注意力权重可视化。(a)来自</p><p>MixATIS 的话语;(b)来自 MixSnips 的话语</p><p>Fig.4-8 Visualization of the attention weights for input utterances on the models proposed in this chapter.(a) Utterance from MixATIS;(b) Utterance</p><p>from MixSnips</p><p>4.5.5消融实验结果及分析</p><p>论文围绕标签映射模块和全局图交互模块设计消融实验,探究它们对于整个模型性能提升的贡献程度。具体地,在本章所提模型的基础上,分别移除标签映射模块(Label Injection Module,LIM)、移除全局图交互模块(Global Graph Interaction Module,GGIM),从而得到两个变种模型,用 w/o LIM 和 w/o GGIM表示,观察它们在 MixATIS 和 MixSnips 上的评价指标变化情况,具体信息见表</p><p>4-5。表中,Base 指本章所提模型,&quot;↑&quot;/&quot;下&quot;表示该项指标越高/低越好。除了变化的模块,所有变种模型的其他部分(包括网络结构、实验环境和训练配置等)均与 Base 模型一致。</p><p>从表中可以看出,所有变种模型的预测精度相较于 Base 模型都有明显降低。具体地,在 MixATIS 上,若移除标签映射模块(w/o LIM),和 Base 模型相比,GPU 内存占用率 GMUR 下降了3.80%,Latency 值下降了1.2ms,二者在实际应用中均影响较小,但整体准确率 OAcc、意图准确率 IAcc 和槽位 F1值分别下降了</p><p>7.6%、1.4%和1.2%;若移除全局图交互模块(w/o GGIM),和 Base 模型相比</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 4 章 基于标签感知图交互的自然语言理解</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>GPU 内存占用率 GMUR 下降了2.96%,Latency 值下降了0.8ms,但整体准确率OAcc、意图准确率 IAcc和槽位F1值分别下降了6.8%、0.6%和2.6%。在MixSnips上的结果与在 MixATIS 上类似。</p><p>表4-5消融实验结果</p><p>Table4-5 Ablation study results</p><p>数据集模型</p><p>OAcc (%)</p><p>↑</p><p>IAcc (%)</p><p>↑</p><p>F1(%)</p><p>↑</p><p>GMUR (%)</p><p>↓</p><p>Latency (ms)</p><p>↓</p><p>MixATIS</p><p>Base 49.977.888.344.629.5±0.3</p><p>w/o LIM</p><p>42.3</p><p>(-7.6)</p><p>76.4</p><p>(-1.4)</p><p>87.1</p><p>(-1.2)</p><p>40.82</p><p>(-3.80)</p><p>8.3±0.7</p><p>(-1.2)</p><p>w/o GGIM</p><p>43.1</p><p>(-6.8)</p><p>77.2</p><p>(-0.6)</p><p>85.7</p><p>(-2.6)</p><p>41.66</p><p>(-2.96)</p><p>8.7±0.8</p><p>(-0.8)</p><p>MixSnips</p><p>Base 77.397.194.840.6610.2±0.3</p><p>w/o LIM</p><p>72.2</p><p>(-5.1)</p><p>95.5</p><p>(-1.6)</p><p>93.8</p><p>(-1.0)</p><p>37.35</p><p>(-3.31)</p><p>9.4±0.4</p><p>(-0.8)</p><p>w/o GGIM</p><p>74.0</p><p>(-3.3)</p><p>96.4</p><p>(-0.7)</p><p>92.5</p><p>(-2.3)</p><p>36.92</p><p>(-3.74)</p><p>8.1±1.2</p><p>(-2.1)</p><p>注:括号内数字表示该模型的该项指标值与 Base 模型的差异。红色表示结果变好,绿色表示结果变差。</p><p>为了更加直观地看出各变种模型和 Base 模型的区别,论文将上述结果进行可视化处理。具体地,将意图准确率 IAcc 作为纵轴、槽位 F1值作为横轴绘制散点图,如图4-9所示。</p><p>图4-9各个变种模型意图准确率 IAcc 和槽位 F1值关系的可视化</p><p>Fig.4-9 Visualization of the relation between intent accuracy and slot F1 for each variant model</p><p>从上图中可以看出,在意图准确率 IAcc 方面,w/o LIM 相较于 Base 模型的下降程度大于 w/o GGIM,说明标签映射模块可以获取原始话语和标签语义之间的</p><p>重庆邮电大学硕士学位论文</p><p>相关性,以辅助意图检测子任务;在槽位 F1值方面,w/o GGIM 相较于 Base 模型的下降程度大于 w/o GGIM,说明全局图交互模块可以对语句级别的意图—槽位交互过程进行建模,并通过全局优化辅助槽位填充子任务,从而显著提高模型性能,进一步论证了所提方法的有效性。</p><p>4.6本章小结</p><p>本章工作在经典显式联合建模方法的基础上,侧重于对特征捕获和交互方式进行优化,实现模型交互能力和预测精度的提高。首先提出标签映射模块获取原始话语和标签语义之间的相关性以提供丰富的先验知识,然后提出全局图交互模块对语句级别的意图—槽位交互过程进行建模以提供全局优化,<em class='similar'>从而提升模型性能。</em><em class='similar'>在MixATIS和MixSnips数据集上的实验结果论证了本章所提方法的有效性。</em></p><p>Equation Chapter (Next) Section (Next)</p><p>第5章面向车载嵌入式设备的本地智能语音对话系统</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第5章 面向车载嵌入式设备的本地智能语音对话系统</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>5.1引言</p><p>当前主流的车载语音对话系统均采用&quot;云—端&quot;方式运行,存在着一定的数据安全隐患。为解决这类问题,面向车载嵌入式设备,研发离线条件下的本地智能语音对话是有效的技术途径。具体地,首先选取 Nvidia Jetson TX2作为车载嵌入式设备并进行刷机、配置环境等操作,然后根据实际应用场景收集、创建驾驶数据集,接着将论文第3章、<em class='similar'>第4章提出的模型分别在驾驶数据集上进行训练,</em>最后集成、移植网络模型至 TX2并围绕搭建全套硬件平台,实现数据安全、自然实时的离线智能语音对话。本章会对系统搭建的细节进行详细介绍并完成相应的系统测试。</p><p>5.2嵌入式设备运行环境搭建</p><p>嵌入式设备是指将计算能力和控制功能嵌入到设备中的计算机系统,通常具有小巧、低功耗、高效率和低成本等特点,广泛应用于工业自动化、医疗设备和安防监控等领域。英伟达(Nvidia)公司推出的 Jetson TX2是一款高性能、低功耗的嵌入式计算平台,其外观如图5-1(a)所示。它的基于 ARM 架构的 CPU 和Nvidia Pascal 架构的 GPU,能够提供强大的计算和图形处理能力,非常适合于嵌入式设备应用[88]。相较于上一代的 TX1,TX2内存和 eMMC(Embedded Multi</p><p>Media Card)提高一倍,CUDA(Compute Unified Device Architecture)<em class='similar'>架构升级为 Pascal,</em><em class='similar'>每瓦性能提高一倍。</em>同时,由于支持 PyTorch、TensorFlow 和 Caffe 等深度学习框架,TX2也是一款非常适合深度学习应用的嵌入式设备,能够处理复杂的神经网络模型和算法,其主要硬件配置信息见表5-1。</p><p>表5-1 TX2主要硬件配置信息</p><p>Table5-1 Main hardware configuration of TX2</p><p>类型规格类型规格</p><p>算力1.33TFLOPS 内存</p><p>8GB 128-bit</p><p>LPDDR459.7GB/s</p><p>GPU 256-core NVIDIA PascalTM GPU 存储32GB eMMC 5.1</p><p>CPU</p><p>Dual-Core NVIDIA Denver 1.564-Bit CPU 和 Quad-</p><p>Core ARM®Cortex®-A57 MPCore processor</p><p>功率7.5W/15W</p><p>USB USB 3.0和 USB 2.0尺寸87mm 50mm</p><p>重庆邮电大学硕士学位论文</p><p>综上所述,Nvidia Jetson TX2是嵌入式 AI 应用的理想选择,所以论文选择该器件作为搭建本地智能语音对话系统的核心嵌入式设备。下面对 TX2的运行环境搭建步骤进行详细介绍。</p><p>首先按照官方教程,将 TX2与 PC 虚拟机(Ubuntu18.04系统)连接,在 PC虚拟机上安装刷机软件SDK Manager并下载一体化软件包 JetPack(版本为4.5.1),</p><p>即可开始刷机操作,中间操作界面和完成界面如图5-1(b)和(c)所示;然后需要安装神经网络模型训练和推理所需的深度学习框架和各种第三方库。具体地,打开TX2终端窗口执行教程上的相关安装命令,如图5-1(d)所示。值得注意的是,由于在 TX2上没有 NVSMI 工具,无法使用 nvidia-smi 命令查看 GPU 运行状态,所</p><p>以需要额外安装 Jtop 工具作为替代,其安装命令为 sudo -H pip3 install -U jetson-</p><p>stats,安装完成后,可直接使用 Jtop 命令查看 CPU 运行状态、GPU 运行状态、内存和软、硬件信息(如图5-1(e)所示),以及使用 pip3 list 命令查看所有已安装的和运行环境有关的软件包,如图5-1(f)所示。至此,TX2的运行环境搭建完成。</p><p>图5-1 TX2外观及运行环境搭建。(a) TX2外观;(b)刷机中间操作界面;(c)刷机完成界面;(d)深度学习框架和第三方库的安装命令;(e) Jtop</p><p>命令查看软、硬件信息;(f) pip3 list 命令查看所有软件包</p><p>Fig.5-1 TX2 appearance and operating environment construction.(a) Appearance of TX2;(b) Intermediate operation interface of flashing OS;(c) Complete interface of flashing OS;(d) Installation command of deep learning framework and third-party library;(e) Jtop command to view software and</p><p>hardware information;(f) pip3 list command to view all packages</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 5 章 面向车载嵌入式设备的本地智能语音对话系统</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>5.3驾驶数据集收集</p><p>由于所搭建的本地智能语音对话系统专用于驾驶员和车载增强现实平视显示器(Augmented Reality Head-Up Display,ARHUD)系统的语音交互,所以需要从使用 ARHUD 的真实场景中收集音频数据。此外,为了增加数据量且降低收集成本,设计以微信小程序为载体的调查问卷,可从移动端(手机端)收集更多数据。下面对数据收集平台和收集过程中的细节进行详细介绍。</p><p>5.3.1数据收集平台</p><p>数据收集平台包括与 ARHUD 相连的车载收集平台和手机云收集平台,如图</p><p>5-2所示。</p><p>图5-2数据收集平台。(a)车载收集平台(驾驶室);(b)车载收集平台</p><p>(后备箱);(c)手机云收集平台</p><p>Fig.5-2 Data collection platform.(a) In-vehicle collection platform (cab);(b) In-vehicle collection platform (trunk);(c) mobile phone cloud collection</p><p>platform</p><p>重庆邮电大学硕士学位论文</p><p>在车载收集平台上,主要硬件单元包括:麦克风阵列、触发器(按钮)和TX2。麦克风和触发器放置于驾驶室,TX2放置于后备箱,它们通过通用串行总线(Universal Serial Bus)进行连接。麦克风阵列包含4个 MEMS(Micro Electro Mechanical System)麦克风单元的,采样率为16bit/44.1Khz。每个麦克风单元都配备了数字信号处理器(Digital Signal Processor,DSP)用于降低车辆噪音,<em class='similar'>如发动机噪音、</em>传输噪音、<em class='similar'>轮胎噪音和空气动力噪音等。</em>为了避免在收集过程中与志愿者交流造成的语音干扰,设计了一个触发器来手动控制麦克风阵列接收的语音输入。TX2用于存储收集的音频数据,为了扩展存储空间,TX2上额外外接了一张8G 的 SD 卡(Secure Digital Memory Card)。在手机云收集平台上,所有收集的音频数据存储于后台服务器。</p><p>5.3.2数据收集过程</p><p>根据 ARHUD 的功能和使用场景,论文按6个功能类别(包含25个意图)收集音频数据,分别是:关机、切换主题、调整高度和亮度、控制驾驶信息、控制导航和控制其他功能。在整个收集过程中,共有570名志愿者参与,年龄范围在</p><p>18~45岁,男女比例约为1.54:1,具体信息见表5-2。所有的音频数据都是在志愿者和工作人员共同驾车的过程中收集的。实验地位于重庆市渝北区鱼嘴工业园区的非公共道路,主要包括一些位于不同工业园区区域的专用测试路线,全长约</p><p>7公里。汽车的行驶速度取决于驾驶员的驾驶水平,但不允许超过道路限速。</p><p>表5-2收集的数据集的详细信息</p><p>Table5-2 Details of the collected dataset</p><p>数据集信息功能类别音频数</p><p>来自车载收集</p><p>平台的数量</p><p>来自手机云收集</p><p>平台的数量</p><p>男性人数346关机24742187287</p><p>女性人数224切换主题24992017482</p><p>时长(小时)52调整高度和亮度14198131451053</p><p>采样率(Hz)16000控制行车信息70426348694</p><p>通道数1控制导航46604092568</p><p>比特率(bps)96000控制其他功能27065240533012</p><p>帧大小(KB)50合计57983518426096</p><p>为保证数据的真实性,需要志愿者根据自己的驾驶习惯完成实验。在每次驾</p><p>车中,志愿者在工作人员(副驾驶位)的引导下行驶约15分钟(路程约3公里),</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 5 章 面向车载嵌入式设备的本地智能语音对话系统</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>根据所需的 ARHUD 功能说出相应的命令词,负责数据采集其他工作(监控设备运行状态是否正常)的工作人员坐在副驾驶后面。</p><p>在 TX2和后台服务器上的初始语音数据集包含近70000个音频文件,总时长约为75小时。首先对其进行数据清理操作,去除噪音过大、音量过小和格式损坏的音频文件,然后聘请专业的数据处理团队对数据集进行文本标记。经过以上数据处理工作,最终得到了完整的驾驶数据集,将其命名为 CQUPT-DS,包含近</p><p>58000个音频文件,总时长约51小时,子集划分见表5-3。</p><p>表5-3 CQUPT-DS 的划分信息</p><p>Table5-3 Split of CQUPT-DS</p><p>语音段数/语句数时长(小时)</p><p>训练集(Train)5194045.7</p><p>验证集(Val)30002.7</p><p>测试集(Test)29982.6</p><p>5.4模型训练与移植</p><p>在完成驾驶数据集的收集工作之后,将第3章和第4章提出的模型分别在CQUPT-DS 上进行训练,并将训练好的模型搭配其他对话系统中的模块进行集成与移植。</p><p>5.4.1自动语音识别模型训练</p><p>将第3章提出的模型在 CQUPT-DS 上进行训练,同时为了验证所提模型在CQUPT-DS 上的适应性,根据节3.5设置对比实验进行比较,表5-4展示了各个模型主要超参数的配置信息,包含编码器激活函数(E-AF)、解码器激活函数(D-AF)、优化器(OP)、热身步数(WU)、学习率(LR)、Dropout 概率(DR)、批处理大小(BS)和迭代次数(EP)。</p><p>表5-4各个模型在 CQUPT-DS 上主要超参数的配置信息</p><p>Table5-4 Configuration of the main hyper-parameters of each model on CQUPT-DS 模型 E-AF D-AF OP WU LR DR BS EP Speech-Transformer[73] ReLU ReLU Adam 2.3e41.20.10160100</p><p>HA-Transformer[74] GLU GLU Adam 2.3e41.20.10128100</p><p>STBD[75] ReLU ReLU Adam 1.4e41.20.1096100</p><p>Proposed Swish ReLU Adam 2.3e41.20.15128100</p><p>重庆邮电大学硕士学位论文</p><p>训练过程中各个模型在验证集上的损失(Loss)曲线如图5-3所示(从第10次迭代开始)。</p><p>图5-3训练过程中 CQUPT-DS 验证集上各个 ASR 模型的损失曲线</p><p>Fig.5-3 Loss curves for each ASR model on the validation subset of CQUPT-DS during training</p><p>对比实验结果见表5-5,表中 MACs 值是在编码序列长度为500、解码序列长度为30的条件下计算得出,RTF 值是在 TX2上计算得出,&quot;↓&quot;表示该项指标越低越好,<em class='similar'>粗体数字表示最优结果,</em><em class='similar'>下划线数字表示次优结果。</em></p><p>表5-5 CQUPT-DS 上各个模型的实验结果</p><p>Table5-5 Experimental results of each model on CQUPT-DS</p><p>模型</p><p>CER (%)↓ Parameters (M)</p><p>↓</p><p>MACs (B)</p><p>↓</p><p>GMUR (%)</p><p>↓</p><p>RTF</p><p>↓ Val Test</p><p>Speech-Transformer[73]8.578.0628.218.173.560.117</p><p>HA-Transformer[74]8.267.6939.523.271.490.121</p><p>STBD[75]7.457.3253.859.363.680.182</p><p>Proposed 7.267.2620.45.354.070.198</p><p>从表中可以看出,第3章所提模型在 CQUPT-DS 上除了 RTF 值外,在测试集上的 CER、参数量、计算量和 GMUR 都达到了最优。具体地,在 CER 方面,相较于 HA-Transformer 下降了0.43%;在参数量和计算量方面,相较于 Speech-</p><p>Transformer 分别相对下降了27.66%和70.72%;在 GMUR 方面,相较于 STBD 下降了9.61%。表明所提模型在 CQUPT-DS 上也有较强的可适应性。</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 5 章 面向车载嵌入式设备的本地智能语音对话系统</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>5.4.2自然语言理解模型训练</p><p>将第4章提出的模型在 CQUPT-DS 上进行训练,同时为了验证所提模型在CQUPT-DS 上的适应性,根据节4.5设置对比实验进行比较,表5-6展示了所提模型主要超参数的配置信息。</p><p>表5-6所提模型在 CQUPT-DS 上主要超参数的配置信息</p><p>Table5-6 Configuration of the main hyper-parameters of the proposed model</p><p>名称值名称值</p><p>隐藏单元维度256优化器 Adam 字符向量维度128学习率 Leraning Rate 1e-3</p><p>标签向量维度256权重衰减1e-6</p><p>表示子空间个数4 Dropout 概率0.4</p><p>图层数2滑动窗口大小1</p><p>批处理大小 Batch Size 256阈值0.5</p><p>迭代次数 Epoch 200阈值0.5</p><p>在第4章所提模型的训练过程中,<em class='similar'>和分别设置为0.9和0.1。</em><em class='similar'>训练过程中各个模型在验证集上的损失</em><em class='similar'>(Loss)</em><em class='similar'>曲线如图5-4所示</em>(从第10次迭代开始)。</p><p>图5-4训练过程中 CQUPT-DS 验证集上各个 NLU 模型的损失曲线</p><p>Fig.5-4 Loss curves for each NLU model on the validation subset of CQUPT-DS during training</p><p>对比实验结果见表5-7,<em class='similar'>表中&quot;↑&quot;/&quot;↓&quot;分别表示该项指标越高/低越好,</em><em class='similar'>粗体数字表示最优结果,</em><em class='similar'>下划线数字表示次优结果。</em>从表中可以看出,第4章所提模型在 CQUPT-DS 上除了 Latency 值外,整体准确率、意图准确率、槽位 F1值和GMUR都达到了最优。其中,在整体准确率OAcc、意图准确率 IAcc和槽位 F1</p><p>重庆邮电大学硕士学位论文</p><p>值方面,相较于 AGIF 分别上升了3.3%、2.1%和0.3%;在 GMUR 方面,相较于SF-ID 下降了1.06%。表明所提模型在 CQUPT-DS 上也有较强的可适应性。</p><p>表5-7 CQUPT-DS 上各个模型的实验结果</p><p>Table5-7 Experimental results of each model on CQUPT-DS</p><p>模型</p><p>OAcc (%)</p><p>↑</p><p>IAcc (%)</p><p>↑</p><p>F1(%)</p><p>↑</p><p>GMUR (%)</p><p>↓</p><p>Latency (ms)</p><p>↓</p><p>Slot-Gated[29]58.793.989.349.2314.4±1.3</p><p>SF-ID[32]61.294.491.642.8313.1±0.4</p><p>Stack-Propagation[31]75.496.093.958.8632.6±3.5</p><p>AGIF[81]78.294.794.851.579.5±0.4</p><p>Proposed 81.596.895.141.7712.8±1.6</p><p>5.4.3模型集成与移植</p><p>模型训练完成后,根据节2.1并参考其他模块的开源方法,完成对话系统的集成,如图5-5所示。这些开源模块方法包括:DST-as-Prompting[89]、Deep Dyna-</p><p>Q[90]、KENLG-Reading[91]和 FastPitch[92]等。</p><p>集成完成后,将系统模型移植至已经搭建了运行环境的 TX2上。为了观察TX2运行对话系统时的性能表现,使用 top 和 Jtop 命令查看 TX2的 CPU 占用、GPU 占用和内存占用情况,如图5-6所示。</p><p>具体地,系统启动后,经过约10s 所有网络模型均加载完成,系统处于等待音频输入的状态(如图5-6(a)所示);在模型加载和模型推理时,CPU 的最大占用率为74.6%(如图5-6(b)所示);GPU 的最大占用率接近85%,在等待音频输入时为55%左右(如图5-6(c)所示);内存占用为60%(如图5-6(d)所示)。表明对话系统能在嵌入式设备 TX2上较好地运行。</p><p>图5-5集成的对话系统</p><p>Fig.5-5 Integrated dialog system</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 5 章 面向车载嵌入式设备的本地智能语音对话系统</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>图5-6 TX2运行对话系统时的性能表现。(a)运行界面;(b) CPU 占用;</p><p>(c) GPU 占用;(d)内存占用</p><p>Fig.5-6 Performance of TX2 running the dialogue system.(a) Running</p><p>interface;(b) CPU usage;(c) GPU usage;(d) Memory usage</p><p>5.5系统硬件平台搭建与测试</p><p>5.5.1平台搭建</p><p>在完成模型的移植工作之后,围绕 TX2搭建全套硬件平台,如图5-7所示。系统运行的基本流程为:驾驶员在驾驶位上,按住触发器(录音开始)说出命令词,结束后松开触发器(录音结束),麦克风阵列将音频传输至 TX2进行处理,处理结果以 Socket 通信方式经车载内网传输给车载 ARHUD,并在车前窗上的特定区域显示,同时播放语音给予驾驶员反馈。例如,驾驶员命令词为&quot;我想看看别的行车信息&quot;,ARHUD 会在特定区域内切换下一个行车信息,同时播放的语音内容为&quot;好的,已为您切换行车信息,当前信息:XXX&quot;。</p><p>值得说明的是,由于触发器的存在,可能会出现驾驶员误触的情况,这时麦克风阵列录制的音频为空白或者音量很小,会影响到后续模块的有效运行。所以在 TX2中(ASR 模块前)加入逻辑控制,当检测到空白和音量很小()</p><p>重庆邮电大学硕士学位论文</p><p>的音频时进行特殊处理,即直接跳过后续模块并播放语音&quot;说话音量较小,请适当提高音量&quot;以提醒驾驶员。</p><p>图5-7系统运行平台。(a)驾驶室;(b)后备箱</p><p>Fig.5-7 System operating platform.(a) Cab;(b) Trunk.</p><p>5.5.2系统测试</p><p>整套系统已在实验用车上完成搭建,支持 ARHUD 包含的25种意图/功能,最大对话轮次达到5轮。整套系统已通过由重庆利龙智能汽车研究院主导的内部项目验收测试,对话通过率为97%,平均响应时间为0.87s,具体结果见表5-8。</p><p>表5-8项目验收结果</p><p>Table5-8 Project acceptance results</p><p>测试项目测试次数测试通过次数通过率判定结果平均响应时间不符合项概述</p><p>对话系统626097% OK 0.87s 无</p><p>5.6本章小结</p><p>本章工作为面向车载嵌入式设备研发离线条件下的本地智能语音对话系统。首先选取 Nvidia Jetson TX2作为车载嵌入式设备并进行刷机、配置环境等操作,然后根据实际应用场景收集、创建驾驶数据集,接着将论文第3章、<em class='similar'>第4章提出的模型分别在驾驶数据集上进行训练,</em>最后集成、移植网络模型至 TX2并围绕搭建全套硬件平台。由重庆利龙智能汽车研究院主导的内部项目验收测试结果论证了本章工作的有效性。</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第6章 总结与展望</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>6.1总结</p><p>车载语音交互已经成为汽车的标准化配置之一,对话系统是则是语音交互的一种具体技术形式。作为对话系统中的重要使能模块,自动语音识别和自然语言理解是近年来备受关注的研究热点。如何解决对话系统由于当前&quot;云—端&quot;运行方式带来的数据安全隐患,实现自动语音识别和自然语言理解技术在车载平台上的高可靠性、强实时性应用,研发离线条件下的智能语音对话也是当前的研究难点。论文首先阐述了自动语音识别和自然语言理解技术的研究现状,<em class='similar'>然后分析现有方法存在的问题,</em><em class='similar'>提出了相应的改进方案,</em>最后基于改进的网络模型搭建了车载对话系统,实现了数据安全、自然实时的离线智能语音交互。论文的主要研究</p><p>成果总结如下:</p><p>(1)在对现有自动语音识别技术的改进方面,<em class='similar'>针对基于深度编—解码器的模型参数量庞大的问题,</em><em class='similar'>提出了一种基于残差分组线性变换的解码器结构。</em><em class='similar'>该结构关键模块为&quot;钻石&quot;型缩放单元,</em>其内部采用稀疏连接,同一组神经元共享相同的权重矩阵。在 AISHELL-1和 TED-LIUM2数据集上的实验结果表明,所提模型能以识别性能的小幅度降低为代价,实现参数量和计算复杂度的降低。</p><p>(2)在对现有自然语言理解技术的改进方面,针对基于显式联合建模的模型交互能力不足的问题,提出了一种基于标签感知的图交互模型。其中标签映射模块可以获取原始话语与标签语义之间的相关性以提供丰富的先验知识,全局图交互模块可以对语句级别的意图—槽位交互过程进行建模以提供全局优化。在MixATIS 和 MixSnips 数据集上的实验结果表明,所提模型能对特征捕获和交互方式进行优化,实现交互能力和预测精度的提高。</p><p>(3)在对现有车载语音对话系统的改进方面,针对&quot;云—端&quot;方式存在数据安全隐患的问题,搭建了面向车载嵌入式设备的本地智能语音对话系统。具体地,首先选取 Nvidia Jetson TX2作为车载嵌入式设备并进行刷机、配置环境等操作,然后根据实际应用场景收集、创建驾驶数据集 CQUPT-DS,接着将研究成果</p><p>(1)和(2)中的模型在 CQUPT-DS 上进行训练,最后集成、移植网络模型至TX2并围绕搭建全套硬件平台。由重庆利龙智能汽车研究院主导的内部项目验收测试结果表明,对话系统能实现数据安全、自然实时的离线运行。</p><p>重庆邮电大学硕士学位论文</p><p>6.2展望</p><p>论文对自动语音识别和自然语言理解模型进行了改进,在降低参数量和计算复杂度的同时提高了识别性能,同时搭建了车载智能语音对话系统,实现了数据安全、自然实时的离线智能语音交互。然而论文中仍有一些不足之处,后续研究</p><p>工作可以从以下两个方面展开:</p><p>(1)流式自动语音识别。论文提出的自动语音识别模型目前还不支持流式识别,即无法在输入语音信号的同时输出识别的话语,这在某些对于实时性要求较高的应用场景中可能会导致体验性降低。<em class='similar'>下一步可以考虑使用分块技术对编码器中的注意力机制进行优化,</em><em class='similar'>以建立当前输入时刻语音帧和历史信息之间的联系,</em><em class='similar'>从而实现流式的自动语音识别。</em></p><p>(2)非自回归式解码。论文提出的自动语音识别和自然语言理解模型均采用自回归式解码,即解码器需要逐个输出预测的字符。当序列长度增加,这种方式的计算复杂程度会大大提升,可能会影响模型推理效率。<em class='similar'>下一步可以考虑采用非自回归的训练方式对模型进行训练,</em><em class='similar'>以进一步提高模型的预测速度。</em></p><p class='uncheck'>参考文献 </p><p class='uncheck'>[1] SEABORN K, MIYAKE N P, PENNEFATHER P, et al. Voice in human-agent interaction: a survey[J]. ACM Computing Surveys, 2021, 54(4): 1-43. </p><p class='uncheck'>[2] 中泰证券研究所. Cerence:人工智能助力未来出行—车智能化海外公司系列报告[EB/O</p><p class='uncheck'>L]. (2021-04-25) [2022-12-25]. https: pdf.dfcfw.com/pdf/H3_AP202104261487921362_1.pdf?1619458440000.pdf. </p><p class='uncheck'>[3] 陈艳华. 基于智能交互的车载语音系统的设计与实现[D]. 北京: 北京交通大学, 2020: 1-2. </p><p class='uncheck'>[4] DERIU J, RODRIGO A, OTEGI A, et al. Survey on evaluation methods for dialogue systems[J]. Artificial Intelligence Review, 2021, 54(1): 755-810. </p><p class='uncheck'>[5] RIGOLL G, NEUKIRCHEN C. A new approach to hybrid HMM/ANN speech recognition </p><p class='uncheck'>using mutual information neural networks[C]. 10th Conference on Neural Information Processing Systems, Denver, USA, 1996: 772-778. </p><p class='uncheck'>[6] MOHAMED A, DAHL G, HINTON G. Deep belief networks for phone recognition[C]. 23rd </p><p class='uncheck'>Conference Neural Information Processing Systems Workshop on Deep Learning for Speech Recognition and Related Applications, Vancouver, Canada, 2009: 39-39. </p><p class='uncheck'>[7] ABDEL-HAMID O, MOHAMED A, JIANG H, et al. Convolutional neural networks for speech </p><p class='uncheck'>recognition[J]. IEEE-ACM Transactions on Audio, Speech, And Language Processing, 2014, 22(10): 1533-1545. </p><p class='uncheck'>[8] GRAVES A. Connectionist temporal classification[M]. Springer, Berlin, Heidelberg: Supervised Sequence Labelling with Recurrent Neural Networks, 2012: 61-93. </p><p class='uncheck'>[9] SUTSKEVER I, VINYALS O, LE Q V. Sequence to sequence learning with neural networks[C]. 28th Conference Neural Information Processing Systems, Montreal, Canada, 2014: 3104-3112. [10] GRAVES A, FERNÁNDEZ S, GOMEZ F, et al. Connectionist temporal classification: </p><p class='uncheck'>labelling unsegmented sequence data with recurrent neural networks[C]. Proceedings of the 23rd International Conference on Machine Learning, Pittsburgh, USA, 2006: 369-376. </p><p class='uncheck'>[11] GRAVES A. Sequence transduction with recurrent neural networks[J]. arXiv preprint arXiv:1211.3711, 2012. </p><p class='uncheck'>[12] GRAVES A, MOHAMED A, HINTON G. Speech recognition with deep recurrent neural </p><p class='uncheck'>networks[C]. 38th IEEE International Conference on Acoustics, Speech and Signal Processing, Vancouver, Canada, 2013: 6645-6649. </p><p class='uncheck'>重庆邮电大学硕士学位论文 </p><p class='uncheck'>[13] GRAVES A. Generating sequences with recurrent neural networks[J]. arXiv preprint arXiv:1308.0850, 2013. </p><p class='uncheck'>[14] BAHDANAU D, CHO K, BENGIO Y. Neural machine translation by jointly learning to align and translate[J]. arXiv preprint arXiv:1409.0473, 2014. </p><p class='uncheck'>[15] CHOROWSKI J, BAHDANAU D, CHO K, et al. End-to-end continuous speech recognition using attention-based recurrent NN: first results[J]. arXiv preprint arXiv:1412.1602, 2014. </p><p class='uncheck'>[16] BAHAR P, ZEYER A, SCHLÜTER R, et al. On using 2d sequence-to-sequence models for </p><p class='uncheck'>speech recognition[C]. 44th IEEE International Conference on Acoustics, Speech and Signal Processing, Brighton, UK, 2019: 5671-5675. </p><p class='uncheck'>[17] VASWANI A, SHAZEER N, PARMAR N, et al. Attention is all you need[C]. 31st Conference Neural Information Processing Systems, Long Beach, USA, 2017: 5998-6008. </p><p class='uncheck'>[18] ZHOU S, DONG L, XU S, et al. A comparison of modeling units in sequence-to-sequence </p><p class='uncheck'>speech recognition with the transformer on mandarin Chinese[C]. 25th International Conference on Neural Information Processing, Siem Reap, Cambodia, 2018: 210-220. </p><p class='uncheck'>[19] MERBOLDT A, ZEYER A, SCHLÜTER R, et al. An analysis of local monotonic attention </p><p class='uncheck'>variants[C]. 20th Annual Conference of the International Speech Communication Association, Graz, Austria, 2019: 1398-1402. </p><p class='uncheck'>[20] ZEYER A, IRIE K, SCHLÜTER R, et al. Improved training of end-to-end attention models for speech recognition[J]. arXiv preprint arXiv:1805.03294, 2018. </p><p class='uncheck'>[21] GULATI A, QIN J, CHIU C C, et al. Conformer: convolution-augmented transformer for speech recognition[J]. arXiv preprint arXiv:2005.08100, 2020. </p><p class='uncheck'>[22] QIN L, XIE T, CHE W, et al. A survey on spoken language understanding: recent advances and new frontiers[J]. arXiv preprint arXiv:2103.03095, 2021. </p><p class='uncheck'>[23] RAVURI S, STOLCKE A. Recurrent neural network and LSTM models for lexical utterance </p><p class='uncheck'>classification[C]. 16th Annual Conference of the International Speech Communication </p><p class='uncheck'>Association, Dresden, Germany, 2015: 35-40 </p><p class='uncheck'>[24] YAO K, ZWEIG G, HWANG M Y, et al. Recurrent neural networks for language </p><p class='uncheck'>understanding[C]. 14th Annual Conference of the International Speech Communication Association, Lyon, France, 2013: 2524-2528. </p><p class='uncheck'>[25] MESNIL G, HE X, DENG L, et al. Investigation of recurrent-neural-network architectures and </p><p class='uncheck'>learning methods for spoken language understanding[C]. 14th Annual Conference of the International Speech Communication Association, Lyon, France, 2013: 3771-3775. </p><p class='uncheck'>[26] MESNIL G, DAUPHIN Y, YAO K, et al. Using recurrent neural networks for slot filling in </p><p class='uncheck'>spoken language understanding[J]. IEEE-ACM Transactions on Audio, Speech, and Language Processing, 2014, 23(3): 530-539. </p><p class='uncheck'>[27] ZHANG X, WANG H. A joint model of intent determination and slot filling for spoken </p><p class='uncheck'>language understanding[C]. 25th International Joint Conference on Artificial Intelligence, New York, USA, 2016: 2993-2999. </p><p class='uncheck'>[28] HAKKANI-TÜR D, TÜR G, CELIKYILMAZ A, et al. Multi-domain joint semantic frame </p><p class='uncheck'>parsing using bi-directional RNN-LSTM[C]. 17th Annual Conference of the International Speech Communication Association, San Francisco, USA, 2016: 715-719. </p><p class='uncheck'>[29] GOO C W, GAO G, HSU Y K, et al. Slot-gated modeling for joint slot filling and intent </p><p class='uncheck'>prediction[C]. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), New Orleans, USA, 2018: 753-757. </p><p class='uncheck'>[30] LI C, LI L, QI J. A self-attentive model with gate mechanism for spoken language </p><p class='uncheck'>understanding[C]. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, 2018: 3824-3833. </p><p class='uncheck'>[31] Qin L, Che W, Li Y, et al. A stack-propagation framework with token-level intent detection for </p><p class='uncheck'>spoken language understanding[C]. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, Hong Kong, China, 2019: 2078-2087. </p><p class='uncheck'>[32] NIU P, CHEN Z, SONG M. A novel bi-directional interrelated model for joint intent detection and slot filling[J]. arXiv preprint arXiv:1907.00390, 2019. </p><p class='uncheck'>[33] ZHANG C, LI Y, DU N, et al. Joint slot filling and intent detection via capsule neural </p><p class='uncheck'>networks[C]. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Florence, Italy, 2019: 5259-5267. </p><p class='uncheck'>[34] LIU Y, MENG F, ZHANG J, et al. CM-Net: a novel collaborative memory network for spoken </p><p class='uncheck'>language understanding[C]. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, Hong Kong, China, 2019: 1051-1060. </p><p class='uncheck'>[35] ZHANG L, MA D, ZHANG X, et al. Graph LSTM with context-gated mechanism for spoken </p><p class='uncheck'>language understanding[C]. Proceedings of the 34th AAAI Conference on Artificial Intelligence, New York, USA, 2020: 9539-9546. </p><p class='uncheck'>重庆邮电大学硕士学位论文 </p><p class='uncheck'>[36] Qin L, Liu T, Che W, et al. A co-interactive transformer for joint slot filling and intent </p><p class='uncheck'>detection[C]. 46th IEEE International Conference on Acoustics, Speech and Signal Processing, Toronto, Canada, 2021: 8193-8197. </p><p class='uncheck'>[37] DEVLIN J, CHANG M W, LEE K, et al. BERT: pre-training of deep bidirectional transformers </p><p class='uncheck'>for language understanding[C]. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Minneapolis, USA, 2019: 4171-4186. </p><p class='uncheck'>[38] CHEN Q, ZHUO Z, WANG W. Bert for joint intent classification and slot filling[J]. arXiv preprint arXiv:1902.10909, 2019. </p><p class='uncheck'>[39] CHEN H, LIU X, YIN D, et al. A survey on dialogue systems: recent advances and new frontiers[J]. ACM SIGKDD Explorations Newsletter, 2017, 19(2): 25-35. </p><p class='uncheck'>[40] MCTEAR M F. Spoken dialogue technology: enabling the conversational user interface[J]. ACM Computing Surveys, 2002, 34(1): 90-169. </p><p class='uncheck'>[41] 赵新颜. 基于深度学习的对话系统研究与应用[D]. 合肥: 中国科学技术大学, 2022: 13-14. [42] NI J, YOUNG T, PANDELEA V, et al. Recent advances in deep learning based dialogue systems: a systematic survey[J]. Artificial Intelligence Review, 2022: 1-101. </p><p class='uncheck'>[43] JORGENSEN P E T, SONG M S, TIAN J. Operator theory, kernels, and feedforward neural networks[J]. arXiv preprint arXiv:2301.01327, 2023. </p><p class='uncheck'>[44] RUMELHART D E, HINTON G E, WILLIAMS R J. Learning representations by back-propagating errors[J]. Nature, 1986, 323(6088): 533-536. </p><p class='uncheck'>[45] LI Z, LIU F, YANG W, et al. A survey of convolutional neural networks: analysis, applications, </p><p class='uncheck'>and prospects[J]. IEEE Transactions on Neural Networks and Learning Systems, 2021, 33(2): 6999-7019. </p><p class='uncheck'>[46] HEWAMALAGE H, BERGMEIR C, BANDARA K. Recurrent neural networks for time series </p><p class='uncheck'>forecasting: current status and future directions[J]. International Journal of Forecasting, 2021, 37(1): 388-427. </p><p class='uncheck'>[47] HOCHREITER S, SCHMIDHUBER J. Long short-term memory[J]. Neural Computation, 1997, 9(8): 1735-1780. </p><p class='uncheck'>[48] YAO S, WAN X. Multimodal transformer for multimodal machine translation[C]. Proceedings </p><p class='uncheck'>of the 58th Annual Meeting of the Association for Computational Linguistics, Online, 2020: 4346-4350. </p><p class='uncheck'>[49] DU Y, PEI B, ZHAO X, et al. Deep scaled dot-product attention based domain adaptation model for biomedical question answering[J]. Methods, 2020, 173: 69-74. </p><p class='uncheck'>[50] MIRSAMADI S, BARSOUM E, ZHANG C. Automatic speech emotion recognition using </p><p class='uncheck'>recurrent neural networks with local attention[C]. 42nd IEEE International Conference on Acoustics, Speech and Signal Processing, New Orleans, USA, 2017: 2227-2231. </p><p class='uncheck'>[51] CHAN W, JAITLY N, LE Q, et al. Listen, attend and spell: a neural network for large </p><p class='uncheck'>vocabulary conversational speech recognition[C]. 41st IEEE International Conference on Acoustics, Speech and Signal Processing, Shanghai, China, 2016: 4960-4964. </p><p class='uncheck'>[52] KARMAKAR P, TENG S W, LU G. Thank you for attention: a survey on attention-based </p><p class='uncheck'>artificial neural networks for automatic speech recognition[J]. arXiv preprint arXiv:2102.07259, 2021. </p><p class='uncheck'>[53] WANG Y, SHEN Y, JIN H. A bi-model based RNN semantic frame parsing model for intent detection and slot filling[J]. arXiv preprint arXiv:1812.10235, 2018. </p><p class='uncheck'>[54] PHAM N Q, NGUYEN T S, NIEHUES J, et al. Very deep self-attention networks for end-to-end speech recognition[J]. arXiv preprint arXiv:1904.13377, 2019. </p><p class='uncheck'>[55] SYNNAEVE G, XU Q, KAHN J, et al. End-to-end ASR: from supervised to semi-supervised learning with modern architectures[J]. arXiv preprint arXiv:1911.08460, 2019. </p><p class='uncheck'>[56] LUO H, ZHANG S, LEI M, et al. Simplified self-attention for transformer-based end-to-end </p><p class='uncheck'>speech recognition[C]. 8th IEEE Spoken Language Technology Workshop, Shenzhen, China, 2021: 75-81. </p><p class='uncheck'>[57] ZHANG S, LIU C, JIANG H, et al. Feedforward sequential memory networks: a new structure to learn long-term dependency[J]. arXiv preprint arXiv:1512.08301, 2015. </p><p class='uncheck'>[58] MEHTA S, GHAZVININEJAD M, IYER S, et al. Delight: deep and light-weight transformer[J]. arXiv preprint arXiv:2008.00623, 2020. </p><p class='uncheck'>[59] WEN Q, ZHOU T, ZHANG C, et al. Transformers in time series: a survey[J]. arXiv preprint arXiv:2202.07125, 2022. </p><p class='uncheck'>[60] MISRA D. Mish: a self regularized non-monotonic activation function[J]. arXiv preprint arXiv:1908.08681, 2019. </p><p class='uncheck'>[61] LIN T, WANG Y, LIU X, et al. A survey of transformers[J]. AI Open, 2022, 1(1): 1-16. </p><p class='uncheck'>[62] WANG D, ZHANG X. Thchs-30: A free Chinese speech corpus[J]. arXiv preprint arXiv:1512.01882, 2015. </p><p class='uncheck'>[63] BU H, DU J, NA X, et al. Aishell-1: An open-source mandarin speech corpus and a speech </p><p class='uncheck'>recognition baseline[C]. 20th Conference of the Oriental Chapter of the International Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment, Seoul, South Korea, 2017: 1-5. </p><p class='uncheck'>重庆邮电大学硕士学位论文 </p><p class='uncheck'>[64] ROUSSEAU A, DELÉGLISE P, ESTEVE Y. Enhancing the TED-LIUM corpus with selected </p><p class='uncheck'>data for language modeling and more TED talks[C]. Proceedings of the 9th International Conference on Language Resources and Evaluation, Reykjavik, Iceland, 2014: 3935-3939. </p><p class='uncheck'>[65] WATANABE S, HORI T, KARITA S, et al. ESPNET: end-to-end speech processing toolkit[J]. arXiv preprint arXiv:1804.00015, 2018. </p><p class='uncheck'>[66] MALIK M, MALIK M K, MEHMOOD K, et al. Automatic speech recognition: a survey[J]. Multimedia Tools and Applications, 2021, 80: 9411-9457. </p><p class='uncheck'>[67] WHITEHEAD N, FIT-FLOREA A. Precision & performance: floating point and IEEE 754 compliance for NVIDIA GPUs[J]. RN (A+ B), 2011, 21(1): 18749-19424. </p><p class='uncheck'>[68] HIGUCHI Y, INAGUMA H, WATANABE S, et al. Improved mask-CTC for non-</p><p class='uncheck'>autoregressive end-to-end ASR[C]. 46th IEEE International Conference on Acoustics, Speech and Signal Processing, Toronto, Canada, 2021: 8363-8367. </p><p class='uncheck'>[69] GHAHREMANI P, BABAALI B, POVEY D, et al. A pitch extraction algorithm tuned for </p><p class='uncheck'>automatic speech recognition[C]. 39th IEEE International Conference on Acoustics, Speech and Signal Processing, Florence, Italy, 2014: 2494-2498. </p><p class='uncheck'>[70] KUDO T, RICHARDSON J. SentencePiece: a simple and language independent subword </p><p class='uncheck'>tokenizer and detokenizer for neural text processing[C]. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Brussels, Belgium, 2018: 66-71. </p><p class='uncheck'>[71] POVEY D, PEDDINTI V, GALVEZ D, et al. Purely sequence-trained neural networks for ASR </p><p class='uncheck'>based on lattice-free MMI[C]. 17th Annual Conference of the International Speech Communication Association, San Francisco, USA, 2016: 2751-2755. </p><p class='uncheck'>[72] SHAN C, WENG C, WANG G, et al. Component fusion: learning replaceable language model </p><p class='uncheck'>component for end-to-end speech recognition system[C]. 44th IEEE International Conference on Acoustics, Speech and Signal Processing, Brighton, UK, 2019: 5361-5635. </p><p class='uncheck'>[73] DONG L, XU S, XU B. Speech-transformer: a no-recurrence sequence-to-sequence model for </p><p class='uncheck'>speech recognition[C]. 43rd IEEE International Conference on Acoustics, Speech and Signal Processing, Calgary, Canada, 2018: 5884-5888. </p><p class='uncheck'>[74] XU M, LI S, ZHANG X L. Transformer-based end-to-end speech recognition with local dense </p><p class='uncheck'>synthesizer attention[C]. 46th IEEE International Conference on Acoustics, Speech and Signal Processing, Toronto, Canada, 2021: 5899-5903. </p><p class='uncheck'>[75] CHEN X, ZHANG S, SONG D, et al. Transformer with bidirectional decoder for speech recognition[J]. arXiv preprint arXiv:2008.04481, 2020. </p><p class='uncheck'>[76] PARK D S, CHAN W, ZHANG Y, et al. Specaugment: a simple data augmentation method for automatic speech recognition[J]. arXiv preprint arXiv:1904.08779, 2019. </p><p class='uncheck'>[77] KANNAN A, WU Y, NGUYEN P, et al. An analysis of incorporating an external language </p><p class='uncheck'>model into a sequence-to-sequence model[C]. 43rd IEEE International Conference on Acoustics, Speech and Signal Processing, Calgary, Canada, 2018: 1-5828. </p><p class='uncheck'>[78] WU T W, SU R, JUANG B. A label-aware BERT attention network for zero-shot multi-intent </p><p class='uncheck'>detection in spoken language understanding[C]. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Online and Punta Cana, Dominican Republic, 2021: 4884-4896. </p><p class='uncheck'>[79] DEL PINO G E, GALAZ H. Statistical applications of the inverse gram matrix: a revisitation[J]. Brazilian Journal of Probability and Statistics, 1995: 177-196. </p><p class='uncheck'>[80] VELIČKOVIĆ P, CUCURULL G, CASANOVA A, et al. Graph attention networks[J]. arXiv preprint arXiv:1710.10903, 2017. </p><p class='uncheck'>[81] QIN L, XU X, CHE W, ET AL. AGIF: An adaptive graph-interactive framework for joint multiple intent detection and slot filling[J]. arXiv preprint arXiv:2004.10087, 2020. </p><p class='uncheck'>[82] HEMPHILL C T, GODFREY J J, DODDINGTON G R. The ATIS spoken language systems </p><p class='uncheck'>pilot corpus[C]. Speech and Natural Language: Proceedings of a Workshop, Hidden Valley, USA, 1990. </p><p class='uncheck'>[83] COUCKE A, SAADE A, BALL A, et al. Snips voice platform: an embedded spoken language </p><p class='uncheck'>understanding system for private-by-design voice interfaces[J]. arXiv preprint arXiv:1805.10190, 2018. </p><p class='uncheck'>[84] MIKOLOV T, CHEN K, CORRADO G, et al. Efficient estimation of word representations in vector space[J]. arXiv preprint arXiv:1301.3781, 2013. </p><p class='uncheck'>[85] LIU B, LANE I. Attention-based recurrent neural network models for joint intent detection and slot filling[J]. arXiv preprint arXiv:1609.01454, 2016. </p><p class='uncheck'>[86] GANGADHARAIAH R, NARAYANASWAMY B. Joint multiple intent detection and slot </p><p class='uncheck'>labeling for goal-oriented dialog[C]. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Minneapolis, USA, 2019: 564-569. </p><p class='uncheck'>[87] CHEN L, ZHOU P, ZOU Y. Joint multiple intent detection and slot filling via self-distillation[C]. </p><p class='uncheck'>47th IEEE International Conference on Acoustics, Speech and Signal Processing, Singapore, 2022: 7612-7616. </p><p class='uncheck'>重庆邮电大学硕士学位论文 </p><p class='uncheck'>[88] SÜZEN A A, DUMAN B, ŞEN B. Benchmark analysis of jetson tx2, jetson nano and raspberry </p><p class='uncheck'>pi using deep-CNN[C]. 2nd International Congress on Human-Computer Interaction, Optimization and Robotic Applications, Online, 2020: 1-5. </p><p class='uncheck'>[89] LEE C H, CHENG H, OSTENDORF M. Dialogue state tracking with a language model using </p><p class='uncheck'>schema-driven prompting[C]. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Online and Punta Cana, Dominican Republic, 2021: 4937-4949. [90] PENG B, LI X, GAO J, et al. Deep Dyna-Q: integrating planning for task-completion dialogue </p><p class='uncheck'>policy learning[C]. Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Melbourne, Australia, 2018: 2182-2192. [91] YU W, JIANG M, HU Z, et al. Knowledge-enriched natural language generation[C]. </p><p class='uncheck'>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts, Online and Punta Cana, Dominican Republic, 2021: 11-16. </p><p class='uncheck'>[92] ŁAŃCUCKI A. Fastpitch: parallel text-to-speech with pitch prediction[C]. 46th IEEE </p><p class='uncheck'>International Conference on Acoustics, Speech and Signal Processing, Toronto, Canada, 2021: 6588-6592. </p><p>作者简介</p><p>1.基本情况</p><p>黄子恒,男,湖北人,1999年1月出生,<em class='similar'>重庆邮电大学自动化学院/工业互联网学院控制科学与工程专业2020级硕士研究生。</em></p><p>2.教育和工作经历</p><p><em class='similar'>2016.09~2020.06重庆邮电大学自动化学院/工业互联网学院,</em><em class='similar'>本科,</em>专业:</p><p>自动化专业卓越工程师班</p><p>2020.09~2022.06重庆邮电大学自动化学院/工业互联网学院,硕士研究生,</p><p>专业:控制科学与工程</p><p>3.攻读学位期间的研究成果</p><p>3.1发表的学术论文和著作</p><p>[1] Li P, Cheng J, Rong Y, et al. Adaptive Mask Based Attention Mechanism for</p><p>Mandarin Speech Recognition<em class='similar'>[C]//202234th Chinese Control and Decision Conference </em><em class='similar'>(CCDC).</em><em class='similar'> IEEE,2022:</em>2816-2820.</p><p>3.2申请(授权)专利</p><p>[1]李鹏华,黄子恒,张奕辉等.一种基于全局—局部对比学习的跨语言自然语言理解方法:.</p><p>[2]李鹏华,张奕辉</p><p>3.3参与的科研项目及获奖</p><p><em class='similar'>[1]</em><em class='similar'>语音智能对话系统</em><em class='similar'>(MCM20180404)</em>,教育部-中国移动科研基金研发项目,</p><p>2019.01-2021.06,参与.</p><p>[2]面向 XXX 的轻量化神经网络技术研究(XM2021XT1032),国防重点实验室基金项目,2021.01-2021.12,参与.</p><p>[3]面向动力锂电池序列的神经网络轻量自动学习技术研究( cstc2021jcyj-jqX0001),重庆市 XXXXX 基金项目,2021.07-2024.06,参与.</p><p>[4]单康恒,朱得臣,黄子恒等.第七届中国国际&quot;互联网+&quot;大学生创新创业大</p><p>重庆邮电大学硕士学位论文</p><p>赛重庆赛区选拔赛,银奖,2021.08.</p><p>[5]黄子恒,新生奖学金一等奖,2020.09</p><p>致谢</p><p><em class='similar'>在即将完成学位论文之际,</em><em class='similar'>我想要借此机会表达我最深切的感激之情。</em></p><p>首先,我要感谢我的指导老师李鹏华教授,您一直是我在学术道路上最坚实的后盾。您给予我耐心指导,帮助我澄清思路,解决问题,并在研究中不断给予鼓励和支持。没有您的帮助和支持,我不可能完成这篇论文。在您的指导下,我不仅获得了专业知识和研究技能,还形成了自己的学术思想和方法论。<em class='similar'>您的教诲将对我今后的学术生涯产生深远的影响,</em><em class='similar'>我将终生铭记。</em></p><p>我也要感谢我的家人和朋友,在我整个学术过程中一直给予我鼓励和支持。没有你们的支持和理解,我不可能取得今天的成果。在我遇到困难和挫折时,你们的支持和鼓励让我能够坚持下来。</p><p>我还要感谢我的各位同门,我们一起探讨学术问题,互相帮助,共同进步。<em class='similar'>你们的意见和建议帮助我不断完善研究内容和论文质量。</em></p><p>最后,我要感谢我的母校和国家,<em class='similar'>给予我接受高等教育的机会和平台。</em><em class='similar'>在我接受教育的过程中,</em>我获得了知识和技能,更加明确了自己的人生方向和责任使命。</p><p>在此,我再次向所有支持和帮助过我的人们表示由衷的感谢和敬意。</p></p>
                </div>
              </div>
             <div class="report_explain2">
              <div class="repExp_left">说明：</div>
              <div class="repExp_rig">
                <p>1.指标是由系统根据《学术论文不端行为的界定标准》自动生成的</p>
                <p>2.本报告单仅对您所选择比对资源范围内检测结果负责</p>
              </div>
            </div>
            <div class="clear"></div>
            <div class="report_footer">
                <div class="assist_tool">
                  <h2>写作辅助工具</h2>
                  <ul>
                    <li>
                      <div class="asst icons asst1"></div>
                      <div class="asstRig"> <a target="_blank" href="https://www.bigan.net/recommendtitle/">选题分析</a>
                        <p>帮您选择合适的论文题目</p>
                      </div>
                    </li>
                    <li>
                      <div class="asst icons asst2"></div>
                      <div class="asstRig"> <a target="_blank" href="https://www.bigan.net/recommenddata/">资料搜集</a>
                        <p>提供最全最好的参考文章</p>
                      </div>
                    </li>
                    <li>
                      <div class="asst icons asst3"></div>
                      <div class="asstRig"> <a target="_blank" href="https://www.bigan.net/recommendoutline/">提纲推荐</a>
                        <p>辅助生成文章大纲</p>
                      </div>
                    </li>
                    <li>
                      <div class="asst icons asst4"></div>
                      <div class="asstRig"> <a target="_blank" href="https://www.bigan.net/writing/">在线写作</a>
                        <p>规范写作，提供灵感</p>
                      </div>
                    </li>
                    <li class="bgNo">
                      <div class="asst icons asst5"></div>
                      <div class="asstRig"> <a target="_blank" href="https://www.bigan.net/reference/">参考文献</a>
                        <p>规范参考文献，查漏补缺</p>
                      </div>
                    </li>
                  </ul>
                </div>
                <div class="repFot_bot">
                  <div class="reportCopy inlineBlock">版权所有：笔杆 www.bigan.net</div>
                  <div class="shareTo inlineBlock"><span>分享到：</span> <a href="http://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?url=https%3A%2F%2Fwww.bigan.net&title=%E5%92%8C%E6%88%91%E4%B8%80%E8%B5%B7%EF%BC%8C%E6%8B%BF%E8%B5%B7%E7%AC%94%E6%9D%86%EF%BC%8C%E5%81%9A%E4%B8%80%E4%B8%AA%E5%BF%AB%E4%B9%90%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%B8%9D%E3%80%82&summary=%E7%AC%94%E6%9D%86%EF%BC%8C%E4%B8%80%E4%B8%AA%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9C%A8%E7%BA%BF%E5%86%99%E4%BD%9C%E5%B9%B3%E5%8F%B0%E3%80%82%E7%AC%94%E6%9D%86%E4%B8%BA%E4%BD%A0%E6%89%AB%E6%B8%85%E5%86%99%E4%BD%9C%E6%80%9D%E8%B7%AF%E7%9A%84%E9%98%B4%E9%9C%BE%EF%BC%8C%E6%89%BE%E5%88%B0%E6%89%8D%E6%80%9D%E6%B3%89%E6%B6%8C%EF%BC%8C%E7%81%B5%E6%84%9F%E8%BF%B8%E5%8F%91%E7%9A%84%E5%BF%AB%E6%84%9F%EF%BC%8C%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C%EF%BC%8C%E6%88%91%E4%BB%AC%E4%BF%A1%E6%89%8B%E6%8B%88%E6%9D%A5%E3%80%82www.bigan.net&pics=https%3A%2F%2Fwww.bigan.net%2Flogo_80_80.png" target="_blank" title="QQ空间" class="inlineBlock sitem1 icons pngfix"></a> <a href="#" title="微信" class="inlineBlock sitem2 icons pngfix"></a> <a href="http://service.weibo.com/share/share.php?url=https%3A%2F%2Fwww.bigan.net&title=%E5%92%8C%E6%88%91%E4%B8%80%E8%B5%B7%EF%BC%8C%E6%8B%BF%E8%B5%B7%E7%AC%94%E6%9D%86%EF%BC%8C%E5%81%9A%E4%B8%80%E4%B8%AA%E5%BF%AB%E4%B9%90%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%B8%9D%E3%80%82%EF%BC%88%E7%AC%94%E6%9D%86%EF%BC%8C%E4%B8%80%E4%B8%AA%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9C%A8%E7%BA%BF%E5%86%99%E4%BD%9C%E5%B9%B3%E5%8F%B0%E3%80%82%E7%AC%94%E6%9D%86%E4%B8%BA%E4%BD%A0%E6%89%AB%E6%B8%85%E5%86%99%E4%BD%9C%E6%80%9D%E8%B7%AF%E7%9A%84%E9%98%B4%E9%9C%BE%EF%BC%8C%E6%89%BE%E5%88%B0%E6%89%8D%E6%80%9D%E6%B3%89%E6%B6%8C%EF%BC%8C%E7%81%B5%E6%84%9F%E8%BF%B8%E5%8F%91%E7%9A%84%E5%BF%AB%E6%84%9F%EF%BC%8C%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C%EF%BC%8C%E6%88%91%E4%BB%AC%E4%BF%A1%E6%89%8B%E6%8B%88%E6%9D%A5%E3%80%82%40%E7%AC%94%E6%9D%86%E7%BD%91%EF%BC%89" target="_blank" title="新浪微博" class="inlineBlock sitem3 icons pngfix"></a> </div>
                </div>
              </div>
           </div>
       </div>
  </div>
</div>
</div>
</body>
</html>