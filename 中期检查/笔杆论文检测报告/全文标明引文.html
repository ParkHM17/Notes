<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<title>笔杆检测报告单（全文标明引文）</title>
	<meta name="keywords" content="" />
	<meta name="description" content="" />
          <meta content="0" http-equiv="Expires"/>
          <meta content="no-cache" http-equiv="Pragma"/>
          <meta content="no-cache" http-equiv="Cache-Control"/>
          <meta content="no-cache" http-equiv="Cache"/>
	<link href="css/report.css?v20180524" type="text/css" rel="stylesheet" />
	<script src="js/jquery.tools.pack.js" type="text/javascript"></script>
	<script type="text/javascript">
    function $ShowMore(n) {
        if ($("#simMore_" + n + " a").text() == '收起相似文献') {//收起
            $("#reportTable_" + n + " .trLike").hide();
            for (var i = 0; i < 5; i++) {
                $("#reportTable_" + n + " .trLike:eq("+i+")").show();
            }
            $("#simMore_" + n + " a").html('查看更多相似文献<span class="icons inlineBlock simDown"></span>');
        } else {
            $("#reportTable_" + n + " .trLike").show();
            $("#simMore_" + n + " a").html('收起相似文献<span class="icons inlineBlock simUp"></span>');
        }
}</script>
<style>
    em.similar{color:Red; font-style:normal;}
</style>
</head>
<body>
<div class="report_bg2">
  <div class="report_bg3">
    <div class="report_top">
      
      <h1>笔杆检测报告单<span>（全文标明引文）</span></h1>
    </div>
    <div class="report_Wrap">
      <div class="report_tab" id="report_tab">
       <ul>
                                            <li class="rep_curr"><div><a href="全文标明引文" class="green">全文标明引文</a></div></li>
                                            <li><div><a href="全文对照.html" class="green">全文对照</a></div></li>
        </ul>
        <div class="report_priSav">
          <a href="javascript:window.print();" class="print inlineBlock"><span class="icons inlineBlock"></span>打印</a>
          <a target="_blank" href="https://www.bigan.net/report/explain.html" class="report_explain inlineBlock"><span class="icons inlineBlock"></span>检测说明</a>
        </div>
      </div>
      <div class="report_content">
        <div class="report_main">
          <a id="toTop" title="回到顶部"></a><!-- 回到顶部 -->
          <script>
              $(document).ready(function () {
                  $("#toTop").hide();
                  //检测屏幕高度
                  var height = $(window).height();
                  //scroll() 方法为滚动事件
                  $(window).scroll(function () {
                      if ($(window).scrollTop() > height) {
                          $("#toTop").fadeIn(500);
                      } else {
                          $("#toTop").fadeOut(500);
                      }
                  });
                  $("#toTop").click(function () {
                      $('body,html').animate({ scrollTop: 0 }, 100);
                      return false;
                  });
              });
          </script>
           <div class="report_Mtop"></div>
          <div class="report_Mbot">
            <div class="report_result">
              <div class="report_info">
                <p><span>标题：</span>查重版.pdf</p>
                <p><span>作者：</span>黄子恒</p>
                <p><span>报告编号：</span>BG202303131111268318</p>
                <p><span>提交时间：</span>2023-03-13 11:31:40</p>
              </div>
              <div class="report_ratio">
                <ul>
                  <li style="display:none;"><span class="icons ratioIcon ratio1"></span>总复制比：<span class="green">19.4%</span></li>
                  <li class="inlineBlock"><span class="icons ratioIcon ratio2"></span>去除引用文献复制比：<span class="green">18.6%</span></li>
                  <li class="inlineBlock"><span class="icons ratioIcon ratio3"></span>去除本人已发表文献复制比：<span class="green">19.4%</span></li>
                  <li class="inlineBlock"><span class="icons ratioIcon ratio4"></span>单篇最大文字复制比：<span class="green">2.3%</span></li>
                </ul>
              </div>
               <div class="clear"></div>
              <div class="seal">
                <div class="SealArea">
                  <div class="SealBg"><img src="data:image/png;base64,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" /></div>
                  <div class="SealPercent"></div>
                </div>
              </div>
            </div>
            <div class="repeat_words">
              <div class="repWords_box1">
                <div class="repWords_row">
                  <div class="test_range"><a href="https://www.bigan.net/qa/?key=%E6%A3%80%E6%B5%8B%E8%8C%83%E5%9B%B4" target="_blank" class="green"><span class="icons inlineBlock"></span>检测范围</a></div>
                  <span>重复字数:<b class="red">9,513</b></span><span>总字数：<b>49,072</b></span> </div>
                <div class="clear"></div>
              </div>
              </div>
              <div class="clear"></div>
              <div class="similarLiter">

                <h2 style="width:100%;text-align:center;margin-top:15px;">中英文摘要等</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>摘要</p><p><em class='reference'>车载语音交互已经成为继通讯社交、智能家居之后的第三大应用场景。随着深度学习的发展,</em>作为智能语音交互的核心,自动语音识别和自然语言理解技术取得了重大进展。然而,<em class='similar'>深度学习模型的高精度运行需消耗庞大的计算资源,</em><em class='similar'>而</em></p><p>受制于车身结构和研发成本,当前车载语音对话系统均采用&quot;云—端&quot;方式运行,</p><p>存在着一定的数据安全隐患。为解决这一问题,论文提出了基于残差分组线性变换解码器的自动语音识别模型和基于标签感知图交互的自然语言理解模型,在满足模型性能要求的前提下降低了运行所需的计算资源,并搭建了面向嵌入式设备的本地智能语音对话系统,实现了数据安全、自然实时的离线智能语音交互。论</p><p>文的主要工作及创新点如下:</p><p>1.针对基于深度编—解码器的自动语音识别模型参数量庞大的问题,提出了一种基于残差分组线性变换的解码器结构。该结构关键模块为&quot;钻石&quot;型缩放单元,其内部采用稀疏连接,同一组神经元共享相同的权重矩阵。所提模型在AISHELL-1数据集上的参数量和计算量分别为20.4M 和5.3B,错误率为6.67%;在 TED-LIUM2数据集上的参数量和计算量分别为20.3M 和4.6B,错误率为</p><p>11.86%,在模型识别性能和轻量化程度方面优于其他对比方法。</p><p>2.针对基于显式联合建模的自然语言理解模型交互能力不足的问题,提出了一种基于标签感知的图交互模型,其中标签映射模块可以获取原始话语与标签语义之间的相关性以提供丰富的先验知识,全局图交互模块可以对语句级别的意图—槽位交互过程进行建模以提供全局优化。所提模型在 MixATIS 和 MixSnips 数据集上的整体准确率分别为49.9%和77.3%,优于其他对比方法。</p><p>3.针对&quot;云—端&quot;方式运行的车载智能语音对话存在数据安全隐患的问题,搭建了面向车载嵌入式设备的本地智能语音对话系统。首先选取了 Nvidia Jetson TX2作为车载嵌入式设备,然后根据实际应用场景收集、创建了驾驶数据集</p><p>CQUPT-DS,<em class='similar'>接着将上述提出的两个模型在驾驶数据集上进行了训练,</em>最后集成、</p><p>移植网络模型至 TX2并围绕搭建了全套硬件平台,实现了数据安全、自然实时的离线智能语音对话。整套系统的对话通过率为97%,平均响应时间为0.87s。</p><p>关键字:<em class='similar'>车载语音交互,</em><em class='similar'>自动语音识别,</em><em class='similar'>自然语言理解,</em>嵌入式设备,<em class='similar'>对话系统</em></p><p>ABSTRACT</p><p>In-vehicle voice interaction is the third largest application scenario after social communication and smart home. With the development of deep learning, as the core of intelligent voice interaction, automatic speech recognition and natural language understanding technology has made significant progress. However, the high-precision operation of the deep learning model consumes vast computing resources.<em class='similar'> Due to the structure of the vehicle body and the cost of research and development,</em><em class='similar'> the </em>current in-vehicle voice dialogue system is operated in a &quot;cloud-end&quot; mode, which has certain hidden dangers in data security. To solve this problem, the paper proposes an automatic speech recognition model based on a residual group linear transform decoder and a natural language understanding model based on label-aware graph interaction, which reduces the computing resources required for operation while meeting the model performance requirements. The paper also builds a local intelligent voice dialogue system based on embedded devices, realizing data security, natural real-time offline intelligent voice</p><p>interaction. The main work and innovations of the paper are as follows:</p><p>1. A decoder structure based on a residual group linear transform is proposed to address the problem of many parameters in the automatic speech recognition model based on a deep encoder-decoder. The key module of this structure is a &quot;diamond&quot; type scaling unit, which uses sparse connections inside, and the same group of neurons shares the same weight matrix. The parameters and calculations of the proposed model on the AISHELL-1 dataset are 20.4M, and 5.3B, respectively, and the error rate is 6.67%; the parameters and calculations on the TED-LIUM2 dataset are 20.3M and 4.6B, respectively, with an error rate of 11.86%, outperforming other comparative methods in terms of model recognition performance and lightweight.</p><p>2. Aiming at the insufficient interaction ability of natural language understanding models based on explicit joint modeling, a label-aware graph interaction model is proposed, which contains label injection module and global graph interaction module. The former can obtain the correlation between the original utterance and label semantics to provide rich prior knowledge, and the latter can model the intent-slot interaction process at the sentence level to provide global optimization. The overall accuracy of the</p><p>proposed model on the MixATIS and MixSnips datasets is 49.9% and 77.3%, respectively, outperforming other comparison methods.</p><p>3. A local intelligent voice dialogue system based on in-vehicle embedded device was built to aim at the hidden dangers of data security in the in-vehicle intelligent voice dialogue operating in the &quot;cloud-end&quot; mode. First, Nvidia Jetson TX2 was selected as the in-vehicle embedded device, and then the driving data set CQUPT-DS was collected and created according to the actual application scenario. Then the two models proposed above were trained on the driving data set, and finally, the network was integrated and transplanted. The model is up to TX2, and a complete hardware platform is built around it, realizing data security and natural real-time offline intelligent voice dialogue. The dialogue pass rate of the whole system is 97%, and the average response time is 0.87s.</p><p>Keywords: In-vehicle Voice Interaction, Automatic Speech Recognition, Natural</p><p>Language Understanding, Embedded Device, Dialogue System</p><p class='uncheck'>目 录 </p><p class='uncheck'>摘 要 I ABSTRACT II </p><p class='uncheck'>目 录 IV 第 1 章 绪论 1 </p><p class='uncheck'>1.1 研究背景及意义 1 </p><p class='uncheck'>1.2 国内外研究现状 2 </p><p class='uncheck'>1.2.1 自动语音识别技术 2 </p><p class='uncheck'>1.2.2 自然语言理解技术 4 </p><p class='uncheck'>1.3 论文研究内容 6 </p><p class='uncheck'>1.4 论文组织结构 7 </p><p class='uncheck'>第 2 章 对话系统基础理论 9 </p><p class='uncheck'>2.1 对话系统基本组成结构 9 </p><p class='uncheck'>2.2 神经网络基础理论 10 </p><p class='uncheck'>2.2.1 前馈神经网络 10 </p><p class='uncheck'>2.2.2 卷积神经网络 11 </p><p class='uncheck'>2.2.3 循环神经网络及其变种 11 </p><p class='uncheck'>2.3 基于编—解码器结构的自动语音识别 13 </p><p class='uncheck'>2.4 基于显式联合建模的自然语言理解 15 </p><p class='uncheck'>2.5 本章小结 16 </p><p class='uncheck'>第 3 章 基于残差分组线性变换解码器的自动语音识别 17 </p><p class='uncheck'>3.1 引言 17 </p><p class='uncheck'>3.2 Transformer 模型及其组件 17 </p><p class='uncheck'>3.2.1 自注意力机制 18 </p><p class='uncheck'>3.2.2 前馈网络层 19 </p><p class='uncheck'>3.2.3 位置编码 19 </p><p class='uncheck'>3.3 基于残差分组线性变换的"钻石"型缩放单元 20 </p><p class='uncheck'>3.3.1 分组线性变换 20 </p><p class='uncheck'>3.3.2 "钻石"型缩放单元 21 </p><p class='uncheck'>3.4 基于"钻石"型缩放单元的改进 Transformer 模型 23 </p><p class='uncheck'>3.4.1 轻量级前馈网络层 23 </p><p class='uncheck'>3.4.2 轻量级解码器 23 </p><p class='uncheck'>3.4.3 改进的 Transformer 模型结构 24 </p><p class='uncheck'>3.5 实验结果及分析 25 </p><p class='uncheck'>3.5.1 实验环境、数据集和评价指标 25 </p><p class='uncheck'>3.5.2 数据预处理 28 </p><p class='uncheck'>3.5.3 训练配置信息及过程 29 </p><p class='uncheck'>3.5.4 对比实验结果及分析 30 </p><p class='uncheck'>3.5.5 消融实验结果及分析 32 </p><p class='uncheck'>3.6 本章小结 34 </p><p class='uncheck'>第 4 章 基于标签感知图交互的自然语言理解 35 </p><p class='uncheck'>4.1 引言 35 </p><p class='uncheck'>4.2 基于最佳线性逼近的标签映射模块 35 </p><p class='uncheck'>4.2.1 最佳线性逼近 35 </p><p class='uncheck'>4.2.2 标签映射模块 36 </p><p class='uncheck'>4.3 基于图注意力网络的全局图交互模块 37 </p><p class='uncheck'>4.3.1 图注意力网络 37 </p><p class='uncheck'>4.3.2 全局图交互模块 39 </p><p class='uncheck'>4.4 基于标签感知的图交互模型 41 </p><p class='uncheck'>4.4.1 共享编码器 41 </p><p class='uncheck'>4.4.2 意图解码器 41 </p><p class='uncheck'>4.4.3 槽位解码器 42 </p><p class='uncheck'>4.5 实验结果及分析 42 </p><p class='uncheck'>4.5.1 实验环境、数据集和评价指标 42 </p><p class='uncheck'>4.5.2 数据预处理 43 </p><p class='uncheck'>4.5.3 训练过程及配置信息 44 </p><p class='uncheck'>4.5.4 对比实验结果及分析 46 </p><p class='uncheck'>4.5.5 消融实验结果及分析 48 </p><p class='uncheck'>4.6 本章小结 50 </p><p class='uncheck'>第 5 章 面向车载嵌入式设备的本地智能语音对话系统 51 </p><p class='uncheck'>5.1 引言 51 </p><p class='uncheck'>5.2 嵌入式设备运行环境搭建 51 </p><p class='uncheck'>5.3 驾驶数据集收集 53 </p><p class='uncheck'>5.3.1 数据收集平台 53 </p><p class='uncheck'>5.3.2 数据收集过程 54 </p><p class='uncheck'>5.4 模型训练与移植 55 </p><p class='uncheck'>5.4.1 自动语音识别模型训练 55 </p><p class='uncheck'>5.4.2 自然语言理解模型训练 57 </p><p class='uncheck'>5.4.3 模型集成与移植 58 </p><p class='uncheck'>5.5 系统硬件平台搭建与测试 59 </p><p class='uncheck'>5.5.1 平台搭建 59 </p><p class='uncheck'>5.5.2 系统测试 60 </p><p class='uncheck'>5.6 本章小结 60 </p><p class='uncheck'>第 6 章 总结与展望 61 </p><p class='uncheck'>6.1 总结 61 </p><p class='uncheck'>6.2 展望 62 </p><p class='uncheck'>参考文献 63 </p><p class='uncheck'>作者简介 71 </p><p class='uncheck'>1. 基本情况 71 </p><p class='uncheck'>2. 教育和工作经历 71 </p><p class='uncheck'>3. 攻读学位期间的研究成果 71 </p><p class='uncheck'>3.1 发表的学术论文和著作 71 </p><p class='uncheck'>3.2 申请(授权)专利 71 </p><p class='uncheck'>3.3 参与的科研项目及获奖 71 </p><p class='uncheck'>致 谢 73 </p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第1章 绪论</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>1.1研究背景及意义</p><p><em class='reference'>语音交互作为人机通信中最自然、直接的交互方式,具有天然的优势[1]。随着智能汽车的兴起,车载语音交互成为继通讯社交、智能家居之后的第三大应用场景。截止2020年9月,我国乘用车车载语音装配率为64.8%,预计2025年国内前装车载语音市场规模约32亿元,</em>智能语音交互俨然成为汽车的标准化配置之</p><p>一[2]。科大讯飞、亚马逊和谷歌等企业也相继推出了自主研发的车载语音对话系</p><p>统,对车载语音市场的发展进程产生了显著影响[3]。自动语音识别(Automatic</p><p><em class='similar'>Speech Recognition,</em><em class='similar'>ASR)</em><em class='similar'>和自然语言理解</em><em class='similar'>(Natural Language Understanding,</em><em class='similar'>NLU)</em><em class='similar'>是智能语音交互的核心技术。</em>在深度学习的驱动下,当前的 ASR 和 NLU</p><p>技术取得了重大进展,模型性能得到极大提升[4]。</p><p>然而,<em class='similar'>高精度的深度学习模型需要消耗庞大的计算资源才能快速运行。</em>受限于车身结构和研发成本,当前主流的车载语音对话系统均采用&quot;云—端&quot;方式运行,即通过互联网将本地收集到的音频数据传输至配置有高性能图形处理器(Graphics Processing Unit,GPU)的云服务器进行处理,再将处理结果通过网络反馈到本地端,最后由汽车中控系统进行相应操作。事实上,此种运行方式在传输过程和云端处理过程中存在着一定的数据安全隐患,例如特斯拉间谍车、滴滴地图数据外泄等事件。为解决上述问题,实现深度学习下的 ASR 和 NLU 技术在车载平台上的高可靠性、强实时性应用,研发离线条件下的智能语音对话是有效的技术途径。</p><p>论文面向计算资源有限的车载嵌入式设备,立足于语音识别和自然语言处理领域中已有的先进成果,重点研究针对 ASR 和 NLU 的技术创新应用方法,开发相应的轻量级别神经网络模型,在满足性能要求的前提下降低其运行所需的计算资源,探索以离线方式运行的车载智能语音对话平台搭建,在本地实现数据安全、自然实时的语音对话。论文提出的语音对话方法可以推广到智能家居、智慧教育和智慧医疗等其他智能语音交互领域,丰富相关领域的技术理论,实现语音交互的价值。特别地,除了具体的 ASR 和 NLU 技术外,论文中涉及的一些关键共性技术,如模型轻量化、嵌入式移植等,亦可为研发其他依靠深度神经网络的设备提供成熟的案例支撑。</p><p>重庆邮电大学硕士学位论文</p><p>1.2国内外研究现状</p><p>1.2.1自动语音识别技术</p><p>自动语音识别技术的发展过程可以分为三个阶段:<em class='similar'>第一阶段主要使用高斯混合模型-隐马尔可夫模型</em><em class='similar'>(Gaussian Mixture Model-Hidden Markov Model,</em><em class='similar'>GMM-</em></p><p>HMM)进行建模,<em class='similar'>基于 GMM-HMM 的语音识别框架在上世纪得到广泛应用</em>[5];</p><p>直至2009年,Hinton 等人首次将深度信念网络(Deep Belief Neiwork,DBN)应</p><p>用于声学建模并提出了深度信念网络-隐马尔可夫模型(DBN-HMM)[6],在</p><p>TIMIT 的核心测试集上达到了23.0%的音素错误率(Phoneme Error Rate,PER),</p><p>这种方法促进了深度学习在 ASR 领域的应用,大量研究人员投入到深度学习研究</p><p>中,并取得较大的成果[7]。至此,GMM-HMM 框架被打破,进入到以深度神经网</p><p><em class='similar'>络-隐马尔可夫模型</em><em class='similar'>(Deep Neural Network- Hidden Markov Model,</em><em class='similar'>DNN-HMM)</em>为主要方法的第二阶段;第三阶段是端到端(End-to-End,E2E)的时代,基于</p><p><em class='similar'>E2E 的语音识别普遍采用深度学习的方法,</em><em class='similar'>它不需要提前进行语音帧对齐,</em><em class='similar'>而是直接采用带标签的语音进行训练。</em><em class='similar'>与经典方法相比,</em><em class='similar'>它更加简洁且具有较强的通用性,</em><em class='similar'>能够减少对专业语音、</em><em class='similar'>语言知识的依赖,</em><em class='similar'>大大降低了系统搭建难度。</em><em class='similar'>总体可分为两类:</em><em class='similar'>一类是基于联结时序分类</em><em class='similar'>(Connectionist Temporal Classification,</em>CTC)[8]<em class='similar'>的 E2E 模型,</em><em class='similar'>另一类是基于注意力机制</em><em class='similar'>(Attention)</em><em class='similar'>的序列到序列</em>(Sequence-to-Sequence,S2S)<em class='similar'>模型</em>[9],<em class='similar'>二者模型结构如图1-1所示。</em>CTC 方法不</p><p>像经典方法需要对语音数据进行对齐操作,只需计算输出序列和真实序列的差距;而 S2S 方法在2017年被 Google 用于语音识别领域,并取得了非常好的效果。</p><p>图1-1基于 CTC 的 E2E 模型(左)和基于 Attention 的 S2S 模型(右) Fig.1-1 CTC-based E2E model (left) and Attention-based S2S model (right)</p><p>基于 CTC 的 E2E 模型结构如图1-1左侧所示。在无先验性对齐情况下,该模</p><p><em class='similar'>型能够度量输入和输出序列的相似度,</em><em class='similar'>并且能刻画语音特征和字符序列的相关性。</em></p><p><em class='similar'>基于联结时序分类的语音识别由卷积神经网络</em><em class='similar'>(Convolutional Neural Network,</em><em class='similar'>CNN)/循环神经网络</em><em class='similar'>(Recurrent Neural Network,</em><em class='similar'>RNN)</em><em class='similar'>编码模块和</em> CTC 损失函数模块组成。<em class='similar'>2006年 Graves 等人利用空白字符对不同长度的序列进行对齐,</em><em class='similar'>首次提出 CTC 模型来解决 MINIST 数据集上的手写数字识别和 TIMIT </em>数据集上的语料</p><p>库音素分类的问题[10],并在 TIMIT 数据集上实现了30.51±0.19的标签错误率</p><p>(Label Error Rate,LER)。<em class='similar'>2012年 Graves 对 CTC 语音识别模型进一步改进,</em><em class='similar'>将RNN 作为转化模型</em>[11],在 TIMIT 数据集上实现了23.2%的 PER。2013年,<em class='similar'>Graves等人利用多层的长短时记忆</em><em class='similar'>(Long Short-Term Memory,</em><em class='similar'>LSTM)</em><em class='similar'>神经网络进行建</em></p><p>模,<em class='similar'>进一步提升 TIMIT 语料库上的识别效果</em>[12],达到了17.7%的 PER。</p><p>基于 Attention 的 S2S 模型结构如图1-1右侧所示。该模型主要由编码器(Encoder)、注意力网络和解码器(Decoder)三个部分组成,在2013年由</p><p>Graves <em class='similar'>首次提出</em>,<em class='similar'>并被用于 MINIST 数据集的手写数字识别任务中</em><em class='similar'>[13]。2014年,</em><em class='similar'>Bahdanau等人在此基础上进一步完善,</em>提出基于Attention的编—解码模型[14]。同</p><p>年,受到上述两项工作的启发,Chorowski 等人将注意力模型进一步应用于语音</p><p>识别任务中[15],使用注意力机制建立输入和输出序列之间的对齐关系,并在目标</p><p>函数中加入约束,最后在 TIMIT 的验证集和测试集上的 PER 分别为16.88%和</p><p>18.57%。<em class='similar'>至此,</em><em class='similar'>大量的研究人员开始投入到基于 Attention 的语音识别研究中,</em><em class='similar'>并提出多种改进模型。</em><em class='similar'>根据改进方法可分为三类:</em><em class='similar'>对 Encoder 的改进、</em>对 Attention的改进和对语言模型的改进。对 Encoder 的改进方面,<em class='similar'>Bahar 等人采用更复杂的二维长短时记忆网络</em><em class='similar'>(Two-Dimensional Long Short Term Memory,</em><em class='similar'>2DLSTM)</em><em class='similar'>作为</em></p><p><em class='similar'>编码模型,</em><em class='similar'>从多维的角度增加语音时序信息的辨认能力</em>[16]。近几年,研究者们热</p><p>衷于研究基于深度编—解码器(Transformer)的 ASR 系统,Transformer 最先由</p><p>Vaswani 等人提出[17],并被用于建立语言模型和机器翻译领域,由于 Transformer</p><p><em class='similar'>采用全连接网络,</em><em class='similar'>因而相较于 RNN 等结构,</em><em class='similar'>其训练效率高,</em><em class='similar'>模型收敛效果好</em>[18];</p><p>对 Attention 的改进方面,<em class='similar'>Merboldt 等人提出局部注意力机制,</em><em class='similar'>对其添加约束,</em><em class='similar'>同时还考虑最大注意力得分,</em><em class='similar'>之后采用启发式搜索对模型进行训练,</em><em class='similar'>最终在</em></p><p><em class='similar'>SwitchBoard 和 LibriSpeech 数据集上取得较好的效果</em>[19];对语言模型的改进方面,</p><p><em class='similar'>除了传统的语言模型,</em><em class='similar'>Zeyer 等人采用 LSTM 作为语言模型,</em><em class='similar'>并采用字节对编码</em><em class='similar'>(Byte Pair Encoding,</em><em class='similar'>BPE)</em><em class='similar'>输出不同的识别基元,</em><em class='similar'>相比于传统语言模型,</em><em class='similar'>取得</em></p><p>了较好的效果[20]。</p><p>重庆邮电大学硕士学位论文</p><p>综上所述,基于 CTC 的 E2E 模型存在解码过程复杂、<em class='similar'>速度慢等缺点,</em><em class='similar'>而基于Attention 的 S2S 模型具有模型简单、</em><em class='similar'>联合训练、</em><em class='similar'>直接输出和无需强制数据对齐等优点。</em>但是,该类模型存在参数量庞大、计算复杂、难以部署等缺点。例如,谷歌提出的 Conformer 模型参数量达到了1.19亿[21]。这些因素为其在实际工程中的应用带来了巨大的挑战。因此,如何降低模型的参数量和计算复杂度、提高模型</p><p>运行速度,在计算资源有限的车载平台上实现高效的语音识别是亟待解决的问题,也是学术界当前研究的热点。</p><p>1.2.2自然语言理解技术</p><p>自然语言理解通常包含意图检测(Intent Detection,ID)和槽位填充(Slot</p><p>Filling,SF)两个子任务,前者属于文本分类(Text Classification)领域,而后者属于序列标注(Sequence Labeling)领域。自然语言理解技术的发展过程从建模</p><p>方法上可以分为两个类别:基于独立建模的方法和基于联合建模的方法[22]。</p><p>基于独立建模的方法如图1-2(a)所示。在意图检测方面,Ravuri 和 Stolcke 在</p><p>2015年成功地将 RNN 和 LSTM 应用到意图检测中,表明序列特征有利于提升准</p><p>确率[23];在槽位填充方面,常用的方法包括条件随机场(Conditional Random</p><p>Fields,CRF)、RNN 和基于 RNN 的衍生模型。2013年,Yao 采用基于 RNN 的</p><p>语言模型来预测槽位标签,<em class='similar'>此外还研究了命名实体、</em><em class='similar'>句法特征和词类信息等</em>[24]。</p><p>2013年,Mesnil 研究了不同的用于 NLU 的 RNN 衍生模型,包括 Elman RNN、Jordan RNN 和双向 Jordan RNN[25]。下一年,Mesnil 利用 Viterbi 编码方式和循环</p><p>CRF 层消除了标签的预测偏差问题[26]。基于独立建模的方法虽然取得了不错的效果,但由于单独训练,单个模型的意图检测任务和槽位填充任务之间没有交互作用,存在共享信息泄漏,导致模型性能不足。</p><p>图1-2 NLU 模型的三种建模方式。(a)独立建模;(b)隐式联合建模;(c)</p><p>显式联合建模</p><p>Fig.1-2 NLU modeling methods.(a) Independent modeling;(b) Implicit joint</p><p>modeling;(c) Explicit joint modeling</p><p>基于联合建模的方法从两个子任务的交互方式可分为隐式交互和显式交互,如图1-2(b)和(c)所示。隐式联合建模采用共享编码器(Shared Encoder)来捕获特征。例如,Zhang 和 Wang 在2016年引入了共享 RNN 模型来获取意图和插槽之</p><p>间的相关性[27]。同年,Hakkani-Tur 等人提出了一种用于联合建模的共享 RNN-</p><p>LSTM 架构[28]。<em class='similar'>隐式联合建模是一种直接整合共享信息的方法,</em><em class='similar'>但后续未对交互关系进行显式建模,</em><em class='similar'>导致模型可解释性</em>不足,性能无法达到预期;而显式联合建模在捕获特征后依旧通过交互模块进行交互,可以充分共享信息,同时具有控制交互过程的特性,所以近年来越来越多的研究者提出基于显式联合建模的方法。Goo 等人提出 Slot-Gated 模型[29],该模型允许槽位填充可以根据学习到的意图设</p><p>置条件,在ATIS和Snips两个数据集上的整体准确率分别达到了82.6%和74.6%。Li 等人提出了一种新的 Self-Attention 模型[30],通过意图来引导槽位填充。<em class='similar'>Qin 等人提出 Stack-Propagation 模型</em>[31],<em class='similar'>直接使用意图检测结果来指导槽位填充,</em>并使</p><p>用字符级别(Token-level)信息来缓解二者间的误差传递,在 ATIS 和 Snips 两个数据集上分别达到了86.5%和86.9%的整体准确率。Niu 等人提出了一种新颖的</p><p>SF-ID 模型[32],为意图和槽位提供了双向关联机制,在 ATIS 和 Snips 两个数据集</p><p>上实现了86.90%和80.57%的整体准确率。Zhang 等人引入动态路径胶囊网络</p><p>(Capsule-NLU)[33],<em class='similar'>将两个任务之间的分层和相关关系纳入其中,</em>在 ATIS 和</p><p>Snips-NLU 两个数据集上分别实现了83.4%和80.9%的整体准确率。Liu 等人提出</p><p>了一种新型的协同记忆网络(CM-Net)用于联合建模意图检测和槽位填充[34],</p><p>在 ATIS 和 Snips 两个数据集上的意图准确率、槽位 F1值分别为99.10%、96.20%、</p><p>99.29%和97.15%。Zhang 等人将 Graph(图)-LSTM 结构引入 NLU 任务中[35],在 ATIS 和 Snips 两个数据集上取得了很好的性能,整体准确率分别为87.57%和</p><p>89.71%。<em class='similar'>Qin 等人通过建立两个任务之间的双向连接,</em><em class='similar'>提出了一种考虑交叉影响的 Co-interactive Transformer 模型</em>[36],在 ATIS 和 Snips 两个数据集上达到了87.4%和90.3%的整体准确率。随着计算资源的不断增强,以 BERT(Bidirectional Encoder Representation from Transformers)[37]为代表的各种预训练模型(Pre-</p><p>trained Language Models,PLMs)在 NLP 任务中取得了振奋人心的结果。例如,Chen 等人使用 BERT 提取共享文本特征用于意图检测和槽位填充,在原始模型的</p><p>基础上取得了显著的性能提升[38]。</p><p>综上所述,基于隐式联合建模的方法只是通过共享字符编码来隐式地考虑两个任务之间的相互连接,而基于显式联合建模的方法构建了交互模块,子任务间存在信息交互通道,使得模型性能有了进一步的提高。但是该类模型在交互模块中没有高效的交互信息融合手段,导致模型交互能力不足。虽然可以在特征提取</p><p>重庆邮电大学硕士学位论文</p><p>阶段引入 PLMs 来补充丰富的语义特征,但 PLMs 的参数量往往非常惊人。以谷</p><p>歌推出的基础模型Bert-base-uncased[37]为例,<em class='similar'>其网络层数为12,</em><em class='similar'>隐藏层维度为768,</em></p><p>拥有12个注意力头,参数量达到1.1亿,如此庞大的模型难以部署于车载平台(嵌入式设备)上。因此,如何在不引入 PLMs 的前提下,构建意图和槽位的双向交互通道,高效融合所有的交互信息,深入挖掘跨任务的语义特征,提升模型的性能,在计算资源有限的车载平台上实现高效的对话理解是亟待解决的问题,也是学术界当前研究的热点。</p><p>1.3论文研究内容</p><p>针对上述问题,论文面向计算资源有限的车载嵌入式设备,重点研究针对ASR 和 NLU 的技术创新应用方法。首先提出了一种基于残差分组线性变换解码器的 ASR 模型,通过在解码器中引入残差分组线性变换,实现模型参数量和计算复杂度的大幅降低;然后提出了一种基于标签感知图交互的 NLU 模型,通过在交互模块中引入标签映射模块和全局图交互模块,实现模型交互能力和预测精度的提高;最后将 ASR 模型和 NLU 模型集成、移植至车载嵌入式设备上,根据实际</p><p>用车环境围绕搭建全套硬件平台,实现数据安全、自然实时的离线智能语音对话。</p><p>具体涉及以下研究内容:</p><p>(1)基于残差分组线性变换解码器的自动语音识别</p><p>针对基于深度编—解码器的自动语音识别模型参数量庞大的问题,提出了一种基于残差分组线性变换的解码器结构,该结构关键模块为&quot;钻石&quot;型缩放单元,其内部采用稀疏连接,同一组神经元共享相同的权重矩阵,实现模型参数量和计算复杂度的降低。</p><p>(2)基于标签感知图交互的自然语言理解</p><p>针对基于显式联合建模的自然语言理解模型交互能力不足的问题,提出了一种基于标签感知的图交互模型,其中标签映射模块可以获取原始话语与标签语义之间的相关性以提供丰富的先验知识,全局图交互模块可以对语句级别的意图—槽位交互过程进行建模以提供全局优化,实现模型交互能力和预测精度的提高。</p><p>(3)面向车载嵌入式设备的本地智能语音对话系统</p><p>针对&quot;云—端&quot;方式运行的车载智能语音对话存在数据安全隐患的问题,搭建了面向车载嵌入式设备的本地智能语音对话系统。具体地,首先选取 Nvidia Jetson TX2作为车载嵌入式设备并进行刷机、配置环境等操作,然后根据实际应用场景收集、创建驾驶数据集,接着将研究内容(1)和(2)的模型在驾驶数据集上进行训练,最后集成、移植网络模型至 TX2并围绕搭建全套硬件平台,实现数据安全、自然实时的离线智能语音对话。</p><p>1.4论文组织结构</p><p><em class='similar'>论文共分为六章,</em><em class='similar'>如图1-3所示,</em><em class='similar'>各章节安排如下:</em></p><p>图1-3论文组织结构</p><p>Fig.1-3 Structure of the thesis</p><p>第一章,绪论。首先阐述论文的研究背景及意义,然后介绍国内外对于自动语音识别和自然语言理解领域的研究现状,针对当前存在的一些关键科学问题,</p><p>列出论文的研究内容并展示组织结构;</p><p>第二章,对话系统基础理论。首先介绍对话系统基本组成结构,同时引入深度学习领域内相关基础理论,然后在此基础上详细叙述自动语音识别和自然语言</p><p>理解的主流建模方法,为后续章节内容进行铺垫;</p><p>第三章,基于残差分组线性变换解码器的自动语音识别。首先基于残差分组线性变换构建&quot;钻石&quot;型缩放单元,然后基于缩放单元搭建改进的编—解码器模</p><p>重庆邮电大学硕士学位论文</p><p>型,实现参数量和计算复杂度的大幅度降低,<em class='similar'>最后通过一系列对比实验和消融实验论证所提模型的有效性。</em></p><p>第四章,基于标签感知图交互的自然语言理解。首先基于最佳线性逼近思想构建标签映射模块,然后基于图注意力网络搭建全局图交互模块,接着融合上述两个模块形成基于标签感知的图交互模型,实现交互能力和预测精度的提高,<em class='similar'>最后通过一系列对比实验和消融实验论证所提模型的有效性。</em></p><p>第五章,面向车载嵌入式设备的本地智能语音对话系统。首先对车载嵌入式设备进行选型,然后根据实际应用场景收集、创建驾驶数据集,接着将前文提出的自动语音识别模型和自然语言理解模型在驾驶数据集上进行训练,最后将训练好的网络模型集成、移植至车载嵌入式设备上并围绕搭建硬件平台,并完成系统测试以论证其有效性。</p><p><em class='similar'>第六章,</em><em class='similar'>总结与展望。</em><em class='similar'>总结论文研究内容,</em><em class='similar'>并对后续研究工作做出展望。</em></p><p>Equation Chapter 2 Section (Next)</p><p>第2章对话系统基础理论</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第2章 对话系统基础理论</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>2.1对话系统基本组成结构</p><p><em class='reference'>对话系统历经半个多世纪的发展,取得了很大的研究成果。现有对话系统按类型主要可分为闲聊型对话系统(Chatbot)和任务驱动型对话系统(</em><em class='reference'>Task-oriented Dialogue System)两类[39]。此外,按照对话系统回复时需考虑的轮数还可以将其分为单轮对话系统和多轮对话系统。值得注意的是,不管是闲聊对话系统还是任务驱动型对话系统都可以考虑单轮或多轮对话,</em>而且在实际应用中,二者经常混合出现,并非是截然分离的[40]。</p><p>早年的对话系统大多使用手动特性或人工设计的规则来表示用户状态,这种方法不仅耗费人<em class='reference'>力,而且限制了对话系统的扩展应用。近年来,研究人员热衷于研究基于深度学习的对话系统,其通过学习特征的高维表示来缓解这些问题。目前,基于深度学习的任务型对话系统有两种方法:流水线(Pipeline)方法端到端</em></p><p>(End-to-End)方法[41]。前者是目前业界的主流方法,而后者由于存在不可控性</p><p>和不可解释性等问题,当前还处于探索阶段[42]。</p><p>基于 Pipeline 方法的对话系统如图2-1所示,主要包含自动语音识别(ASR)、</p><p><em class='similar'>自然语言理解</em><em class='similar'>(NLU)、</em><em class='similar'>对话状态跟踪</em><em class='similar'>(Dialogue State Tracking,</em><em class='similar'>DST)、</em><em class='similar'>对话策略学习</em><em class='similar'>(Dialogue Policy Learning,</em><em class='similar'>DPL)、</em><em class='similar'>自然语言生成</em><em class='similar'>(Nature Language Generation,</em><em class='similar'>NLG)、</em>语音合成(Text-to-Speech,TTS)等6个模块。其中,ASR将用户语音转换为系统可接收的文本输入,NLU 将文本输入解析为包含意图和槽位的结构化数据。它们为下游模块提供了必要的信息,同时也是对话系统的重要支撑,故论文主要研究针对 ASR 和 NLU 的技术创新应用方法。</p><p>图2-1基于 Pipeline 方法的对话系统</p><p>Fig.2-1 Dialogue system based on pipeline method</p><p>重庆邮电大学硕士学位论文</p><p>2.2神经网络基础理论</p><p>常见的 ASR 和 NLU 建模方法是基于深度学习,<em class='similar'>使用不同类型的神经网络进行各种拓扑组合,</em><em class='similar'>从而提升模型的各种性能。</em><em class='similar'>鉴于此,</em>在介绍常见的ASR和NLU建模方法之前,<em class='similar'>本节先介绍在其中常用的前馈神经网络、</em><em class='similar'>卷积神经网络和循环神经网络及其变种。</em></p><p>2.2.1前馈神经网络</p><p><em class='similar'>前馈神经网络</em><em class='similar'>(Feedforward Neural Network,</em><em class='similar'>FNN)[43]</em><em class='similar'>是最简单的神经网络之一,</em><em class='similar'>同时也是卷积神经网络和循环神经网络的基本组成单元。</em><em class='similar'>以经典的三层前馈神经网络为例,</em><em class='similar'>其结构如图2-2所示。</em><em class='similar'>所有神经元按层组织排列,</em><em class='similar'>第一层称为输入层,</em><em class='similar'>中间层称为隐藏层,</em><em class='similar'>最后一层称为输出层。</em><em class='similar'>前一层的每个神经元会向后一层所有与之相连接的神经元传输信号。</em><em class='similar'>由于前、</em><em class='similar'>后层的神经元都是&quot;全连接&quot;的,</em><em class='similar'>故前馈神经网络也称为全连接神经网络。</em></p><p>图2-2三层前馈神经网络结构</p><p>Fig.2-2 Structure of three-layer FNN</p><p>FNN 进行信息前向传播的方式如下:</p><p>(2-1)</p><p>式中,表示当前层数;<em class='similar'>表示第层神经元的输出向量;</em><em class='similar'>表示第层到第层的权重矩阵;</em><em class='similar'>表示第层神经元的激活函数;</em><em class='similar'>表示第层的偏置向量。</em>如果引入更多层数,FNN 能更好地对&quot;输入—输出&quot;关系进行建模。本质上,</p><p><em class='similar'>对神经网络进行训练就是通过一些特定的算法不断更新网络参数和,</em>使得其</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 2 章 对话系统基础理论</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p><em class='similar'>在某一组参数配置下对于输入的拟合能力达到最优。</em><em class='similar'>目前最常见的参数更新算法是反向传播</em><em class='similar'>(Back Propagation,</em><em class='similar'>BP)</em><em class='similar'>算法</em>[44]。<em class='similar'>BP 算法首先根据输出结果与参考结果的偏差计算损失函数的值,</em><em class='similar'>然后使用链式法则从输出层往输入层方向逐层求取各参数梯度,</em><em class='similar'>最后将学习率与所求梯度相乘以更新参数。</em></p><p>2.2.2卷积神经网络</p><p><em class='similar'>卷积神经网络</em>[45]是一种特殊的 FNN,<em class='similar'>具有局部连接和权值共享等特点。</em>相较</p><p>于 FNN,它能以<em class='similar'>较少的参数量实现对局部特征的提取,</em><em class='similar'>其中一个简单的卷积操作如图2-3所示。</em><em class='similar'>其中卷积滤波器又称为卷积核,</em><em class='similar'>是一组可学习权值的集合,</em><em class='similar'>主要作用是将输入图像中的一个小区域像素加权平均为输出图像中的每个对应像素。</em></p><p><em class='similar'>首先卷积核在输入图像上按照特定的方向移动,</em><em class='similar'>扫描视野范围内的像素信息进而提取特征。</em><em class='similar'>每个卷积核提取得到的特征是尺寸更小的单通道图片,</em><em class='similar'>即为特征图;</em><em class='similar'>然后在非线性激活函数的作用下,</em><em class='similar'>对每张特征图进行处理并对得到的激活特征图按顺序进行通道维度上的堆叠,</em><em class='similar'>即可得到最后的输出结果。</em>通常,<em class='similar'>在进行卷积操作后,</em><em class='similar'>还会使用池化操作对特征图进行裁剪,</em><em class='similar'>进一步压缩特征图大小。</em><em class='similar'>通过对卷积核数、</em><em class='similar'>尺寸等超参数进行合适的设置,</em><em class='similar'>可实现CNN对图像信息特征的扩展与丰富。</em></p><p>图2-3卷积神经网络中的卷积操作</p><p>Fig.2-3 Convolution operation in CNN</p><p>2.2.3循环神经网络及其变种</p><p><em class='similar'>循环神经网络</em>[46]<em class='similar'>是一种处理序列数据或时序数据的递归神经网络,</em><em class='similar'>其网络拓</em></p><p>扑结构在时间维度上以递归形式展开,如图2-4所示。</p><p>重庆邮电大学硕士学位论文</p><p>图2-4循环神经网络拓扑结构</p><p>Fig.2-4 Topology of RNN</p><p><em class='similar'>RNN 的一个主要特点是:</em><em class='similar'>在前一时间步计算得到的隐藏状态会作为当前时间步的输入。</em><em class='similar'>在时间步,</em><em class='similar'>RNN 接收当前时间步的输入和前一时间步的隐藏状态</em></p><p><em class='similar'>,产生当前时间步的输出,</em><em class='similar'>计算过程如下:</em></p><p>(2-2)</p><p>(2-3)</p><p>上式中,<em class='similar'>为作用于前一时间步隐藏状态的权重矩阵;</em><em class='similar'>为作用于输入的权重矩阵;</em><em class='similar'>为作用于输出的权重矩阵;</em><em class='similar'>和均为非线性激活函数。</em><em class='similar'> RNN 的结构决定了其在处理时序信息上具有天然的优势。</em>然而,<em class='similar'>RNN 在反向传播过程中产生的梯度会随着时间推移逐渐放大或衰减。</em><em class='similar'>这一现象在RNN处理具有较长时间依赖性的时间序列尤为明显,</em><em class='similar'>其梯度放大或衰减的程度趋近于指数级,</em><em class='similar'>即梯度爆炸和梯度消失。</em>为了缓解这一问题,<em class='similar'>改善RNN的性能,</em><em class='similar'>近年来许多研究人员对此提出了一些改进措施并取得了不错的效果。</em></p><p><em class='similar'>长短时记忆神经网络</em>[47]<em class='similar'>是一种改进的 RNN,</em>其中引入的门控机制能够有效地</p><p><em class='similar'>缓解 RNN 存在的梯度消失和梯度爆炸问题,</em><em class='similar'>其基本结构如图2-5所示。</em>结合LSTM 前向计算过程说明遗忘门<em class='similar'>(Forget Gate)</em><em class='similar'>、输入门</em><em class='similar'>(Input Gate)</em><em class='similar'>和输出门</em><em class='similar'>(Output Gate)</em><em class='similar'>三个门控单元的作用。</em></p><p><em class='similar'>遗忘门决定当前时间步哪些输入将被丢弃。</em>具体地,<em class='similar'>它将前一时间步的输出和当前时间步的输入拼接作为真正的输入,</em><em class='similar'>通过非线性激活函数将输出范围固定为,</em><em class='similar'>越接近1说明遗忘门越倾向于保留该输出,</em><em class='similar'>计算过程如下:</em></p><p>(2-4)</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 2 章 对话系统基础理论</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p><em class='similar'>图2-5长短时记忆神经网络基本结构</em></p><p>Fig.2-5 Basic structure of LSTM</p><p><em class='similar'>输入门产生输入的映射,</em><em class='similar'>计算过程如下:</em></p><p>(2-5)</p><p>随后会将输入更新到网络状态中,即状态更新,其计算过程如下:</p><p>(2-6)</p><p>(2-7)</p><p>输出门产生输出,<em class='similar'>并将其与前一时间步的单元状态交互得到输出,</em>其计</p><p>算过程如下:</p><p>(2-8)</p><p>(2-9)</p><p>上式中,、、<em class='similar'>和分别表示遗忘门、</em><em class='similar'>输入门、</em>候选状态和输出门;表示 Sigmoid 函数;<em class='similar'>和分别表示权重矩阵和偏置向量;</em><em class='similar'>表示按元素相乘。</em></p><p>2.3基于编—解码器结构的自动语音识别</p><p><em class='similar'>编—解码器结构最先应用于机器翻译领域</em>[48]。<em class='similar'>在该结构中,</em><em class='similar'>编码器首先将输入文本编码成向量序列,</em><em class='similar'>然后解码器根据该向量序列逐步输出文本。</em><em class='similar'>在每一个输出步骤中,</em><em class='similar'>解码器基于注意力机制为向量序列分配不同的权重,</em><em class='similar'>且下一个输出将由历史输出序列和该向量序列的加权和决定。</em><em class='similar'>编—解码器结构非常适合用于自动语音识别领域,</em>这是由该结构中的注意力机制和自动语音识别任务本身特点共同决定的,具体如下:本质上自动语音识别任务和机器翻译任务都属于&quot;序列—序</p><p>重庆邮电大学硕士学位论文</p><p>列&quot;任务,<em class='similar'>即将输入序列识别为输出序列;</em><em class='similar'>编—解码器结构使用的注意力机制可以隐式学习输入序列与输出序列之间的软对齐关系,</em><em class='similar'>而不需要将数据提前对齐;</em><em class='similar'>编码器产生的向量序列不再是固定长度的单个向量,</em><em class='similar'>因此可以处理较长的语音输入序列。</em></p><p>基于以上原因,编—解码<em class='similar'>器结构在自动语音识别领域受到了广泛关注,</em>其结构如图2-6所示。<em class='similar'>其中,</em><em class='similar'>编码器将输入特征序列映射为抽象的高级表征并传递给解码器,</em><em class='similar'>解码器根据接收信息预测输出序列。</em>通常,<em class='similar'>预测特定所需的信息仅依赖于少数关键输入帧,</em><em class='similar'>因此在解码时不需要考虑每一个输入帧。</em><em class='similar'>注意力机制通过为每一对输入帧和分配相应的匹配分数来完成输入序列和输出序列的对齐</em><em class='similar'>,匹配分数的高低代表特定输入帧与的相关程度,</em><em class='similar'>解码器对每个输入帧的关注程度进行决定进而预测输出序列。</em></p><p>图2-6编—解码器结构</p><p>Fig.2-6 Structure of Encoder-Decoder</p><p>具体地,<em class='similar'>以加性注意力机制为例,</em><em class='similar'>首先使用解码器前一时间步的状态和</em></p><p>编码器得到的某一时刻状态计算注意力得分,计算过程如下:</p><p>(2-10)</p><p>式中,和分别为可训练的权值向量和偏置向量;和均为可训练的权重矩</p><p>阵。<em class='similar'>然后使用 Softmax 函数进行归一化,</em><em class='similar'>计算过程如下:</em></p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 2 章 对话系统基础理论</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>(2-11)</p><p>再加权求和得到输出向量:</p><p>(2-12)</p><p><em class='similar'>最后解码器根据</em>、<em class='similar'>前一时间步输出向量和状态向量得到当前时间步的状态向量和输出向量,</em>经过 Softmax 函数之后得到预测的输出序列。</p><p>(2-13)</p><p>(2-14)</p><p><em class='similar'>除了上述加性注意力机制外,</em><em class='similar'>目前使用较为广泛的注意力机制还有缩放点积注意力</em><em class='similar'>(Scaled Dot-Product Attention,</em><em class='similar'>SDPA)</em>机制[49]、<em class='similar'>多头注意力</em><em class='similar'>(Multi-Head Attention,</em><em class='similar'>MHA)</em>机制[17]和局部注意力<em class='similar'>(Local Attention,</em><em class='similar'>LA)</em>机制[50]。通常,<em class='similar'>为了从不同角度获取各种层次的信息,</em><em class='similar'>编—解码器结构中会使用不同的注意力机制,</em><em class='similar'>所以目前出现了各种类型的编—解码器网络,</em><em class='similar'>如LAS模型</em>[51]、<em class='similar'>Transformer模</em></p><p>型等。其中,Transformer 模型以极高的识别准确度和便捷的建模方式受到广泛关</p><p>注[52]<em class='similar'>。论文第三章将详细探讨 Transformer 模型,</em><em class='similar'>并在此基础上进行改进。</em></p><p>2.4基于显式联合建模的自然语言理解</p><p><em class='similar'>在自然语言理解领域,</em><em class='similar'>考虑到意图检测和槽位填充两个子任务之间的密切相关性,</em>主要采用联合建模的方式来利用两个子任务间的共享特征,主要可分为隐式联合建模和显式联合建模两种。前者仅采用共享编码器直接整合共享信息,而后者捕获特征后依旧通过交互模块进行交互,可以充分共享信息,从而提升模型性能,同时明确其交互过程有助于提高模型可解释性[22]。</p><p>以文献[53]提出的模型为例,其结构如图2-7所示,主要包含两个相互连接的双向 LSTM(Bi-LSTM),分别用于意图检测和槽位填充。每个 Bi-LSTM 双向读取输入序列并生成一系列隐藏状态,其中对应用于意图检测的网络,对应用于槽位填充的网络。</p><p>具体地,在意图检测的网络中,隐藏状态与来自另一个 Bi-LSTM()</p><p>产生的隐藏状态连接,计算时间步上的状态,最后输出预测的意图标</p><p>签,其计算过程如下:</p><p>(2-15)</p><p>重庆邮电大学硕士学位论文</p><p>(2-16)</p><p>式中,表示所有意图标签在最后一个时间步上的预测概率。</p><p>图2-7简单的显式联合建模</p><p>Fig.2-7 Simple explicit joint modeling</p><p>在槽位填充的网络中,计算过程与上式类似。区别之处在于,由于属于序列</p><p>标注领域,在每个时间步都有一个输出,计算过程如下:</p><p>(2-17)</p><p>(2-18)</p><p>式中,表示在时间步上的预测槽位标签。上述例子是一个经典的显式联合建模方法实例,论文第四章将在此方法基础上进行改进。</p><p>2.5本章小结</p><p>本章作为论文的相关基础理论章节,<em class='similar'>主要包含三部分。</em><em class='similar'>第一部分主要介绍了对话系统的基本组成结构,</em>然后阐述了常用的几种深度神经网络;第二部分主要介绍了基于编—解码器结构的自动语音识别方法;第三部分主要介绍了基于显式</p><p>联合建模的自然语言理解方法。<em class='similar'>这一章节为后续第三章、</em><em class='similar'>第四章的内容做下铺垫。</em></p><p>Equation Chapter (Next) Section (Next)</p><p>第3章基于残差分组线性变换解码器的自动语音识别</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第3章 基于残差分组线性变换解码器的自动语音识别</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>3.1引言</p><p><em class='similar'>Transformer 模型是近年较为流行的一种深度编—解码器模型,</em><em class='similar'>因其对语音输入—输出序列有强大的建模能力和优秀的识别能力,</em><em class='similar'>在自动语音识别任务中显现了良好的性能,</em>受到了广泛的关注。<em class='similar'>随着研究的深入,</em><em class='similar'>逐渐衍生出了各式各样的变种模型。</em>然而,<em class='similar'>Transformer 模型的性能往往与编码器块和解码器块的深度呈正相关。</em><em class='similar'>此外,</em><em class='similar'>模型维度越大,</em>模型的性能、<em class='similar'>识别准确率和泛化能力也越高,</em>但这些都会引发模型中权重矩阵维度的增加,进而导致模型参数量和计算复杂度的剧烈增长。例如,<em class='similar'>Pham 等人构建了一个包含48个编码器块和48个解码器块的Transformer 模型,</em><em class='similar'>其参数量达到了252M</em><em class='similar'>[54];</em><em class='similar'>Synnaeve 等人搭建了一个包含24个编码器块的 Transformer 模型,</em><em class='similar'>在模型维度为1024的情况下,</em><em class='similar'>其参数量达到了</em></p><p><em class='similar'>270M[55]</em><em class='similar'>。为了降低基于 Transformer 模型的参数量,</em><em class='similar'>研究人员做出了各种尝试。</em></p><p>Luo 等人提出了一种简化的自注意力机制[56],使用前馈顺序记忆网络</p><p>(Feedforward Sequential Memory Network,FSMN)[57]代替映射层实现状态序列的转换,使得模型参数量下降了20%。但是,这种简单的替换并不能使得模型表现出良好的性能,而且很少有相关研究将这种轻量级网络应用于实际。</p><p>为解决上述问题,本章工作在原始 Transformer 模型的基础上,<em class='similar'>侧重于以识别准确率的小幅度降低为代价,</em><em class='similar'>实现模型</em>参数量和计算复杂度的降低,具体地,通过将机器翻译领域提出的 DeLighT 模块[58]改进、嵌入至解码器中,使得模型在训练过程中可以适应向量维度和深度的变化,<em class='similar'>从而在增加模型宽度和深度的同时能减少参数量和计算量。</em></p><p>3.2 Transformer 模型及其组件</p><p><em class='similar'>Transformer 模型如图3-1所示,</em><em class='similar'>可以看出是一个典型的编—解码器结构。</em><em class='similar'>左侧为编码器,</em><em class='similar'>由若干个块</em><em class='similar'>(Block)</em>堆叠而成,<em class='similar'>每个块中包含两个子层,</em><em class='similar'>分别为多头注意力层和前馈网络层</em><em class='similar'>(Feedforward Network Layer,</em>FNL);<em class='similar'>右侧为解码器,</em>其结构与编码器类似。不同之处在于,<em class='similar'>解码器块将多头注意力层替换为掩膜</em>(Masked)<em class='similar'>多头注意力层,</em><em class='similar'>并在之后级联了一个多头注意力层用于获取预测字符与语音特征间的相关性。</em><em class='similar'>每个子层之间都使用残差连接进行耦合,</em><em class='similar'>并在之后使用层归一化</em><em class='similar'>(Layer Normalization)</em><em class='similar'>对输出进行调整。</em>下面对 Transformer 模型的几</p><p>重庆邮电大学硕士学位论文</p><p>个重要组件进行简介。值得注意的是,<em class='similar'>在较新的文献中,</em><em class='similar'>常用线性层指代全连接层</em>[59]。<em class='similar'>因此,</em><em class='similar'>后续对这两种说法不加区分。</em></p><p>图3-1 Transformer 模型结构</p><p>Fig.3-1 Structure of Transformer</p><p>3.2.1自注意力机制</p><p><em class='similar'>Transformer 模型中的编码器和解码器通过频繁地使用自注意力机制,</em><em class='similar'>从不同表示子空间中获取丰富的信息。</em><em class='similar'>自注意力机制使用点积运算来获取序列中任意位置元素间的相关性。</em></p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 3 章 基于残差分组线性变换解码器的自动语音识别</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>假设<em class='similar'>一个长度为</em>、<em class='similar'>向量维度</em><em class='similar'>(模型维度)</em><em class='similar'>为的序列为自注意力在表示子空间上的输入。</em><em class='similar'>在中,</em><em class='similar'>自注意力首先使用由三个不同的权重矩阵</em></p><p><em class='similar'>形成的全连接层将映射到向量维度为的三个序列,</em>计算过程如下:</p><p>(3-1)</p><p><em class='similar'>式中,</em><em class='similar'>,分别表示查询</em><em class='similar'>(Query)</em><em class='similar'>向量序列、</em><em class='similar'>键(</em><em class='similar'>Key)</em><em class='similar'>向量序列和值</em><em class='similar'>( Value )</em><em class='similar'>向量序列;</em><em class='similar'>,分别为对应的全连接网络权重矩阵。</em></p><p>然后将和的转置进行点积运算并使用进行缩放,便于产生的梯度能顺利地进行反向传播,接着使用 Softmax 函数进行概率化表示,进而得到二者之间的注意力权重矩阵,最后将该矩阵作用于即可得到的输出,计算</p><p>过程如下:</p><p>(3-2)</p><p>在此<em class='similar'>基础上</em>,<em class='similar'>将至中各个输出序列按维度拼接,</em><em class='similar'>通过一个全连接层即可</em></p><p><em class='similar'>得到多头注意力的输出,</em><em class='similar'>计算过程如下:</em></p><p>(3-3)</p><p>式中,表示拼接操作;,表示输出权重矩阵。</p><p>3.2.2前馈网络层</p><p><em class='similar'>在 Transformer 模型中,</em><em class='similar'>模型维度通常指各模块输出向量的维度,</em><em class='similar'>取值常为256、</em><em class='similar'>512或1024</em><em class='similar'>(2的整数幂)</em><em class='similar'>。前馈网络层本质上是两层全连接网络,</em><em class='similar'>第一层全连接网络将扩张到更大的维度,</em><em class='similar'>第二个全连接网络再将维度还原。</em>前馈网</p><p>络层的主要作用是增强模型的拟合能力,其前向传播的计算过程如下:</p><p>(3-4)</p><p>式中,<em class='similar'>表示激活函数;</em><em class='similar'>,分别表示两个全连接网络的权重矩阵;</em><em class='similar'>,分别表示两个全连接网络的偏置向量。</em></p><p>3.2.3位置编码</p><p>输入的语音序列和预测的字符序列都属于时间序列,具有特定的排列顺序,</p><p><em class='similar'>且富含上下文语义信息,</em><em class='similar'>因此引入位置信息有助于增强表示序列内部关系的能力。</em></p><p>由于 Transformer 模型内部不存在与 RNN 类似的循环结构,无法利用其天然包含</p><p>重庆邮电大学硕士学位论文</p><p>的时序位置信息,<em class='similar'>注意力机制就会在不同上下文环境中输出相同的状态序列,</em><em class='similar'>进而导致位置信息完全丢失。</em>可见在序列中附加各个向量的位置信息,对模型的训练学习是非常必要的。</p><p>因此,<em class='similar'>Transformer 模型中引入了位置编码</em><em class='similar'>(Positional Encoding,</em><em class='similar'>PE)</em><em class='similar'>来表示序列内部向量间的位置信息。</em><em class='similar'>将 PE 与输入序列进行加性组合,</em><em class='similar'>使得输入序列附带关于序列内部的位置信息,</em><em class='similar'>使得模型自身具备学习位置信息的能力。</em></p><p>具体地,假设序列长度为,表示第个时间步上维度为的位置编</p><p>码向量,其中每个元素的计算过程如下:</p><p>(3-5)</p><p>可以看出,每个元素正、余弦值的频率随位置的值的增加而减少。对于输入序列中的每个向量,线性加上对应的位置编码向量即可得到含有位置信息的模型向量。</p><p>3.3基于残差分组线性变换的&quot;钻石&quot;型缩放单元</p><p>本节首先介绍分组线性变换的特点,给出其计算过程,然后在此基础上加入残差连接,构建包含维度扩张和维度收缩两个阶段的&quot;钻石&quot;型缩放单元,<em class='similar'>同时给出其数学定义和拓扑描述。</em></p><p>3.3.1分组线性变换</p><p><em class='similar'>与普通全连接层使用的线性变换不同,</em><em class='similar'>分组线性变换允许下层网络中的一部分神经元与上一层网络的部分神经元全连接,</em><em class='similar'>如图3-2所示。</em><em class='similar'>在不同的网络层,</em><em class='similar'>通过设定神经元被划分的组数,</em><em class='similar'>可以控制整个网络的参数量。</em>具体地,<em class='similar'>对于一个层全连接网络,</em><em class='similar'>与分别表示第层的输入和输出,</em><em class='similar'>表示第层的神经元的划分组数,</em>那么其参数量为,若采用分层分组线性变换,参数量为</p><p><em class='similar'>。可见,</em><em class='similar'>在网络层数和每层神经元个数相同的情况下,</em><em class='similar'>引入分组线性变换后,</em>参数量可下降至原始的。</p><p>分组线性变换第层中的每个神经元,都能通过多种途径到达输入向量中的某个输入神经元,以降低流动过程中的信息损失。具体地,<em class='similar'>假设输入向量和输出向量分别为和,</em><em class='similar'>分组线性变换首先根据维度和构建个维度逐渐增加的中间层。</em><em class='similar'>因此,</em><em class='similar'>第层的输出向量比第层拥有更大的维度。</em><em class='similar'>若需要进行变换的向量维度能够被设置的最大组数整除,</em><em class='similar'>某一层的输出向量的</em></p><p>计算过程如下:</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 3 章 基于残差分组线性变换解码器的自动语音识别</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>(3-6)</p><p>式中,表示分组线性变换操作;<em class='similar'>表示第层的可训练的权重矩阵;</em>表示第层的划分组数,值为。</p><p>图3-2分组线性变换</p><p>Fig.3-2 Group linear transformation</p><p>3.3.2&quot;钻石&quot;型缩放单元</p><p><em class='similar'>在分组线性变换的基础上,</em>加入残差连接,<em class='similar'>可以构建包含维度扩张和维度收缩两个阶段的&quot;钻石&quot;型缩放单元,</em>如图3-3所示。<em class='similar'>缩放单元由5个配置参数决定:</em><em class='similar'>深度</em><em class='similar'>(层数)</em><em class='similar'>、宽度因子、</em><em class='similar'>输入维度、</em><em class='similar'>输出维度和分组线性变换的最大组数。</em></p><p>如图3-3(b)所示,缩放单元首先将输入向量扩张到更高维度(扩张阶段),</p><p>使网络具有比较强大的学习能力,再将维度收缩以匹配模型的维度(收缩阶段),</p><p>进而减少整个模型的参数量。具体地,在扩张阶段,维度为的输入向量在前</p><p><em class='similar'>层被逐层映射到最高维度为的向量;</em><em class='similar'>在收缩阶段,</em><em class='similar'>最高维度向量在剩下的层变换为输出维度。</em></p><p>缩放单元的残差分组线性变换计算步骤如图3-3(a)所示,包含切分混合和分组线性变换两个主要操作,<em class='similar'>分组线性变换的输入为原始输入序列或中间层输出结果,</em><em class='similar'>表示切分混合函数。</em></p><p>重庆邮电大学硕士学位论文</p><p>图3-3&quot;钻石&quot;型缩放单元的结构。(a)残差分组线性变换;(b)拓扑结</p><p>构;(c)逐块缩放策略</p><p>Fig.3-3 Structure of the &quot;diamond&quot; scaling unit.(a) Residual grouping linear</p><p>transformation;(b) Topology;(c) Block-wise scaling strategy</p><p><em class='similar'>以第2层为例,</em><em class='similar'>首先将上一层的输出和原始输入根据该层组数按照相同规律进行切分,</em>得到、、和4个中间向量,前两者来自于,后两者来自于;<em class='similar'>然后按特定顺序合并和、</em>和,<em class='similar'>得到的两个向量即为的输</em></p><p>出;最后将结果输入得到最终输出。的计算过程如下:</p><p>(3-7)</p><p>(3-8)</p><p>上式中,<em class='similar'>和分别表示第层的权重矩阵和偏置向量。</em></p><p><em class='similar'>对于某些层数较深的网络,</em><em class='similar'>可以在其组件中嵌入含有逐块缩放策略的&quot;钻石&quot;型缩放单元,</em>如图3-3<em class='similar'>(c)</em>所示。<em class='similar'>每一个六边形表示一个缩放单元,</em><em class='similar'>对于处于位置</em></p><p>上的缩放单元,其配置参数和的计算过程如下:</p><p>(3-9)</p><p>(3-10)</p><p>上式中,和均<em class='similar'>为超参数</em>,<em class='similar'>分别表示最小深度和最大深度;</em>表示缩放单元总个数。</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 3 章 基于残差分组线性变换解码器的自动语音识别</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>3.4基于&quot;钻石&quot;型缩放单元的改进 Transformer 模型</p><p>本节在原始 Transformer 模型基础上,首先将普通前馈网络层改进为轻量级前馈网络层,然后将上节所述&quot;钻石&quot;型缩放单元嵌入到解码器中,形成轻量级解码器,最终得到改进的、轻量级 Transformer 模型。</p><p>3.4.1轻量级前馈网络层</p><p><em class='similar'>轻量级前馈网络层的结构如图3-4右侧所示,</em><em class='similar'>类似于普通前馈网络层,</em><em class='similar'>也由两层全连接网络组成。</em>区别之处在于不需要对输入序列的维度进行扩张。</p><p>图3-4普通前馈网络层(左侧)和轻量级前馈网络层(右侧)的结构 Fig.3-4 Structure of the normal FNL (left) and the lightweight FNL (right)</p><p>假设<em class='similar'>收缩因子为</em>,<em class='similar'>轻量级前馈网络层的全连接层1将维度为</em><em class='similar'>(和模型维度一致)</em>的序列转变成维度为的序列,<em class='similar'>全连接层2再将序列维度还原至。</em><em class='similar'>假定普通前馈网络层需要先将序列维度扩张倍,</em>那么同等条件下,<em class='similar'>轻量级前馈网络层的参数量是普通前馈网络层的。</em>另外,<em class='similar'>轻量级前馈网络层的激活函数为最近提出的 MISH 激活函数</em>[60]。<em class='similar'>相比于 ReLU 激活函数,</em><em class='similar'>MISH 求得的梯度更加平滑,</em><em class='similar'>更有利于模型的训练。</em></p><p>3.4.2轻量级解码器</p><p>通常,提升 Transformer 模型的表达能力主要有以下三种方法[61]:</p><p><em class='similar'>(1)</em><em class='similar'>宽度缩放:</em><em class='similar'>增加模型中向量维度和各个映射维度;</em></p><p><em class='similar'>(2)</em><em class='similar'>深度缩放:</em><em class='similar'>增加模型深度</em>(层数);</p><p>(3)综合考虑宽度缩放和深度缩放。</p><p>然而,上述方法在某些体量较小的数据集上的表现差强人意。例如,针对THCHS-30数据集[62],<em class='similar'>将 Transformer 模型的模型维度从512增加至1024时,</em>其</p><p>重庆邮电大学硕士学位论文</p><p>参数量扩大了4倍,但性能变化不大。而在 Transformer 模型各模块中均匀地增加模型维度和深度,同样会导致模型参数量的急剧增加。</p><p>为了减少 Transformer 模型的参数量,基于&quot;钻石&quot;型缩放单元的逐块缩放策略,<em class='similar'>在不同的解码器块中嵌入深度、</em><em class='similar'>宽度不均匀的缩放单元,</em><em class='similar'>并将普通的前馈网络层替换为改进的轻量级前馈网络层,</em><em class='similar'>形成轻量级解码器块,</em>可以使得模型在宽度和深度增加的同时进一步减少参数量,其结构如图3-5所示。</p><p>图3-5轻量级解码器结构</p><p>Fig.3-5 Structure of the lightweight Decoder</p><p><em class='similar'>在每一个块中,</em><em class='similar'>首先通过一个缩放单元完成输入序列的深度缩放和宽度缩放,</em><em class='similar'>配置参数中的深度和宽度因子将逐块增加,</em><em class='similar'>越靠近输出端,</em><em class='similar'>缩放单元的结构会变得越深、</em><em class='similar'>越宽;</em><em class='similar'>解码器中的自注意力机制和普通自注意力机制有所不同,</em><em class='similar'>普通自注意力机制使用的三个权重矩阵通常将输入序列维度减小,</em><em class='similar'>而在轻量级解码器块中,</em><em class='similar'>由于缩放单元的输出向量维度较低,</em>因此掩膜自注意力的权重矩阵不</p><p><em class='similar'>必再次减小输入序列的维度,</em><em class='similar'>只需将计算得到的序列通过线性层映射至模型维度;</em></p><p><em class='similar'>之后的编—解码器自注意力也是如此;</em><em class='similar'>最后使用轻量级前馈网络层代替普通前馈网络层进一步降低参数量。</em></p><p>3.4.3改进的 Transformer 模型结构</p><p>在原始 Transformer 模型的基础上,<em class='similar'>采用上述内容对解码器进行优化,</em><em class='similar'>得到如图3-6所示的改进 Transformer 模型。</em>其中,<em class='similar'>解码器由若干个轻量级解码器块堆叠而成,</em><em class='similar'>用于实现整个模型的轻量化。</em></p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 3 章 基于残差分组线性变换解码器的自动语音识别</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>图3-6轻量级 Transformer 模型结构</p><p>Fig.3-6 Structure of the lightweight Transformer</p><p>3.5实验结果及分析</p><p>本节首先介绍实验环境、使用的数据集和评价指标,然后阐述语音信号的频域特征提取方法,并给出详细的语音信号及标签文本预处理流程,接着描述网络模型的训练配置信息及过程,最后通过对比实验结果分析改进模型的性能,同时设计一系列消融实验论证所提方法的有效性。</p><p>3.5.1实验环境、数据集和评价指标</p><p>1)实验环境</p><p>实验环境包含硬件和软件两部分。<em class='similar'>在硬件方面,</em><em class='similar'>主要使用一台高性能服务器完成所有模型的训练,</em><em class='similar'>其主要配置为:</em><em class='similar'>处理器</em><em class='similar'>(Central Processing Unit,</em><em class='similar'>CPU)/2×Intel</em><em class='similar'>(R)</em><em class='similar'> Xeon(</em><em class='similar'>R)</em><em class='similar'> CPU E5-2680 v4@2.40GHz;</em><em class='similar'>内存</em>(Memory)<em class='similar'>/8×ECC Registered DDR4@32GB;</em><em class='similar'>显卡</em><em class='similar'>(GPU)/4×NVIDIA Tesla P100@16GB;</em><em class='similar'>在软件方面,</em><em class='similar'>所有模型的训练都在 Ubuntu 18.04系统下完成,</em><em class='similar'>编程语言为 Python </em>3.6.9,<em class='similar'>深度学习框架为 PyTorch 1.</em>3。详细信息见表3-1。</p><p>重庆邮电大学硕士学位论文</p><p>表3-1实验环境信息</p><p>Table3-1 Experimental environment</p><p>类型型号规格</p><p>硬件</p><p>CPU Intel(R) Xeon(R) CPU E5-2680 v4@2.40GHz 2</p><p>GPU NVIDIA Tesla P10016GB×4</p><p>内存 ECC Registered DDR4@32GB 32GB×4硬盘西部数据 SSD 阵列21TB 麦克风 Actins ATS3605D 1</p><p>软件</p><p>操作系统 Ubuntu 18.04</p><p>编程语言 Python 3.6.9</p><p>深度学习框架 PyTorch 1.6</p><p>2)数据集</p><p>实验使用在自动语音识别领域内应用较为广泛的 AISHELL-1数据集[63](开源,中文)和 TED-LIUM2数据集[64](开源,英文)。其中,AISHELL-1包含近</p><p>170小时的语音数据,语音段数约14万,<em class='similar'>来自于400个来自中国不同口音区域的发言人,</em><em class='similar'>以44.1KHz</em><em class='similar'>(高保真麦克风)</em><em class='similar'>和16kHz</em><em class='similar'>(Android 或 iOS 系统手机)</em>的采</p><p>样率进行录制,<em class='similar'>录制过程在安静室内环境中进行,</em><em class='similar'>内容涉及智能家居、</em><em class='similar'>无人驾驶、</em><em class='similar'>工业生产等11个领域;</em>TED-LIUM2包含约207小时的语音数据,语音段数约9万,来自于1242个发言人的1495段演讲的音频和文字稿,演讲视频全部来自于TED 网站。</p><p>AISHELL-1和 TED-LIUM2都按照一定的比例被划分为训练集(Train)、<em class='similar'>验证集</em>(Val)<em class='similar'>和测试集</em>(Test)三个子集,<em class='similar'>前者的划分比例参照文献</em>[63],后者的划分比例参照文献[65],具体信息见表3-2。另外,AISHELL-1中发言人的年龄、性别和方言信息见表3-3。</p><p>表3-2 AISHELL-1和 TED-LIUM2的详细信息 Table3-2 Detail of AISHELL-1 and TED-LIUM2</p><p>数据集子集语音段数时长(小时)</p><p>AISHELL-1</p><p>Train 120098150</p><p>Val 1432610</p><p>Test 72005</p><p>TED-LIUM2</p><p>Train 91262203</p><p>Val 5801.5</p><p>Test 11342.5</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 3 章 基于残差分组线性变换解码器的自动语音识别</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>表3-3 AISHELL-1中发言人的年龄、性别和方言信息</p><p>Table3-3 The age, gender, and accent of speakers in AISHELL-1</p><p>年龄、性别信息方言信息</p><p>年龄段人数</p><p>比例</p><p>(%)</p><p>男性人数女性人数区域人数</p><p>比例</p><p>(%)</p><p>16—25岁31679140176北方33383</p><p>26—40岁71183635南方3810</p><p>40岁以上133103闽、贵、粤及其他297</p><p>合计400100186214400100</p><p>3)评价指标</p><p>在模型准确度方面,论文选用常用的标签错误率作为评价指标[66]。对于采用</p><p>不同建模单位的数据集,评价指标的计算方式类似。对于 AISHELL-1,因其以汉</p><p><em class='similar'>字单字符作为基本建模单位,</em><em class='similar'>故对应的评价指标为字错误率</em><em class='similar'>(Character Error Rate,</em></p><p>CER);对于 TED-LUM2,因其以一元模型(Unigram)作为基本建模单位,故</p><p><em class='similar'>对应的评价指标为词错误率</em><em class='similar'>(Word Error Rate,</em><em class='similar'>WER)。</em>二者定义如下:</p><p>(3-11)</p><p><em class='similar'>式中,</em><em class='similar'>分别表示相较于参考文本替换/增加/删除的字符</em><em class='similar'>(单词)</em><em class='similar'>数;</em><em class='similar'>表示参考文本的字符</em><em class='similar'>(单词)</em>数目。</p><p>在模型轻量化程度方面,论文选用参数量(Parameters)和计算量作为评价指标。对于前者,深度学习框架 PyTorch 中集成了成熟的计算方式,只需加载模型即可得到具体结果;对于后者,使用常用的乘加累积操作数(Multiplication and</p><p>Accumulations,MACs)[67]进行表示。对于同一个网络,在其他条件相同的情况</p><p>下,<em class='similar'>其 MACs 值约是另一个常见的反映网络复杂度的指标——浮点运算数</em><em class='similar'>(Floating Point Operations,</em><em class='similar'>FLOPs)</em><em class='similar'>的。</em><em class='similar'>对于一层将输入维度映射为输</em></p><p>出维度的全连接网络,其 MACs 值的计算过程如下:</p><p>(3-12)</p><p><em class='similar'>对于较为复杂的卷积操作,</em><em class='similar'>MACs 值的计算过程如下:</em></p><p>(3-13)</p><p><em class='similar'>式中,</em><em class='similar'>分别表示卷积核的高度和宽度;</em><em class='similar'>分别表示输入和输出的通道数;</em><em class='similar'>分别表示输出图像的高度和宽度。</em></p><p>重庆邮电大学硕士学位论文</p><p>在模型实际应用(推理)方面,论文使用 GPU 内存占用率(GPU Memory-Usage Rate,GMUR)和实时因子(Real Time Factor,RTF)[68]分别反映模型的资源占用和识别速度。对于前者,可使用 Nvidia 官方提供的显卡驱动自带工具</p><p>(nvidia-smi,NVSMI)监控 GPU 状态得到,为了保证不受某些特殊因素的影响,</p><p>对于每个模型的 GMUR 值,论文共设计5轮测试并计算其均值作为最终的结果;</p><p>对于后者,其定义如下:</p><p>(3-14)</p><p>式中,表示模型识别所有音频的处理时间;表示所有音频的总时间。</p><p>RTF 值越小表示模型的识别速度越快,小于1才能达到实时效果,通常在</p><p>0.2~0.3。同样地,对于每个模型的 RTF 值,论文共设计5轮测试并计算其均值作为最终的结果。</p><p>3.5.2数据预处理</p><p>1)频域特征提取</p><p>论文使用应用较为广泛的 FBank 特征作为语音信号的频域特征[66]。具体地,</p><p>首先对分帧、<em class='similar'>加窗后的语音帧做快速傅里叶变换</em><em class='similar'>(Fast Fourier Transform,</em><em class='similar'>FFT)</em></p><p>得到频谱,对频谱取模得到振幅;然后使用公式</p><p>将实际频率转化为梅尔(Mel)<em class='similar'>频域,</em><em class='similar'>以便模拟人耳对声音的感知;</em><em class='similar'>最后使用三角滤波器组进行滤波,</em><em class='similar'>再取对数即可得到 FBank 特征。</em></p><p>对于 AISHELL-1,<em class='similar'>论文使用窗口长度为25ms、</em><em class='similar'>帧移为10ms 的80维 FBank特征作为模型输入;</em><em class='similar'>对于 TED-LIUM2,</em><em class='similar'>在80维 FBank 特征的基础上额外增加了</em></p><p>3维 Pitch 特征[69]。在上述原始特征基础上,论文还使用卷积模块对其进一步处理。</p><p><em class='similar'>其中,</em><em class='similar'>卷积核为3×3、</em><em class='similar'>步长为2、</em><em class='similar'>输出通道数为256、</em><em class='similar'>激活函数为 ReLU。</em>在卷积模块的作用下,<em class='similar'>原始特征的时间步长度被压缩至。</em></p><p>2)标签文本处理</p><p><em class='similar'>对于 AISHELL-1,</em><em class='similar'>论文使用不同的4336个字符来构成词汇表</em><em class='similar'>(字典)</em><em class='similar'>。其中4333个字符来自于本身的标注文本,</em>剩下3个为增加的用于辅助<em class='similar'>模型训练的特殊字符,</em>分别为&lt;EOS/BOS&gt;、&lt;PAD&gt;和&lt;UNK&gt;。<em class='similar'>&lt;EOS/BOS&gt;可指示模型开始或停止输出字符序列;</em><em class='similar'>&lt;PAD&gt;可在训练阶段将一个批处理</em>(Batch)<em class='similar'>大小的数据中不同长度的语音特征序列或字符序列填充到相同的长度;</em><em class='similar'>&lt;UNK&gt;可在输出时代替某些在词汇表中未出现的字符,</em><em class='similar'>避免模型训练时出</em>错;对于 TED-LUM2数据集,</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 3 章 基于残差分组线性变换解码器的自动语音识别</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>论文使用 SentencePiece 工具[70]生成建模单位为一元模型的词汇表,共包含500个字符。</p><p>3.5.3训练配置信息及过程</p><p>论文选择领域内近年来一些性能表现优秀的同类模型作为对比组,具体为:TDNN-Chain[71]、LAS[72]、SSAN[56]、Speech-Transformer(非官方开源)[73]、HA-</p><p>Transformer(非官方开源)[74]和 STBD(非官方开源)[75]。由于提出这些方法的</p><p>文献中未涉及评价指标中的 MACs、GMUR 和 RTF 值,且只在单一语种的数据集上进行训练,所以论文根据这些开源方法,逐一搭建了网络模型并在两个目标数据集上完成了训练。表3-4展示了各个模型主要超参数的配置信息,包含编码器激活函数(E-AF)、解码器激活函数(D-AF)、优化器(OP)、热身步数(WU)、学习率(LR)、Dropout 概率(DR)、批处理大小(BS)和迭代次数(EP)。</p><p><em class='similar'>改进的 Transformer 模型由12个编码器块和6个解码器块堆叠而成,</em>表示子空间的个数均为4,模型维度为256,其中缩放单元的参数、、、、和 DR 分别为4、12、6、2、8和0.03。</p><p><em class='similar'>论文使用Kullback-Leibler散度作为模型训练的损失函数,</em>标签平滑度(Label</p><p>Smoothing)设为1,其定义如下:</p><p>(3-15)</p><p>式中,和分别表示第个字符的真实分布和预测分布。</p><p>表3-4各个模型主要超参数的配置信息</p><p>Table3-4 Configuration of the main hyper-parameters of each model</p><p>模型数据集 E-AF D-AF OP WU LR DR BS EP</p><p>Speech-</p><p>Transformer[73]</p><p>AISHELL-1 ReLU ReLU</p><p>Adam</p><p>2.5e41.00.10128100</p><p>TED-LIUM2 GLU GLU 1.2e40.8e-30.15 DBS 150</p><p>HA-</p><p>Transformer[74]</p><p>AISHELL-1</p><p>GLU GLU Adam</p><p>2.5e41.00.10128100</p><p>TED-LIUM21.2e40.2e-20.20 DBS 150</p><p>STBD[75] AISHELL-1 ReLU ReLU Adam 1.6e41.00.1096100</p><p>Proposed</p><p>AISHELL-1</p><p>Swish ReLU Adam</p><p>2.5e41.00.10128100</p><p>TED-LIUM21.2e40.8e-30.10 DBS 150注:STBD 模型在 TED-LIUM2等英文数据集上无法收敛。</p><p>重庆邮电大学硕士学位论文</p><p>与在 AISHELL-1上不同,在 TED-LIUM2上的批处理大小设置为动态的(Dynamic Batch Size,DBS)。在 AISHELL-1/TED-LIUM2上,模型在迭代训练</p><p>100/150次后,将最后30/50次迭代的参数值进行平均形成最终训练好的模型[74]。<em class='similar'>论文使用了近期提出的 SpecAugment</em>[76]<em class='similar'>对训练集数据进行必要的增强。</em>另外,为</p><p>了保证实验结果对比的公平性,在模型构建过程中没有使用任何例如以 C++语言进行编程的加速技巧。训练过程中各个模型在验证集上的损失(Loss)曲线如图</p><p>3-7所示(从第10次迭代开始)。</p><p>参照大多数自动语音识别系统,论文构建了外部语言模型(Language Model,</p><p>LM)。<em class='similar'>在推理阶段,</em><em class='similar'>将原始模型和语言模型的预测结果以浅融合</em>[77]<em class='similar'>的方式综合,</em></p><p>可以进一步提高识别精度。论文针对 AISHELL-1和 TED-LIUM2构建了两个相同</p><p>的具有1024个隐藏单元的两层 LSTM 网络模型作为 LM,BS 分别为512和128,训练语料来自于各自训练集中的标签文本。使用单个GPU进行训练,LMs在迭代训练60次后,将最后10次迭代的参数值进行平均形成最终训练好的模型。</p><p>图3-7训练过程中 AISHELL-1(左侧)和 TED-LIUM2(右侧)验证集</p><p>上各个模型的损失曲线</p><p>Fig.3-7 Loss curves for each model on the validation subset of AISHELL-1</p><p>(left) and TED-LIUM2(right) during training</p><p>3.5.4对比实验结果及分析</p><p>将改进的 Transformer 模型和其他基于 Transformer 的同类模型在 AISHELL-1和 TED-LIUM2上的实验结果进行对比,具体信息分别见表3-5和表3-6。表中标&quot;*&quot;表示未开源的方法,MACs 值是在编码序列长度为500、解码序列长度为30的条件下计算得出,&quot;↓&quot;表示该项指标越低越好,<em class='similar'>粗体数字表示最优结果,</em><em class='similar'>下划线数字表示次优结果。</em></p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 3 章 基于残差分组线性变换解码器的自动语音识别</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>从表3-5中可以看出,在 AISHELL-1上,与对比组中识别效果最好的 HA-Transformer相比,改进模型在测试集上的CER上升了0.71%,RTF值上升了0.015(在实际应用中影响较小),但参数量和计算量分别相对下降了48.35%和77.16%,</p><p>同时 GMUR 也下降了24.94%,引入语言模型后 CER 可进一步降低至6.38%;与对比组中参数量和计算量都最小的 Speech-Transformer 相比,引入语言模型后,验证集和测试集上的 CER 分别下降了0.97%和1.09%,参数量和计算量分别相对下降了27.66%和70.72%,同时GMUR也下降了25.06%。改进模型在TED-LIUM2上的结果与在 AISHELL-1上类似。</p><p>表3-5 AISHELL-1上各个模型的实验结果</p><p>Table3-5 Experimental results of each model on AISHELL-1</p><p>模型</p><p>CER (%)↓ Parameters (M)</p><p>↓</p><p>MACs (B)</p><p>↓</p><p>GMUR (%)</p><p>↓</p><p>RTF</p><p>↓ Val Test</p><p>TDNN-Chain (Kaldi)*[71]7.45</p><p>LAS*[72]10.56</p><p>SSAN*[56]6.8436.0</p><p>Speech-Transformer[73]6.917.4728.218.175.470.059</p><p>HA-Transformer[74]5.585.9639.523.275.350.067</p><p>STBD[75]7.438.0353.859.364.340.083</p><p>Proposed 6.186.6720.45.350.410.082</p><p>Proposed (With LM)5.946.38</p><p>表3-6 TED-LIUM2上各个模型的实验结果</p><p>Table3-6 Experimental results of each model on TED-LIUM2</p><p>模型</p><p>WER (%)↓ Parameters (M)</p><p>↓</p><p>MACs (B)</p><p>↓</p><p>GMUR (%)</p><p>↓</p><p>RTF</p><p>↓ Val Test</p><p>Speech-Transformer*[73]10.9511.4527.316.740.460.090</p><p>HA-Transformer*[74]10.8111.4139.421.542.390.091</p><p>Proposed 11.8011.8620.34.634.070.115</p><p>Proposed (With LM)10.7611.11</p><p>为了更加直观地看出所提方法的效果,论文将上述结果进行可视化处理。具体地,选择在测试集上的 CER/WER、参数量 Parameters、计算量 MACs 和 GPU内存占用率 GMUR 等指标,将 CER/WER 作为纵轴、其他三项分别作为横轴绘制</p><p>散点图,如图3-8所示。可以看出,改进模型的对应点在三幅图上都位于最左侧,除了在 AISHELL-1上的 CER 略高于 HA-Transformer,在参数量、计算量和 GPU</p><p>重庆邮电大学硕士学位论文</p><p>内存占用率等方面都达到了最优,进一步证明了所提方法的有效性,即在保证一定识别准确率的情况下,实现模型参数量和计算复杂度的降低。</p><p>图3-8各个模型实验结果的可视化处理。(a) CER/WER 与 Parameters 的关系;(b) CER/WER 与 MACs 的关系;(c) CER/WER 与 GMUR 的关系</p><p>Fig.3-8 Visualization of experimental results for each model.(a) Relation between CER/WER and parameters;(b) Relation between CER/WER and</p><p>MACs;(c) Relation between CER/WER and GMUR</p><p>3.5.5消融实验结果及分析</p><p>论文围绕解码器类型和注意力表示子空间个数设计消融实验,探究它们对于整个改进模型性能提升的贡献程度。具体地,<em class='similar'>在本章提出的改进模型的基础上,</em><em class='similar'>将轻量级解码器替换为普通解码器、</em><em class='similar'>将轻量级解码器的注意力表示子空间个数从</em></p><p>4降低为1,从而得到两个变种模型,分别用ND(Normal Decoder)和SH(Single</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 3 章 基于残差分组线性变换解码器的自动语音识别</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>Head)表示,观察它们在 AISHELL-1和 TED-LIUM2上的评价指标变化情况,具体信息见表3-7。表中,Base 指本章提出的改进模型,&quot;↓&quot;表示该项指标越低越好,括号内数字表示该项指标值与 Base 模型的差异,红色代表结果变好,绿色代表结果变差。除了变化的模块,所有变种模型的其他部分(包括网络结构、实验环境和训练配置等)均与 Base 模型一致,同时移除所有的外部语言模型。</p><p>表3-7消融实验结果</p><p>Table3-7 Ablation study results</p><p>数据集</p><p>模</p><p>型</p><p>CER/WER (%)↓ Parameters</p><p>(M)↓</p><p>MACs (B)</p><p>↓</p><p>GMUR (%)</p><p>↓</p><p>RTF</p><p>↓ Val Test</p><p>AISHELL-1</p><p>Base 6.186.6720.45.350.410.082</p><p>ND</p><p>6.00</p><p>(-0.18)</p><p>6.47</p><p>(-0.20)</p><p>25.5</p><p>(+5.1)</p><p>15.6</p><p>(+10.3)</p><p>63.24</p><p>(+12.83)</p><p>0.064</p><p>(-0.018)</p><p>SH</p><p>6.36</p><p>(+0.18)</p><p>6.79</p><p>(+0.12)</p><p>20.4</p><p>(0)</p><p>5.3</p><p>(0)</p><p>52.37</p><p>(+1.96)</p><p>0.073</p><p>(-0.009)</p><p>TED-LIUM2</p><p>Base 11.8011.8620.34.634.070.115</p><p>ND</p><p>11.05</p><p>(-0.75)</p><p>11.45</p><p>(-0.41)</p><p>24.9</p><p>(+4.6)</p><p>14.2</p><p>(+9.6)</p><p>42.73</p><p>(+8.66)</p><p>0.087</p><p>(-0.028)</p><p>SH</p><p>11.41</p><p>(-0.39)</p><p>12.07</p><p>(+0.21)</p><p>20.3</p><p>(0)</p><p>4.6</p><p>(0)</p><p>35.43</p><p>(+1.36)</p><p>0.109</p><p>(-0.006)</p><p>从上表中可以看出,所有变种模型的RTF值相较于Base模型都有略微降低,均在0.03以内,在实际应用中影响较小。在 AISHELL-1上,若使用普通解码器(ND),验证集和测试集上的 CER 分别下降了0.18%和0.20%,但参数量和计算量分别相对上升了25.00%和194.34%,同时 GPU 内存占用率也上升了12.83%;若使用含有单个注意力表示子空间的轻量级解码器(SH),验证集和测试集上的CER 绝对值分别上升了0.18%和0.12%,参数量和计算量没有变化,GPU 内存占用率上升了1.96%。在 TED-LIUM2上的结果与在 AISHELL-1上类似。</p><p>为了更加直观地看出各变种模型和 Base 模型的区别,论文将上述结果进行可视化处理。具体地,选择在测试集上的 CER/WER、参数量 Parameters、计算量MACs 和 GPU 内存占用率 GMUR 等指标,将 CER/WER 作为纵轴、<em class='similar'>其他三项分别作为横轴绘制散点图,</em><em class='similar'>如图3-9所示。</em>可以看出,Base 模型的对应点在三幅图上都位于最左侧,表明引入轻量级解码器,可以实现模型参数量、计算复杂度和GPU 内存占用率的显著降低,而只牺牲小部分的识别性能,达到识别性能和模型轻量化的平衡状态,<em class='similar'>进一步论证了所提方法的有效性。</em><em class='similar'>此外,</em><em class='similar'>轻量级解码器中注意力表示子空间的个数在很大程度上会影响到模型的识别准确率,</em><em class='similar'>采用单一表示子空间时模型的识别性能较差。</em></p><p>重庆邮电大学硕士学位论文</p><p>图3-9各个变种模型实验结果的可视化。(a) CER/WER 与 Parameters 的关系;(b) CER/WER 与 MACs 的关系;(c) CER/WER 与 GMUR 的关系</p><p>Fig.3-9 Visualization of experimental results for each variant model.(a) Relation between CER/WER and parameters;(b) Relation between CER/WER</p><p>and MACs;(c) Relation between CER/WER and GMUR</p><p>3.6本章小结</p><p>本章工作在原始 Transformer 模型的基础上,侧重于以识别准确率的小幅度降低为代价,实现模型参数量和计算复杂度的降低。首先提出基于残差分组线性变换的&quot;钻石&quot;型缩放单元,然后将缩放单元嵌入至原始 Transformer 模型的解码器中形成轻量级解码器,最终得到改进的、轻量级 Transformer 模型。在 AISHELL-</p><p>1和 TED-LIUM2数据集上的实验结果论证了本章所提方法的有效性。</p><p>Equation Chapter (Next) Section (Next)</p><p>第4章基于标签感知图交互的自然语言理解</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第4章 基于标签感知图交互的自然语言理解</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>4.1引言</p><p>基于显式联合建模的自然语言理解是近年来较为流行的一种建模方式,因其交互模块能充分共享捕获到的两个子任务间的共享特征信息,且能明确两个子任务间的交互过程以提高模型可解释性,在自然语言理解任务中展现了良好的性能,</p><p>受到了广泛的关注。然而,当前的研究方法需要进一步讨论以下问题:</p><p>(1)过于简化的标签特征。捕获特征只关注于原始话语和特征(One-hot)编码之间的相关性,而忽略了直观的原始标签特征。而标签语义可以通过评估话语中的字符和标签中的字符之间的语义相似性来提高自然语言理解模型的性能;</p><p>(2)维度较低的交互模块。交互模块只考虑了在字符级别(Token-level)上的意图—槽位交互,缺少了在语句级别(Sentence-level)上的全局优化,导致上下文特征泄露,从而影响模型性能。</p><p>为解决上述问题,本章工作在经典显式联合建模方法的基础上,侧重于对特征捕获和交互方式进行优化,实现模型交互能力和预测精度的提高。具体地,首先提出标签映射模块获取原始话语和标签语义之间的相关性以提供丰富的先验知识,然后提出全局图交互模块对语句级别的意图—槽位交互过程进行建模以提供全局优化,从而提升模型性能。</p><p>4.2基于最佳线性逼近的标签映射模块</p><p>受文献[78]的启发,论文利用最佳线性逼近(Best Linear Approximation)[79]</p><p>思想辅助检测原始话语的意图。具体地,基于最佳线性逼近构建标签映射模块,将意图标签特征自适应地融合到话语特征中,以增强模型的表征能力。</p><p>4.2.1最佳线性逼近</p><p>线性逼近是一种数学方法,它可以将一个函数在某一点处近似地表示为一个线性函数。具体来说,对于一个可导函数,可以在其某一点处,通过求取其在该点的导数来构造一个线性函数,使得在该点附近可以近似地代替。最佳线性逼近是在线性逼近的基础上,使得可以最优地代替,即在所有一次函数中,与的误差最小。</p><p>在多维空间上,最佳线性逼近指的是:通过一个线性函数来最好地拟合给定的数据点集合。具体来说,假设有一组数据点,它们位于多维空间中的某个子空</p><p>重庆邮电大学硕士学位论文</p><p>间上。需要想要找到一个线性函数,能够最好地拟合这些数据点。这个线性函数可以用一个向量表示,即最佳线性逼近向量。</p><p>上述过程用数学符号可描述为:假设表示 Hilbert 空间、<em class='similar'>表示的一个子空间,</em><em class='similar'>对于一个给定向量,</em><em class='similar'>需要找到一个离最近的向量,</em></p><p>的解是维空间的一组基向量的线性组合,其</p><p>中系数满足,为 Gram 矩阵,。</p><p>4.2.2标签映射模块</p><p>将上述思想迁移应用至自然语言理解领域,利用意图标签数据构建标签映射模块,其结构如图4-1所示。下面对模块的构建细节进行介绍。</p><p>图4-1标签映射模块结构</p><p>Fig.4-1 Structure of label injection module</p><p>1)标签编码器</p><p>构造一个以为基向量的标签空间,其中表示意图标签的个数。为了得到这一组基向量,设计标签编码器,通过堆叠的 Bi-LSTM 和自注意力机制对原始标签数据进行编码。具体地,首先使用 Bi-LSTM 双向读取输入序列</p><p>以生成隐藏状态,其中</p><p>;然后使用与式(3-2)相同的计算步骤得到自注意力的输出;接着将与连接形成编码向量;最后使用</p><p>Softmax 函数得到最终的输出,计算过程如下:</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 4 章 基于标签感知图交互的自然语言理解</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>(4-1)</p><p>(4-2)</p><p>式中,为可训练的权重矩阵;表示偏置向量。</p><p>2)标签映射</p><p>通过上述编码方式构造标签空间后,对于一个给定的原始话语向量,可</p><p>将其映射到上以获取最佳线性逼近向量,计算过程如下:</p><p>(4-3)</p><p>式中,系数,Gram 矩阵和定义如下:</p><p>(4-4)</p><p>需要注意的是,每个向量代表一个意图,且是线性独立的,因此 Gram 矩阵是正定的并且有逆。</p><p>4.3基于图注意力网络的全局图交互模块</p><p>为了高效融合所有的交互信息,深入挖掘两个子任务间的语义特征,论文提出全局图交互模块提供双向交互通道。具体地,基于图注意力网络搭建全局图交互模块,对语句级别的意图—槽位交互过程进行建模以提供全局优化,从而提升模型的交互能力。</p><p>4.3.1图注意力网络</p><p><em class='similar'>图注意力网络</em><em class='similar'>(Graph Attention Network,</em><em class='similar'>GAT)[80]</em><em class='similar'>是一种图神经网络的变体,</em></p><p>融合了图的结构信息和节点特征,其掩膜自注意力层让节点专注于邻域特征并学习不同的注意力权重。<em class='similar'>与传统的图卷积神经网络</em><em class='similar'>(Graph Convolutional Network,</em></p><p>GCN)不同,GAT 不仅考虑节点之间的邻接关系,而且还考虑节点之间的关联性,</p><p>并根据关联性对邻节点进行不同程度的加权。具体地,GAT 接收节点特征</p><p><em class='similar'>作为输入,</em><em class='similar'>其中,</em>表示<em class='similar'>节点个数</em>,<em class='similar'>表示每个节点的特征个数;</em><em class='similar'>输出一组新的节点特征,</em>其中<em class='similar'>(可能具有不同的基数)</em>。</p><p>重庆邮电大学硕士学位论文</p><p><em class='similar'>为了将输入特征转化为更高层次的特征,</em><em class='similar'>首先将由权重矩阵参数化的共享线性变换应用于每个节点;</em><em class='similar'>然后在节点上使用自注意力机制计算系数</em></p><p>,计算过程如下:</p><p>(4-5)</p><p>式中,表示节点的特征与节点的关联性;由权重矩阵得到,并使用 LeakyReLU 激活函数。</p><p><em class='similar'>接着使用 Softmax 函数对进行归一化处理得到,</em><em class='similar'>计算过程如下:</em></p><p>(4-6)</p><p>式中,表示节点的所有一阶邻节点(包括本身)的集合。</p><p>将上两式综合,自注意力机制计算出的系数可以表示为:</p><p>(4-7)</p><p>式中,表示转置操作。上述过程如图4-2所示。</p><p>图4-2系数的计算过程</p><p>Fig.4-2 Calculation process of coefficient</p><p>最后使用非线性函数作用于,并引入多头注意力机制得到最终的输出特</p><p>征,计算过程如下:</p><p>(4-8)</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 4 章 基于标签感知图交互的自然语言理解</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>式中,表示多头注意力机制中表示子空间的个数;表示由第个表示子空间计算得到的归一化系数;表示相对应的权重矩阵。上述过程如所示(以为例),其中不同颜色、形式的箭头表示各自独立的注意力计算方式。</p><p>图4-3多头注意力机制下的特征融合过程</p><p>Fig.4-3 Feature aggregation based on multi-head attention mechanism</p><p>4.3.2全局图交互模块</p><p>基于上述思想,为了实现语句级别的意图—槽位交互,搭建全局图交互模块,</p><p>其中所有的意图预测结果和槽位序列都相互连接,如图4-4所示,橙色方框中的</p><p>表示与槽位相关的隐藏状态序列,绿色圆框中的</p><p>表示意图预测结果。在数学意义上,图(Graph)可以表示为,其中顶点指的是意图和槽位,边指的是它们之间的相关性。下面将从这些方面对模块的构建细节进行介绍。</p><p>图4-4全局图交互模块结构</p><p>Fig.4-4 Structure of global graph interaction module</p><p>重庆邮电大学硕士学位论文</p><p>1)顶点</p><p>图中共有个顶点,其中表示原始话语的长度,表示的个数,即</p><p>预测出的意图个数。输入的槽位字符特征定义如下:</p><p>(4-9)</p><p>(4-10)</p><p>式中,表示层数;表示可训练的权重矩阵。初始状态。</p><p>输入的意图特征定义如下:</p><p>(4-11)</p><p>式中,表示用于映射的可训练的嵌入参数。</p><p>将上述两种特征(节点)组合,形成图的第1层状态向量。</p><p>2)边</p><p>图中有3种连接关系,分别为:槽位—槽位连接:构造槽位—槽位连接关系,</p><p>其中每个槽位节点通过滑动窗口(超参数)连接其他槽位节点,以进一步获取槽位之间的依赖性并合并上下文信息;意图—意图连接:考虑到所有意图节点都出自于同一个原始话语,构造意图—意图连接关系,获取所有意图节点之间的关联性;意图—槽位连接:由于意图和槽位之间的密切联系,构造意图—槽位连接关系,对两个子任务之间的全局交互过程进行建模。具体地,引入缩放点积注意力</p><p>机制计算意图和槽位之间的相关程度,计算过程如下:</p><p>(4-12)</p><p>式中,表示第个状态向量(槽位)和第个意图预测结果的相关程度;表示隐藏单元的维度。</p><p>若(超参数),表明该槽位节点和意图节点有足够的关联性。在这种情况下,将该槽位节点与预测意图节点进行连接。</p><p>3)特征融合</p><p>图中在第层的特征融合过程如下:</p><p>(4-13)</p><p>式中,<em class='similar'>和分别表示槽位节点和意图节点的集合;</em><em class='similar'>表示可训练的权重矩阵。</em></p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 4 章 基于标签感知图交互的自然语言理解</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>4.4基于标签感知的图交互模型</p><p>在经典显式联合建模方法(如图2-7所示)的基础上,采用上述模块对交互过程进行优化,得到如图4-5所示的基于标签感知的图交互模型。下面对该模型的一些重要组件进行简介。</p><p>图4-5基于标签感知的图交互模型</p><p>Fig.4-5 Structure of graph interaction model based on label-aware</p><p>4.4.1共享编码器</p><p>对于给定的长度为的输入序列,使用与标签编码器(节</p><p>4.2.2)相同的编码方式得到和,其中共享向量作为意图检测子网络和槽位填充子网络的输入。此外,在槽位填充子网络中使用 Bi-LSTM 对共享向量进一步编码,得到与槽位相关的隐藏状态序列,其中第个时间步的隐藏状态。</p><p>4.4.2意图解码器</p><p>受文献[81]的启发,论文引入阈值(超参数)辅助意图的解码预测,的值可以根据不同的数据集进行调整。具体地,在经过标签映射模块(节4.2.2)</p><p>重庆邮电大学硕士学位论文</p><p>得到最佳线性逼近向量之后,首先将其输入至意图解码器,获得关于所有意图</p><p>的概率分布序列,计算过程如下:</p><p>(4-14)</p><p><em class='similar'>式中,</em><em class='similar'>表示 Sigmoid 函数;</em><em class='similar'>和表示可训练的权重矩阵和偏置向量。</em></p><p>然后逐一比较和,若,则将其作为最终的意图之一。例如,当且时,。</p><p>4.4.3槽位解码器</p><p>在经过全局图交互模块(节4.3.2)中层的特征融合后,使用 argmax 和</p><p>Softmax 函数得到槽位预测结果,计算过程如下:</p><p>(4-15)</p><p>式中,表示第个字符的槽位;表示可训练的权重矩阵。</p><p>4.5实验结果及分析</p><p>本节首先介绍实验环境、使用的数据集和评价指标,然后给出对于标签文本的预处理流程及文本特征提取方法,接着描述网络模型的训练配置信息及过程,最后通过对比实验结果分析改进模型的性能,同时设计一系列消融实验论证所提方法的有效性。</p><p>4.5.1实验环境、数据集和评价指标</p><p>1)实验环境</p><p>实验环境与节3.5.1相同,具体信息可见表3-1。</p><p>2)数据集</p><p>实验使用在自然语言理解领域内应用较为广泛的开源多意图(Multiple Intent)</p><p>数据集:MixATIS[81]和 MixSnips[81],二者分别由单意图(Single Intent)数据集ATIS(Airline Travel Information Systems)[82]和 Snips[83]扩充而来,使得每句话语</p><p>包含1~3个意图。其中,MixATIS 包含14746句话语,涵盖关于航班的编号</p><p>(atis_flight_no)、时间(atis_flight_time)和餐食(atis_meal)等17个意图类别,</p><p>由录音和相应的人工记录组成,内容涉及用户在航空公司旅行查询系统上的航班询问信息;MixSnips 包含44173句话语,涵盖查询天气(GetWeather)、预定餐厅(BookRestaruant)和播放歌曲(PlayMusic)等7个意图类别。MixATIS 和MixSnips 的子集划分见表4-1,二者均已完成数据清洗,去除了重复的语句。</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 4 章 基于标签感知图交互的自然语言理解</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>表4-1 MixATIS 和 MixSnips 的划分信息 Table4-1 Split of MixATIS and MixSnips</p><p>数据集训练集(句)验证集(句)测试集(句)</p><p>MixATIS 13162756828</p><p>MixSnips 3977621982199</p><p>3)评价指标</p><p>在模型准确度方面,论文选用常用的意图准确率(Intent Accuracy,IAcc)、槽位 F1值和整体准确率(Overall Acc,OAcc)分别作为意图检测、槽位填充和</p><p>整体语句的评价指标[52]。其中,OAcc 表示意图和槽位均预测正确的句子所占的</p><p>比例,IAcc 和 F1值的定义如下:</p><p>(4-16)</p><p>(4-17)</p><p><em class='similar'>式中,</em><em class='similar'>TP(</em><em class='similar'>True Positive)</em><em class='similar'>表示预测正样本正确的个数;</em><em class='similar'>TN(</em><em class='similar'>True Negative)</em><em class='similar'>表示预测负样本正确的个数;</em><em class='similar'>FP(</em><em class='similar'>False Positive)</em><em class='similar'>表示预测正样本错误的个数;</em><em class='similar'>FN(</em><em class='similar'>False Negative)</em><em class='similar'>表示预测负样本错误的个数。</em></p><p>在模型实际应用(推理)方面,论文使用 GPU 内存占用率(GPU Memory-Usage Rate,GMUR)和推理时间(Latency)分别反映模型的资源占用和预测速度。对于前者,与节3.5.1中计算方式一致;对于后者,定义为模型推理一个批次数据的时间,可使用 Python 中自带的工具库 Time 计算得到,为了保证不受某些特殊因素的影响,对于每个模型的 Latency 值,论文共设计5轮测试并计算其均值和标准差作为最终的结果。</p><p>4.5.2数据预处理</p><p>1)标签文本处理</p><p>和节3.5.2类似,在自然语言理解任务中,同样需要建立词汇表(字典)。对于 MixATIS 和 MixSnips,论文分别使用不同的554/1435个字符来构成词汇表,其中551/1432个字符来自于本身的标注文本,另外包含3个用于辅助模型训练的特殊字符,即&lt;EOS/BOS&gt;、&lt;PAD&gt;和&lt;UNK&gt;。</p><p>2)文本特征提取</p><p>论文使用应用较为广泛的 Word2Vec 中的 Skip-gram 模型提取文本特征[84]。</p><p>Skip-gram 是一种用于获取文本特征的模型,它的输入是一个中心字符,而输出是</p><p>重庆邮电大学硕士学位论文</p><p>与该中心字符相关的字符。具体地,给定一个长度为的文本序列</p><p>,其中表示文本中的第个字符。对于任意一个中心字符,Skip-gram 模型的目标是预测它周围的上下文字符。具体地,Skip-gram 模型的损</p><p>失函数基于交叉熵损失,定义如下:</p><p>(4-18)</p><p>式中,表示上下文窗口的大小;表示在给定中心字符的条件下生成上下文字符的概率。</p><p>这个概率可以通过对它们之间的向量(One-hot编码)进行点积操作,然后将</p><p>结果送入 Softmax 函数得到,计算过程如下:</p><p>(4-19)</p><p>式中,表示上下文字符对应的向量;表示中心字符对应的向量;表示词汇</p><p>表的字符数;</p><p>在训练结束后,对于词汇表中的任一字符,可得到以该字符的中心字符向量和上下文字符向量,使用前者作为提取的文本特征。</p><p>4.5.3训练过程及配置信息</p><p>论文选择领域内近年来一些性能表现优秀的同类模型作为对比组,具体为:</p><p>Attention BiRNN[85],Slot-Gated(官方开源)[29]、Bi-Model[53]、SF-ID(官方开源)</p><p>[32]、Stack-Propagation(官方开源)[31]、Joint Multiple ID-SF[86]、AGIF(官方开源)</p><p>[81]和 SDJN[87]。由于提出这些方法的文献中未涉及评价指标中的 GMUR 和 RTF值,所以论文根据这些开源方法,逐一搭建了网络模型并在两个目标数据集上完成了训练。表4-2展示了本章所提模型主要超参数的配置信息。</p><p>对于本章所提模型,论文采用联合训练的模式来综合考虑意图检测和槽位填充两个子任务,联合训练的损失函数由它们各自的损失函数按一定的比例组合而</p><p>成,定义如下:</p><p>(4-20)</p><p>(4-21)</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 4 章 基于标签感知图交互的自然语言理解</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>(4-22)</p><p>式中,、和分别表示联合训练任务、意图检测任务和槽位填充任务的损失函数;和均为超参数;表示输入话语序列的长度;表示意图标签个数;表示槽位标签个数;和分别表示真实的和预测的意图结果。在本章所提模型的训练过程中,和分别设置为0.85和0.15。训练过程中各个模型在验证集上的损失(Loss)曲线如图4-6所示(从第10次迭代开始)。在所有对比组实验中,<em class='similar'>论文选择在验证集上表现最佳的模型作为训练好的模型,</em>然后在测试集上对其进行测试以进行公平比较。此外,在模型构建过程中没有使用任何例如以 C++语言进行编程的加速技巧。</p><p>表4-2本章所提模型主要超参数的配置信息</p><p>Table4-2 Configuration of the main hyper-parameters of the model proposed in this chapter</p><p>名称 MixATIS MixSnips 名称 MixATIS MixSnips 隐藏单元维度256256优化器 Adam Adam 字符向量维度12864学习率 Learning Rate 1e-31e-3</p><p>标签向量维度256128权重衰减1e-61e-6</p><p>表示子空间个数48 Dropout 概率0.40.4</p><p>图层数22滑动窗口大小21</p><p>批处理大小 Batch Size 64128阈值0.50.5</p><p>迭代次数 Epoch 200200阈值0.50.5</p><p>图4-6训练过程中 MixATIS(左侧)和 MixSnips(右侧)验证集上各个</p><p>模型的损失曲线</p><p>Fig.4-6 Loss curves for each model on the validation subset of MixATIS (left)</p><p>and MixSnips (right) during training</p><p>重庆邮电大学硕士学位论文</p><p>4.5.4对比实验结果及分析</p><p>将本章提出的模型和其他同类模型在 MixATIS 和 MixSnips 测试集上的实验</p><p>结果进行对比,具体信息分别见表4-3和表4-4。表中标&quot;*&quot;表示未开源的方法,</p><p>&quot;↑&quot;/&quot;↓&quot;分别表示该项指标越高/低越好,<em class='similar'>粗体数字表示最优结果,</em><em class='similar'>下划线数字表示次优结果。</em></p><p>表4-3 MixATIS 上各个模型的实验结果</p><p>Table4-3 Experimental results of each model on MixATIS</p><p>模型</p><p>OAcc (%)</p><p>↑</p><p>IAcc (%)</p><p>↑</p><p>F1(%)</p><p>↑</p><p>GMUR (%)</p><p>↓</p><p>Latency (ms)</p><p>↓</p><p>Attention BiRNN*[85]39.174.686.4</p><p>Slot-Gated[29]35.563.987.746.2411.7±1.1</p><p>Bi-Model*[53]34.470.383.9</p><p>SF-ID[32]34.966.287.448.3514.2±1.3</p><p>Stack-Propagation[31]39.676.286.555.7336.6±2.1</p><p>Joint Multiple ID-SF*[86]36.173.484.6</p><p>AGIF[81]40.874.486.753.787.3±0.5</p><p>SDJN*[87]44.677.188.2</p><p>Proposed 49.977.888.344.629.5±0.3</p><p>表4-4 MixSnips 上各个模型的实验结果</p><p>Table4-4 Experimental results of each model on MixSnips</p><p>模型</p><p>OAcc (%)</p><p>↑</p><p>IAcc (%)</p><p>↑</p><p>F1(%)</p><p>↑</p><p>GMUR (%)</p><p>↓</p><p>Latency (ms)</p><p>↓</p><p>Attention BiRNN*[85]59.595.489.4</p><p>Slot-Gated[29]55.494.687.948.2512.3±1.7</p><p>Bi-Model*[53]63.495.690.7</p><p>SF-ID[32]59.995.090.645.7714.6±1.2</p><p>Stack-Propagation[31]72.496.293.744.5437.7±2.3</p><p>Joint Multiple ID-SF*[86]62.995.190.6</p><p>AGIF[81]74.295.194.243.147.2±0.4</p><p>SDJN*[87]75.796.594.4</p><p>Proposed 77.397.194.840.6610.2±0.3</p><p>从表4-3中可以看出,在 MixATIS 上,与对比组中预测效果最好的 SDJN 相比,本章所提模型的整体准确率 OAcc、意图准确率 IAcc 和槽位 F1值分别上升了</p><p>5.3%、0.7%和0.1%;与对比组中 GMUR 最小的 Slot-Gated 相比,GMUR 下降了</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 4 章 基于标签感知图交互的自然语言理解</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>1.62%;与对比组中 Latency 值最小的 AGIF 相比,Latency 的均值上升了3ms(在实际应用中影响较小)。本章所提模型在 MixSnips 上的结果与在 MixATIS 上类似。</p><p>为了更加直观地看出所提方法的效果,论文将上述结果进行可视化处理。具体地,选择整体准确率 OAcc、GPU 内存占用率和推理时间 Latency 均值等指标,将 OAcc 作为纵轴、其他两项分别作为横轴绘制散点图,如图4-7所示。可以看出,本章所提模型的对应点在两幅图上都位于最高点,除了在 Latency 值方面略高于 AGIF,在整体准确率和 GPU 内存占用率两方面都达到了最优,进一步证明了所提方法的有效性,即优化了特征捕获和交互方式,实现模型交互能力和预测精度的提高。</p><p>图4-7各个模型实验结果的可视化处理。(a) OAcc与GMUR的关系;(b)</p><p>OAcc 与 Latency 的关系</p><p>Fig.4-7 Visualization of experimental results for each model.(a) Relation</p><p>between OAcc and GMUR;(b) Relation between OAcc and Latency</p><p>另外,为了更好地理解本章所提模型在双向交互过程中捕获的语义特征,分别使用来自 MixATIS 和 MixSnips 数据集的话语输入至模型并将其注意力权重可视化,纵轴为预测意图,横轴为输入话语,标*表示含有槽位的字符,如图4-8所</p><p>示。对于每个权重块,颜色越深,表示意图和槽位的关联性越强。在 MixATIS 上,</p><p>例句具有两个意图 atis_flight 和 atis_meal,可以看到注意力权重成功地集中在正确的位置上,具体地,在槽位 delta 处正确聚合了 atis_flight 意图信息,在槽位meal 处正确聚合了 atis_meal 意图信息,表明模型能很好地关注每个槽位并合并相关意图信息。在 MixSnips 上的结果与在 MixATIS 上类似,在槽位 song 和 siesta</p><p>重庆邮电大学硕士学位论文</p><p>处聚合了 AddToPlaylist 意图,在槽位 shadow、of、suribachi、five 和 starts 处聚合了 RateBook 意图。</p><p>图4-8输入话语在本章所提模型上的注意力权重可视化。(a)来自</p><p>MixATIS 的话语;(b)来自 MixSnips 的话语</p><p>Fig.4-8 Visualization of the attention weights for input utterances on the models proposed in this chapter.(a) Utterance from MixATIS;(b) Utterance</p><p>from MixSnips</p><p>4.5.5消融实验结果及分析</p><p>论文围绕标签映射模块和全局图交互模块设计消融实验,探究它们对于整个模型性能提升的贡献程度。具体地,在本章所提模型的基础上,移除标签映射模块(Label Injection Module,LIM)、移除全局图交互模块(Global Graph Interaction Module,GGIM),从而得到两个变种模型,分别用 w/o LIM 和 w/o GGIM 表示,观察它们在 MixATIS 和 MixSnips 上的评价指标变化情况,具体信息</p><p>见表4-5。表中,Base 指本章所提模型,&quot;↑&quot;/&quot;下&quot;表示该项指标越高/低越好。</p><p>除了变化的模块,所有变种模型的其他部分(包括网络结构、实验环境和训练配置等)均与 Base 模型一致。</p><p>从表中可以看出,所有变种模型的预测精度相较于 Base 模型都有明显降低。具体地,在 MixATIS 上,若移除标签映射模块(w/o LIM),和 Base 模型相比,GPU 内存占用率 GMUR 下降了3.80%,Latency 值下降了1.2ms,二者在实际应用中均影响较小,但整体准确率 OAcc、意图准确率 IAcc 和槽位 F1值分别下降了</p><p>7.6%、1.4%和1.2%;若移除全局图交互模块(w/o GGIM),和 Base 模型相比</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 4 章 基于标签感知图交互的自然语言理解</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>GPU 内存占用率 GMUR 下降了2.96%,Latency 值下降了0.8ms,但整体准确率OAcc、意图准确率 IAcc和槽位F1值分别下降了6.8%、0.6%和2.6%。在MixSnips上的结果与在 MixATIS 上类似。</p><p>表4-5消融实验结果</p><p>Table4-5 Ablation study results</p><p>数据集模型</p><p>OAcc (%)</p><p>↑</p><p>IAcc (%)</p><p>↑</p><p>F1(%)</p><p>↑</p><p>GMUR (%)</p><p>↓</p><p>Latency (ms)</p><p>↓</p><p>MixATIS</p><p>Base 49.977.888.344.629.5±0.3</p><p>w/o LIM</p><p>42.3</p><p>(-7.6)</p><p>76.4</p><p>(-1.4)</p><p>87.1</p><p>(-1.2)</p><p>40.82</p><p>(-3.80)</p><p>8.3±0.7</p><p>(-1.2)</p><p>w/o GGIM</p><p>43.1</p><p>(-6.8)</p><p>77.2</p><p>(-0.6)</p><p>85.7</p><p>(-2.6)</p><p>41.66</p><p>(-2.96)</p><p>8.7±0.8</p><p>(-0.8)</p><p>MixSnips</p><p>Base 77.397.194.840.6610.2±0.3</p><p>w/o LIM</p><p>72.2</p><p>(-5.1)</p><p>95.5</p><p>(-1.6)</p><p>93.8</p><p>(-1.0)</p><p>37.35</p><p>(-3.31)</p><p>9.4±0.4</p><p>(-0.8)</p><p>w/o GGIM</p><p>74.0</p><p>(-3.3)</p><p>96.4</p><p>(-0.7)</p><p>92.5</p><p>(-2.3)</p><p>36.92</p><p>(-3.74)</p><p>8.1±1.2</p><p>(-2.1)</p><p>注:括号内数字表示该模型的该项指标值与 Base 模型的差异。红色表示结果变好,绿色表示结果变差。</p><p>为了更加直观地看出各变种模型和 Base 模型的区别,论文将上述结果进行可视化处理。具体地,将意图准确率 IAcc 作为纵轴、槽位 F1值作为横轴绘制散点图,如图4-9所示。</p><p>图4-9各个变种模型意图准确率 IAcc 和槽位 F1值关系的可视化</p><p>Fig.4-9 Visualization of the relation between intent accuracy and slot F1 for each variant model</p><p>从上图中可以看出,在意图准确率 IAcc 方面,w/o LIM 相较于 Base 模型的下降程度大于 w/o GGIM,说明标签映射模块可以获取原始话语和标签语义之间的</p><p>重庆邮电大学硕士学位论文</p><p>相关性,以辅助意图检测子任务;在槽位 F1值方面,w/o GGIM 相较于 Base 模型的下降程度大于 w/o GGIM,说明全局图交互模块可以对语句级别的意图—槽位交互过程进行建模,并通过全局优化辅助槽位填充子任务,从而显著提高模型性能,进一步论证了所提方法的有效性。</p><p>4.6本章小结</p><p>本章工作在经典显式联合建模方法的基础上,侧重于对特征捕获和交互方式进行优化,实现模型交互能力和预测精度的提高。首先提出标签映射模块获取原始话语和标签语义之间的相关性以提供丰富的先验知识,然后提出全局图交互模块对语句级别的意图—槽位交互过程进行建模以提供全局优化,<em class='similar'>从而提升模型性能。</em><em class='similar'>在MixATIS和MixSnips数据集上的实验结果论证了本章所提方法的有效性。</em></p><p>Equation Chapter (Next) Section (Next)</p><p>第5章面向车载嵌入式设备的本地智能语音对话系统</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第5章 面向车载嵌入式设备的本地智能语音对话系统</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>5.1引言</p><p>当前主流的车载语音对话系统均采用&quot;云—端&quot;方式运行,存在着一定的数据安全隐患。为解决这类问题,面向车载嵌入式设备,研发离线条件下的本地智能语音对话是有效的技术途径。具体地,首先选取 Nvidia Jetson TX2作为车载嵌入式设备并进行刷机、配置环境等操作,然后根据实际应用场景收集、创建驾驶数据集,接着将论文第3章、<em class='similar'>第4章提出的模型分别在驾驶数据集上进行训练,</em>最后集成、移植网络模型至 TX2并围绕搭建全套硬件平台,实现数据安全、自然实时的离线智能语音对话。本章会对系统搭建的细节进行详细介绍并完成相应的系统测试。</p><p>5.2嵌入式设备运行环境搭建</p><p>嵌入式设备是指将计算能力和控制功能嵌入到设备中的计算机系统,通常具有小巧、低功耗、高效率和低成本等特点,广泛应用于工业自动化、医疗设备和安防监控等领域。英伟达(Nvidia)公司推出的 Jetson TX2是一款高性能、低功耗的嵌入式计算平台,其外观如图5-1(a)所示。它的基于 ARM 架构的 CPU 和Nvidia Pascal 架构的 GPU,能够提供强大的计算和图形处理能力,非常适合于嵌入式设备应用[88]。相较于上一代的 TX1,TX2内存和 eMMC(Embedded Multi</p><p>Media Card)提高一倍,CUDA(Compute Unified Device Architecture)<em class='similar'>架构升级为 Pascal,</em><em class='similar'>每瓦性能提高一倍。</em>同时,由于支持 PyTorch、TensorFlow 和 Caffe 等深度学习框架,TX2也是一款非常适合深度学习应用的嵌入式设备,能够处理复杂的神经网络模型和算法,其主要硬件配置信息见表5-1。</p><p>表5-1 TX2主要硬件配置信息</p><p>Table5-1 Main hardware configuration of TX2</p><p>类型规格类型规格</p><p>算力1.33TFLOPS 内存</p><p>8GB 128-bit</p><p>LPDDR459.7GB/s</p><p>GPU 256-core NVIDIA PascalTM GPU 存储32GB eMMC 5.1</p><p>CPU</p><p>Dual-Core NVIDIA Denver 1.564-Bit CPU 和 Quad-</p><p>Core ARM®Cortex®-A57 MPCore processor</p><p>功率7.5W/15W</p><p>USB USB 3.0和 USB 2.0尺寸87mm 50mm</p><p>重庆邮电大学硕士学位论文</p><p>综上所述,Nvidia Jetson TX2是嵌入式 AI 应用的理想选择,所以论文选择该器件作为搭建本地智能语音对话系统的核心嵌入式设备。下面对 TX2的运行环境搭建步骤进行详细介绍。</p><p>首先按照官方教程,将 TX2与 PC 虚拟机(Ubuntu18.04系统)连接,在 PC虚拟机上安装刷机软件SDK Manager并下载一体化软件包 JetPack(版本为4.5.1),</p><p>即可开始刷机操作,中间操作界面和完成界面如图5-1(b)和(c)所示;然后需要安装神经网络模型训练和推理所需的深度学习框架和各种第三方库。具体地,打开TX2终端窗口执行教程上的相关安装命令,如图5-1(d)所示。值得注意的是,由于在 TX2上没有 NVSMI 工具,无法使用 nvidia-smi 命令查看 GPU 运行状态,所</p><p>以需要额外安装 Jtop 工具作为替代,其安装命令为 sudo -H pip3 install -U jetson-</p><p>stats,安装完成后,可直接使用 Jtop 命令查看 CPU 运行状态、GPU 运行状态、内存和软、硬件信息(如图5-1(e)所示),以及使用 pip3 list 命令查看所有已安装的和运行环境有关的软件包,如图5-1(f)所示。至此,TX2的运行环境搭建完成。</p><p>图5-1 TX2外观及运行环境搭建。(a) TX2外观;(b)刷机中间操作界面;(c)刷机完成界面;(d)深度学习框架和第三方库的安装命令;(e) Jtop</p><p>命令查看软、硬件信息;(f) pip3 list 命令查看所有软件包</p><p>Fig.5-1 TX2 appearance and operating environment construction.(a) Appearance of TX2;(b) Intermediate operation interface of flashing OS;(c) Complete interface of flashing OS;(d) Installation command of deep learning framework and third-party library;(e) Jtop command to view software and</p><p>hardware information;(f) pip3 list command to view all packages</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 5 章 面向车载嵌入式设备的本地智能语音对话系统</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>5.3驾驶数据集收集</p><p>由于所搭建的本地智能语音对话系统专用于驾驶员和车载增强现实平视显示器(Augmented Reality Head-Up Display,ARHUD)系统的语音交互,所以需要从使用 ARHUD 的真实场景中收集音频数据。此外,为了增加数据量且降低收集成本,设计以微信小程序为载体的调查问卷,可从移动端(手机端)收集更多数据。下面对数据收集平台和收集过程中的细节进行详细介绍。</p><p>5.3.1数据收集平台</p><p>数据收集平台包括与 ARHUD 相连的车载收集平台和手机云收集平台,如图</p><p>5-2所示。</p><p>图5-2数据收集平台。(a)车载收集平台(驾驶室);(b)车载收集平台</p><p>(后备箱);(c)手机云收集平台</p><p>Fig.5-2 Data collection platform.(a) In-vehicle collection platform (cab);(b) In-vehicle collection platform (trunk);(c) mobile phone cloud collection</p><p>platform</p><p>重庆邮电大学硕士学位论文</p><p>在车载收集平台上,主要硬件单元包括:麦克风阵列、触发器(按钮)和TX2。麦克风和触发器放置于驾驶室,TX2放置于后备箱,它们通过通用串行总线(Universal Serial Bus)进行连接。麦克风阵列包含4个 MEMS(Micro Electro Mechanical System)麦克风单元的,采样率为16bit/44.1Khz。每个麦克风单元都配备了数字信号处理器(Digital Signal Processor,DSP)用于降低车辆噪音,<em class='similar'>如发动机噪音、</em>传输噪音、<em class='similar'>轮胎噪音和空气动力噪音等。</em>为了避免在收集过程中与志愿者交流造成的语音干扰,设计了一个触发器来手动控制麦克风阵列接收的语音输入。TX2用于存储收集的音频数据,为了扩展存储空间,TX2上额外外接了一张8G 的 SD 卡(Secure Digital Memory Card)。在手机云收集平台上,所有收集的音频数据存储于后台服务器。</p><p>5.3.2数据收集过程</p><p>根据 ARHUD 的功能和使用场景,论文按6个功能类别(包含25个意图)收集音频数据,分别是:关机、切换主题、调整高度和亮度、控制驾驶信息、控制导航和控制其他功能。在整个收集过程中,共有570名志愿者参与,年龄范围在</p><p>18~45岁,男女比例约为1.54:1,具体信息见表5-2。所有的音频数据都是在志愿者和工作人员共同驾车的过程中收集的。实验地位于重庆市渝北区鱼嘴工业园区的非公共道路,主要包括一些位于不同工业园区区域的专用测试路线,全长约</p><p>7公里。汽车的行驶速度取决于驾驶员的驾驶水平,但不允许超过道路限速。</p><p>表5-2收集的数据集的详细信息</p><p>Table5-2 Details of the collected dataset</p><p>数据集信息功能类别音频数</p><p>来自车载收集</p><p>平台的数量</p><p>来自手机云收集</p><p>平台的数量</p><p>男性人数346关机24742187287</p><p>女性人数224切换主题24992017482</p><p>时长(小时)52调整高度和亮度14198131451053</p><p>采样率(Hz)16000控制行车信息70426348694</p><p>通道数1控制导航46604092568</p><p>比特率(bps)96000控制其他功能27065240533012</p><p>帧大小(KB)50合计57983518426096</p><p>为保证数据的真实性,需要志愿者根据自己的驾驶习惯完成实验。在每次驾</p><p>车中,志愿者在工作人员(副驾驶位)的引导下行驶约15分钟(路程约3公里),</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 5 章 面向车载嵌入式设备的本地智能语音对话系统</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>根据所需的 ARHUD 功能说出相应的命令词,负责数据采集其他工作(监控设备运行状态是否正常)的工作人员坐在副驾驶后面。</p><p>在 TX2和后台服务器上的初始语音数据集包含近70000个音频文件,总时长约为75小时。首先对其进行数据清理操作,去除噪音过大、音量过小和格式损坏的音频文件,然后聘请专业的数据处理团队对数据集进行文本标记。经过以上数据处理工作,最终得到了完整的驾驶数据集,将其命名为 CQUPT-DS,包含近</p><p>58000个音频文件,总时长约51小时,子集划分见表5-3。</p><p>表5-3 CQUPT-DS 的划分信息</p><p>Table5-3 Split of CQUPT-DS</p><p>语音段数/语句数时长(小时)</p><p>训练集(Train)5194045.7</p><p>验证集(Val)30002.7</p><p>测试集(Test)29982.6</p><p>5.4模型训练与移植</p><p>在完成驾驶数据集的收集工作之后,将第3章和第4章提出的模型分别在CQUPT-DS 上进行训练,并将训练好的模型搭配其他对话系统中的模块进行集成与移植。</p><p>5.4.1自动语音识别模型训练</p><p>将第3章提出的模型在 CQUPT-DS 上进行训练,同时为了验证所提模型在CQUPT-DS 上的适应性,根据节3.5设置对比实验进行比较,表5-4展示了各个模型主要超参数的配置信息,包含编码器激活函数(E-AF)、解码器激活函数(D-AF)、优化器(OP)、热身步数(WU)、学习率(LR)、Dropout 概率(DR)、批处理大小(BS)和迭代次数(EP)。</p><p>表5-4各个模型在 CQUPT-DS 上主要超参数的配置信息</p><p>Table5-4 Configuration of the main hyper-parameters of each model on CQUPT-DS 模型 E-AF D-AF OP WU LR DR BS EP Speech-Transformer[73] ReLU ReLU Adam 2.3e41.20.10160100</p><p>HA-Transformer[74] GLU GLU Adam 2.3e41.20.10128100</p><p>STBD[75] ReLU ReLU Adam 1.4e41.20.1096100</p><p>Proposed Swish ReLU Adam 2.3e41.20.15128100</p><p>重庆邮电大学硕士学位论文</p><p>训练过程中各个模型在验证集上的损失(Loss)曲线如图5-3所示(从第10次迭代开始)。</p><p>图5-3训练过程中 CQUPT-DS 验证集上各个 ASR 模型的损失曲线</p><p>Fig.5-3 Loss curves for each ASR model on the validation subset of CQUPT-DS during training</p><p>对比实验结果见表5-5,表中 MACs 值是在编码序列长度为500、解码序列长度为30的条件下计算得出,RTF 值是在 TX2上计算得出,&quot;↓&quot;表示该项指标越低越好,<em class='similar'>粗体数字表示最优结果,</em><em class='similar'>下划线数字表示次优结果。</em></p><p>表5-5 CQUPT-DS 上各个模型的实验结果</p><p>Table5-5 Experimental results of each model on CQUPT-DS</p><p>模型</p><p>CER (%)↓ Parameters (M)</p><p>↓</p><p>MACs (B)</p><p>↓</p><p>GMUR (%)</p><p>↓</p><p>RTF</p><p>↓ Val Test</p><p>Speech-Transformer[73]8.578.0628.218.173.560.117</p><p>HA-Transformer[74]8.267.6939.523.271.490.121</p><p>STBD[75]7.457.3253.859.363.680.182</p><p>Proposed 7.267.2620.45.354.070.198</p><p>从表中可以看出,第3章所提模型在 CQUPT-DS 上除了 RTF 值外,在测试集上的 CER、参数量、计算量和 GMUR 都达到了最优。具体地,在 CER 方面,相较于 HA-Transformer 下降了0.43%;在参数量和计算量方面,相较于 Speech-</p><p>Transformer 分别相对下降了27.66%和70.72%;在 GMUR 方面,相较于 STBD 下降了9.61%。表明所提模型在 CQUPT-DS 上也有较强的可适应性。</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 5 章 面向车载嵌入式设备的本地智能语音对话系统</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>5.4.2自然语言理解模型训练</p><p>将第4章提出的模型在 CQUPT-DS 上进行训练,同时为了验证所提模型在CQUPT-DS 上的适应性,根据节4.5设置对比实验进行比较,表5-6展示了所提模型主要超参数的配置信息。</p><p>表5-6所提模型在 CQUPT-DS 上主要超参数的配置信息</p><p>Table5-6 Configuration of the main hyper-parameters of the proposed model</p><p>名称值名称值</p><p>隐藏单元维度256优化器 Adam 字符向量维度128学习率 Leraning Rate 1e-3</p><p>标签向量维度256权重衰减1e-6</p><p>表示子空间个数4 Dropout 概率0.4</p><p>图层数2滑动窗口大小1</p><p>批处理大小 Batch Size 256阈值0.5</p><p>迭代次数 Epoch 200阈值0.5</p><p>在第4章所提模型的训练过程中,<em class='similar'>和分别设置为0.9和0.1。</em><em class='similar'>训练过程中各个模型在验证集上的损失</em><em class='similar'>(Loss)</em><em class='similar'>曲线如图5-4所示</em>(从第10次迭代开始)。</p><p>图5-4训练过程中 CQUPT-DS 验证集上各个 NLU 模型的损失曲线</p><p>Fig.5-4 Loss curves for each NLU model on the validation subset of CQUPT-DS during training</p><p>对比实验结果见表5-7,<em class='similar'>表中&quot;↑&quot;/&quot;↓&quot;分别表示该项指标越高/低越好,</em><em class='similar'>粗体数字表示最优结果,</em><em class='similar'>下划线数字表示次优结果。</em>从表中可以看出,第4章所提模型在 CQUPT-DS 上除了 Latency 值外,整体准确率、意图准确率、槽位 F1值和GMUR都达到了最优。其中,在整体准确率OAcc、意图准确率 IAcc和槽位 F1</p><p>重庆邮电大学硕士学位论文</p><p>值方面,相较于 AGIF 分别上升了3.3%、2.1%和0.3%;在 GMUR 方面,相较于SF-ID 下降了1.06%。表明所提模型在 CQUPT-DS 上也有较强的可适应性。</p><p>表5-7 CQUPT-DS 上各个模型的实验结果</p><p>Table5-7 Experimental results of each model on CQUPT-DS</p><p>模型</p><p>OAcc (%)</p><p>↑</p><p>IAcc (%)</p><p>↑</p><p>F1(%)</p><p>↑</p><p>GMUR (%)</p><p>↓</p><p>Latency (ms)</p><p>↓</p><p>Slot-Gated[29]58.793.989.349.2314.4±1.3</p><p>SF-ID[32]61.294.491.642.8313.1±0.4</p><p>Stack-Propagation[31]75.496.093.958.8632.6±3.5</p><p>AGIF[81]78.294.794.851.579.5±0.4</p><p>Proposed 81.596.895.141.7712.8±1.6</p><p>5.4.3模型集成与移植</p><p>模型训练完成后,根据节2.1并参考其他模块的开源方法,完成对话系统的集成,如图5-5所示。这些开源模块方法包括:DST-as-Prompting[89]、Deep Dyna-</p><p>Q[90]、KENLG-Reading[91]和 FastPitch[92]等。</p><p>集成完成后,将系统模型移植至已经搭建了运行环境的 TX2上。为了观察TX2运行对话系统时的性能表现,使用 top 和 Jtop 命令查看 TX2的 CPU 占用、GPU 占用和内存占用情况,如图5-6所示。</p><p>具体地,系统启动后,经过约10s 所有网络模型均加载完成,系统处于等待音频输入的状态(如图5-6(a)所示);在模型加载和模型推理时,CPU 的最大占用率为74.6%(如图5-6(b)所示);GPU 的最大占用率接近85%,在等待音频输入时为55%左右(如图5-6(c)所示);内存占用为60%(如图5-6(d)所示)。表明对话系统能在嵌入式设备 TX2上较好地运行。</p><p>图5-5集成的对话系统</p><p>Fig.5-5 Integrated dialog system</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第 5 章 面向车载嵌入式设备的本地智能语音对话系统</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>图5-6 TX2运行对话系统时的性能表现。(a)运行界面;(b) CPU 占用;</p><p>(c) GPU 占用;(d)内存占用</p><p>Fig.5-6 Performance of TX2 running the dialogue system.(a) Running</p><p>interface;(b) CPU usage;(c) GPU usage;(d) Memory usage</p><p>5.5系统硬件平台搭建与测试</p><p>5.5.1平台搭建</p><p>在完成模型的移植工作之后,围绕 TX2搭建全套硬件平台,如图5-7所示。系统运行的基本流程为:驾驶员在驾驶位上,按住触发器(录音开始)说出命令词,结束后松开触发器(录音结束),麦克风阵列将音频传输至 TX2进行处理,处理结果以 Socket 通信方式(车载内网)传输给车载 ARHUD,并在车前窗上的特定区域显示,同时播放语音给予驾驶员反馈。例如,驾驶员命令词为&quot;我想看看别的行车信息&quot;,ARHUD 会在特定区域内切换下一个行车信息,同时播放的语音内容为&quot;好的,已为您切换行车信息,当前信息:XXX&quot;。</p><p>值得说明的是,由于触发器的存在,可能会出现驾驶员误触的情况,这时麦克风阵列录制的音频为空白或者音量很小,会影响到后续模块的有效运行。所以在 TX2中(ASR 模块前)加入逻辑控制,当检测到空白和音量很小()</p><p>重庆邮电大学硕士学位论文</p><p>的音频时进行特殊处理,即直接跳过后续模块并播放语音&quot;说话音量较小,请适当提高音量&quot;以提醒驾驶员。</p><p>图5-7系统运行平台。(a)驾驶室;(b)后备箱</p><p>Fig.5-7 System operating platform.(a) Cab;(b) Trunk.</p><p>5.5.2系统测试</p><p>整套系统已在实验用车上完成搭建,支持 ARHUD 包含的25种意图/功能,最大对话轮次达到5轮。整套系统已通过由重庆利龙智能汽车研究院主导的内部项目验收测试,对话通过率为97%,平均响应时间为0.87s,具体结果见表5-8。</p><p>表5-8项目验收结果</p><p>Table5-8 Project acceptance results</p><p>测试项目测试次数测试通过次数通过率判定结果平均响应时间不符合项概述</p><p>对话系统626097% OK 0.87s 无</p><p>5.6本章小结</p><p>本章工作为面向车载嵌入式设备研发离线条件下的本地智能语音对话系统。首先选取 Nvidia Jetson TX2作为车载嵌入式设备并进行刷机、配置环境等操作,然后根据实际应用场景收集、创建驾驶数据集,接着将论文第3章、<em class='similar'>第4章提出的模型分别在驾驶数据集上进行训练,</em>最后集成、移植网络模型至 TX2并围绕搭建全套硬件平台。由重庆利龙智能汽车研究院主导的内部项目验收测试结果论证了本章工作的有效性。</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">第6章 总结与展望</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>6.1总结</p><p>对话系统是智能语音交互的一种具体技术形式,<em class='similar'>在车载终端控制、</em><em class='similar'>智能客服和智能家居等领域有着广泛的应用。</em>作为对话系统的重要技术,自动语音识别和自然语言理解是近年来备受关注的研究热点。如何解决对话系统由于当前&quot;云—端&quot;运行方式带来的数据安全隐患,实现自动语音识别和自然语言理解技术在车载平台上的高可靠性、强实时性应用,研发离线条件下的智能语音对话也是当前的研究难点。论文首先阐述了自动语音识别和自然语言理解技术的研究现状,<em class='similar'>然后分析现有方法存在的问题,</em><em class='similar'>提出了相应的改进方案,</em>最后基于改进的网络模型搭建了车载对话系统,实现了数据安全、自然实时的离线智能语音交互。论文的</p><p>主要研究成果如下:</p><p>(1)在对现有自动语音识别技术的改进方面,<em class='similar'>针对基于深度编—解码器的模型参数量庞大的问题,</em><em class='similar'>提出了一种基于残差分组线性变换的解码器结构。</em><em class='similar'>该结构关键模块为&quot;钻石&quot;型缩放单元,</em>其内部采用稀疏连接,同一组神经元共享相同的权重矩阵。在 AISHELL-1和 TED-LIUM2数据集上的实验结果表明,<em class='similar'>所提模型能以识别准确率的小幅度降低为代价,</em><em class='similar'>实现参数量和计算复杂度的降低。</em></p><p>(2)在对现有自然语言理解技术的改进方面,针对基于显式联合建模的模型交互能力不足的问题,提出了一种基于标签感知的图交互模型。其中标签映射模块可以获取原始话语与标签语义之间的相关性以提供丰富的先验知识,全局图交互模块可以对语句级别的意图—槽位交互过程进行建模以提供全局优化。在MixATIS 和 MixSnips 数据集上的实验结果表明,所提模型能对特征捕获和交互方式进行优化,实现交互能力和预测精度的提高。</p><p>(3)在对现有车载语音对话系统的改进方面,针对&quot;云—端&quot;方式存在数据安全隐患的问题,搭建了面向车载嵌入式设备的本地智能语音对话系统。具体地,首先选取 Nvidia Jetson TX2作为车载嵌入式设备并进行刷机、配置环境等操作,然后根据实际应用场景收集、创建驾驶数据集 CQUPT-DS,接着将研究成果</p><p>(1)和(2)中的模型在 CQUPT-DS 上进行训练,最后集成、移植网络模型至TX2并围绕搭建全套硬件平台。由重庆利龙智能汽车研究院主导的内部项目验收测试结果表明,对话系统能实现数据安全、自然实时的离线运行。</p><p>重庆邮电大学硕士学位论文</p><p>6.2展望</p><p>论文对自动语音识别和自然语言理解模型进行了改进,在降低参数量和计算复杂度的同时提高了识别性能,同时搭建了车载智能语音对话系统,实现了数据安全、自然实时的离线智能语音交互。然而论文中仍有一些不足之处,后续研究</p><p>工作可以从以下两个方面展开:</p><p>(1)流式自动语音识别。论文提出的自动语音识别模型不支持流式识别,<em class='similar'>即无法在输入语音信号的同时进行识别,</em><em class='similar'>这会在一些实时性要求较高的场景中导致语音识别的体验降低。</em><em class='similar'>下一步可以考虑使用分块技术对编码器网络中的注意力机制进行优化,</em><em class='similar'>以建立在当前输入时刻语音帧和历史信息之间的联系,</em><em class='similar'>从而支持流式自动语音识别。</em></p><p>(2)非自回归式解码。论文提出的自动语音识别和自然语言理解模型均采用自回归式解码,即解码器需要逐个输出预测的字符。当序列长度增加,这种方式的计算复杂程度会大大提升,可能会影响模型推理效率。<em class='similar'>下一步可以考虑采用非自回归的训练方式对模型进行训练,</em><em class='similar'>以进一步提高模型的预测速度。</em></p><p class='uncheck'>参考文献 </p><p class='uncheck'>[1] SEABORN K, MIYAKE N P, PENNEFATHER P, et al. Voice in human-agent interaction: a survey[J]. ACM Computing Surveys, 2021, 54(4): 1-43. </p><p class='uncheck'>[2] 中泰证券研究所. Cerence:人工智能助力未来出行—车智能化海外公司系列报告[EB/O</p><p class='uncheck'>L]. (2021-04-25) [2022-12-25]. https: pdf.dfcfw.com/pdf/H3_AP202104261487921362_1.pdf?1619458440000.pdf. </p><p class='uncheck'>[3] 陈艳华. 基于智能交互的车载语音系统的设计与实现[D]. 北京: 北京交通大学, 2020: 1-2. </p><p class='uncheck'>[4] DERIU J, RODRIGO A, OTEGI A, et al. Survey on evaluation methods for dialogue systems[J]. Artificial Intelligence Review, 2021, 54(1): 755-810. </p><p class='uncheck'>[5] RIGOLL G, NEUKIRCHEN C. A new approach to hybrid HMM/ANN speech recognition </p><p class='uncheck'>using mutual information neural networks[C]. 10th Conference on Neural Information Processing Systems, Denver, USA, 1996: 772-778. </p><p class='uncheck'>[6] MOHAMED A, DAHL G, HINTON G. Deep belief networks for phone recognition[C]. 23rd </p><p class='uncheck'>Conference Neural Information Processing Systems Workshop on Deep Learning for Speech Recognition and Related Applications, Vancouver, Canada, 2009: 39-39. </p><p class='uncheck'>[7] ABDEL-HAMID O, MOHAMED A, JIANG H, et al. Convolutional neural networks for speech </p><p class='uncheck'>recognition[J]. IEEE-ACM Transactions on Audio, Speech, And Language Processing, 2014, 22(10): 1533-1545. </p><p class='uncheck'>[8] GRAVES A. Connectionist temporal classification[M]. Springer, Berlin, Heidelberg: Supervised Sequence Labelling with Recurrent Neural Networks, 2012: 61-93. </p><p class='uncheck'>[9] SUTSKEVER I, VINYALS O, LE Q V. Sequence to sequence learning with neural networks[C]. 28th Conference Neural Information Processing Systems, Montreal, Canada, 2014: 3104-3112. [10] GRAVES A, FERNÁNDEZ S, GOMEZ F, et al. Connectionist temporal classification: </p><p class='uncheck'>labelling unsegmented sequence data with recurrent neural networks[C]. Proceedings of the 23rd International Conference on Machine Learning, Pittsburgh, USA, 2006: 369-376. </p><p class='uncheck'>[11] GRAVES A. Sequence transduction with recurrent neural networks[J]. arXiv preprint arXiv:1211.3711, 2012. </p><p class='uncheck'>[12] GRAVES A, MOHAMED A, HINTON G. Speech recognition with deep recurrent neural </p><p class='uncheck'>networks[C]. 38th IEEE International Conference on Acoustics, Speech and Signal Processing, Vancouver, Canada, 2013: 6645-6649. </p><p class='uncheck'>重庆邮电大学硕士学位论文 </p><p class='uncheck'>[13] GRAVES A. Generating sequences with recurrent neural networks[J]. arXiv preprint arXiv:1308.0850, 2013. </p><p class='uncheck'>[14] BAHDANAU D, CHO K, BENGIO Y. Neural machine translation by jointly learning to align and translate[J]. arXiv preprint arXiv:1409.0473, 2014. </p><p class='uncheck'>[15] CHOROWSKI J, BAHDANAU D, CHO K, et al. End-to-end continuous speech recognition using attention-based recurrent NN: first results[J]. arXiv preprint arXiv:1412.1602, 2014. </p><p class='uncheck'>[16] BAHAR P, ZEYER A, SCHLÜTER R, et al. On using 2d sequence-to-sequence models for </p><p class='uncheck'>speech recognition[C]. 44th IEEE International Conference on Acoustics, Speech and Signal Processing, Brighton, UK, 2019: 5671-5675. </p><p class='uncheck'>[17] VASWANI A, SHAZEER N, PARMAR N, et al. Attention is all you need[C]. 31st Conference Neural Information Processing Systems, Long Beach, USA, 2017: 5998-6008. </p><p class='uncheck'>[18] ZHOU S, DONG L, XU S, et al. A comparison of modeling units in sequence-to-sequence </p><p class='uncheck'>speech recognition with the transformer on mandarin Chinese[C]. 25th International Conference on Neural Information Processing, Siem Reap, Cambodia, 2018: 210-220. </p><p class='uncheck'>[19] MERBOLDT A, ZEYER A, SCHLÜTER R, et al. An analysis of local monotonic attention </p><p class='uncheck'>variants[C]. 20th Annual Conference of the International Speech Communication Association, Graz, Austria, 2019: 1398-1402. </p><p class='uncheck'>[20] ZEYER A, IRIE K, SCHLÜTER R, et al. Improved training of end-to-end attention models for speech recognition[J]. arXiv preprint arXiv:1805.03294, 2018. </p><p class='uncheck'>[21] GULATI A, QIN J, CHIU C C, et al. Conformer: convolution-augmented transformer for speech recognition[J]. arXiv preprint arXiv:2005.08100, 2020. </p><p class='uncheck'>[22] QIN L, XIE T, CHE W, et al. A survey on spoken language understanding: recent advances and new frontiers[J]. arXiv preprint arXiv:2103.03095, 2021. </p><p class='uncheck'>[23] RAVURI S, STOLCKE A. Recurrent neural network and LSTM models for lexical utterance </p><p class='uncheck'>classification[C]. 16th Annual Conference of the International Speech Communication </p><p class='uncheck'>Association, Dresden, Germany, 2015: 35-40 </p><p class='uncheck'>[24] YAO K, ZWEIG G, HWANG M Y, et al. Recurrent neural networks for language </p><p class='uncheck'>understanding[C]. 14th Annual Conference of the International Speech Communication Association, Lyon, France, 2013: 2524-2528. </p><p class='uncheck'>[25] MESNIL G, HE X, DENG L, et al. Investigation of recurrent-neural-network architectures and </p><p class='uncheck'>learning methods for spoken language understanding[C]. 14th Annual Conference of the International Speech Communication Association, Lyon, France, 2013: 3771-3775. </p><p class='uncheck'>[26] MESNIL G, DAUPHIN Y, YAO K, et al. Using recurrent neural networks for slot filling in </p><p class='uncheck'>spoken language understanding[J]. IEEE-ACM Transactions on Audio, Speech, and Language Processing, 2014, 23(3): 530-539. </p><p class='uncheck'>[27] ZHANG X, WANG H. A joint model of intent determination and slot filling for spoken </p><p class='uncheck'>language understanding[C]. 25th International Joint Conference on Artificial Intelligence, New York, USA, 2016: 2993-2999. </p><p class='uncheck'>[28] HAKKANI-TÜR D, TÜR G, CELIKYILMAZ A, et al. Multi-domain joint semantic frame </p><p class='uncheck'>parsing using bi-directional RNN-LSTM[C]. 17th Annual Conference of the International Speech Communication Association, San Francisco, USA, 2016: 715-719. </p><p class='uncheck'>[29] GOO C W, GAO G, HSU Y K, et al. Slot-gated modeling for joint slot filling and intent </p><p class='uncheck'>prediction[C]. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), New Orleans, USA, 2018: 753-757. </p><p class='uncheck'>[30] LI C, LI L, QI J. A self-attentive model with gate mechanism for spoken language </p><p class='uncheck'>understanding[C]. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, 2018: 3824-3833. </p><p class='uncheck'>[31] Qin L, Che W, Li Y, et al. A stack-propagation framework with token-level intent detection for </p><p class='uncheck'>spoken language understanding[C]. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, Hong Kong, China, 2019: 2078-2087. </p><p class='uncheck'>[32] NIU P, CHEN Z, SONG M. A novel bi-directional interrelated model for joint intent detection and slot filling[J]. arXiv preprint arXiv:1907.00390, 2019. </p><p class='uncheck'>[33] ZHANG C, LI Y, DU N, et al. Joint slot filling and intent detection via capsule neural </p><p class='uncheck'>networks[C]. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Florence, Italy, 2019: 5259-5267. </p><p class='uncheck'>[34] LIU Y, MENG F, ZHANG J, et al. CM-Net: a novel collaborative memory network for spoken </p><p class='uncheck'>language understanding[C]. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, Hong Kong, China, 2019: 1051-1060. </p><p class='uncheck'>[35] ZHANG L, MA D, ZHANG X, et al. Graph LSTM with context-gated mechanism for spoken </p><p class='uncheck'>language understanding[C]. Proceedings of the 34th AAAI Conference on Artificial Intelligence, New York, USA, 2020: 9539-9546. </p><p class='uncheck'>重庆邮电大学硕士学位论文 </p><p class='uncheck'>[36] Qin L, Liu T, Che W, et al. A co-interactive transformer for joint slot filling and intent </p><p class='uncheck'>detection[C]. 46th IEEE International Conference on Acoustics, Speech and Signal Processing, Toronto, Canada, 2021: 8193-8197. </p><p class='uncheck'>[37] DEVLIN J, CHANG M W, LEE K, et al. BERT: pre-training of deep bidirectional transformers </p><p class='uncheck'>for language understanding[C]. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Minneapolis, USA, 2019: 4171-4186. </p><p class='uncheck'>[38] CHEN Q, ZHUO Z, WANG W. Bert for joint intent classification and slot filling[J]. arXiv preprint arXiv:1902.10909, 2019. </p><p class='uncheck'>[39] CHEN H, LIU X, YIN D, et al. A survey on dialogue systems: recent advances and new frontiers[J]. ACM SIGKDD Explorations Newsletter, 2017, 19(2): 25-35. </p><p class='uncheck'>[40] MCTEAR M F. Spoken dialogue technology: enabling the conversational user interface[J]. ACM Computing Surveys, 2002, 34(1): 90-169. </p><p class='uncheck'>[41] 赵新颜. 基于深度学习的对话系统研究与应用[D]. 合肥: 中国科学技术大学, 2022: 13-14. [42] NI J, YOUNG T, PANDELEA V, et al. Recent advances in deep learning based dialogue systems: a systematic survey[J]. Artificial Intelligence Review, 2022: 1-101. </p><p class='uncheck'>[43] JORGENSEN P E T, SONG M S, TIAN J. Operator theory, kernels, and feedforward neural networks[J]. arXiv preprint arXiv:2301.01327, 2023. </p><p class='uncheck'>[44] RUMELHART D E, HINTON G E, WILLIAMS R J. Learning representations by back-propagating errors[J]. Nature, 1986, 323(6088): 533-536. </p><p class='uncheck'>[45] LI Z, LIU F, YANG W, et al. A survey of convolutional neural networks: analysis, applications, </p><p class='uncheck'>and prospects[J]. IEEE Transactions on Neural Networks and Learning Systems, 2021, 33(2): 6999-7019. </p><p class='uncheck'>[46] HEWAMALAGE H, BERGMEIR C, BANDARA K. Recurrent neural networks for time series </p><p class='uncheck'>forecasting: current status and future directions[J]. International Journal of Forecasting, 2021, 37(1): 388-427. </p><p class='uncheck'>[47] HOCHREITER S, SCHMIDHUBER J. Long short-term memory[J]. Neural Computation, 1997, 9(8): 1735-1780. </p><p class='uncheck'>[48] YAO S, WAN X. Multimodal transformer for multimodal machine translation[C]. Proceedings </p><p class='uncheck'>of the 58th Annual Meeting of the Association for Computational Linguistics, Online, 2020: 4346-4350. </p><p class='uncheck'>[49] DU Y, PEI B, ZHAO X, et al. Deep scaled dot-product attention based domain adaptation model for biomedical question answering[J]. Methods, 2020, 173: 69-74. </p><p class='uncheck'>[50] MIRSAMADI S, BARSOUM E, ZHANG C. Automatic speech emotion recognition using </p><p class='uncheck'>recurrent neural networks with local attention[C]. 42nd IEEE International Conference on Acoustics, Speech and Signal Processing, New Orleans, USA, 2017: 2227-2231. </p><p class='uncheck'>[51] CHAN W, JAITLY N, LE Q, et al. Listen, attend and spell: a neural network for large </p><p class='uncheck'>vocabulary conversational speech recognition[C]. 41st IEEE International Conference on Acoustics, Speech and Signal Processing, Shanghai, China, 2016: 4960-4964. </p><p class='uncheck'>[52] KARMAKAR P, TENG S W, LU G. Thank you for attention: a survey on attention-based </p><p class='uncheck'>artificial neural networks for automatic speech recognition[J]. arXiv preprint arXiv:2102.07259, 2021. </p><p class='uncheck'>[53] WANG Y, SHEN Y, JIN H. A bi-model based RNN semantic frame parsing model for intent detection and slot filling[J]. arXiv preprint arXiv:1812.10235, 2018. </p><p class='uncheck'>[54] PHAM N Q, NGUYEN T S, NIEHUES J, et al. Very deep self-attention networks for end-to-end speech recognition[J]. arXiv preprint arXiv:1904.13377, 2019. </p><p class='uncheck'>[55] SYNNAEVE G, XU Q, KAHN J, et al. End-to-end ASR: from supervised to semi-supervised learning with modern architectures[J]. arXiv preprint arXiv:1911.08460, 2019. </p><p class='uncheck'>[56] LUO H, ZHANG S, LEI M, et al. Simplified self-attention for transformer-based end-to-end </p><p class='uncheck'>speech recognition[C]. 8th IEEE Spoken Language Technology Workshop, Shenzhen, China, 2021: 75-81. </p><p class='uncheck'>[57] ZHANG S, LIU C, JIANG H, et al. Feedforward sequential memory networks: a new structure to learn long-term dependency[J]. arXiv preprint arXiv:1512.08301, 2015. </p><p class='uncheck'>[58] MEHTA S, GHAZVININEJAD M, IYER S, et al. Delight: deep and light-weight transformer[J]. arXiv preprint arXiv:2008.00623, 2020. </p><p class='uncheck'>[59] WEN Q, ZHOU T, ZHANG C, et al. Transformers in time series: a survey[J]. arXiv preprint arXiv:2202.07125, 2022. </p><p class='uncheck'>[60] MISRA D. Mish: a self regularized non-monotonic activation function[J]. arXiv preprint arXiv:1908.08681, 2019. </p><p class='uncheck'>[61] LIN T, WANG Y, LIU X, et al. A survey of transformers[J]. AI Open, 2022, 1(1): 1-16. </p><p class='uncheck'>[62] WANG D, ZHANG X. Thchs-30: A free Chinese speech corpus[J]. arXiv preprint arXiv:1512.01882, 2015. </p><p class='uncheck'>[63] BU H, DU J, NA X, et al. Aishell-1: An open-source mandarin speech corpus and a speech </p><p class='uncheck'>recognition baseline[C]. 20th Conference of the Oriental Chapter of the International Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment, Seoul, South Korea, 2017: 1-5. </p><p class='uncheck'>重庆邮电大学硕士学位论文 </p><p class='uncheck'>[64] ROUSSEAU A, DELÉGLISE P, ESTEVE Y. Enhancing the TED-LIUM corpus with selected </p><p class='uncheck'>data for language modeling and more TED talks[C]. Proceedings of the 9th International Conference on Language Resources and Evaluation, Reykjavik, Iceland, 2014: 3935-3939. </p><p class='uncheck'>[65] WATANABE S, HORI T, KARITA S, et al. ESPNET: end-to-end speech processing toolkit[J]. arXiv preprint arXiv:1804.00015, 2018. </p><p class='uncheck'>[66] MALIK M, MALIK M K, MEHMOOD K, et al. Automatic speech recognition: a survey[J]. Multimedia Tools and Applications, 2021, 80: 9411-9457. </p><p class='uncheck'>[67] WHITEHEAD N, FIT-FLOREA A. Precision & performance: floating point and IEEE 754 compliance for NVIDIA GPUs[J]. RN (A+ B), 2011, 21(1): 18749-19424. </p><p class='uncheck'>[68] HIGUCHI Y, INAGUMA H, WATANABE S, et al. Improved mask-CTC for non-</p><p class='uncheck'>autoregressive end-to-end ASR[C]. 46th IEEE International Conference on Acoustics, Speech and Signal Processing, Toronto, Canada, 2021: 8363-8367. </p><p class='uncheck'>[69] GHAHREMANI P, BABAALI B, POVEY D, et al. A pitch extraction algorithm tuned for </p><p class='uncheck'>automatic speech recognition[C]. 39th IEEE International Conference on Acoustics, Speech and Signal Processing, Florence, Italy, 2014: 2494-2498. </p><p class='uncheck'>[70] KUDO T, RICHARDSON J. SentencePiece: a simple and language independent subword </p><p class='uncheck'>tokenizer and detokenizer for neural text processing[C]. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Brussels, Belgium, 2018: 66-71. </p><p class='uncheck'>[71] POVEY D, PEDDINTI V, GALVEZ D, et al. Purely sequence-trained neural networks for ASR </p><p class='uncheck'>based on lattice-free MMI[C]. 17th Annual Conference of the International Speech Communication Association, San Francisco, USA, 2016: 2751-2755. </p><p class='uncheck'>[72] SHAN C, WENG C, WANG G, et al. Component fusion: learning replaceable language model </p><p class='uncheck'>component for end-to-end speech recognition system[C]. 44th IEEE International Conference on Acoustics, Speech and Signal Processing, Brighton, UK, 2019: 5361-5635. </p><p class='uncheck'>[73] DONG L, XU S, XU B. Speech-transformer: a no-recurrence sequence-to-sequence model for </p><p class='uncheck'>speech recognition[C]. 43rd IEEE International Conference on Acoustics, Speech and Signal Processing, Calgary, Canada, 2018: 5884-5888. </p><p class='uncheck'>[74] XU M, LI S, ZHANG X L. Transformer-based end-to-end speech recognition with local dense </p><p class='uncheck'>synthesizer attention[C]. 46th IEEE International Conference on Acoustics, Speech and Signal Processing, Toronto, Canada, 2021: 5899-5903. </p><p class='uncheck'>[75] CHEN X, ZHANG S, SONG D, et al. Transformer with bidirectional decoder for speech recognition[J]. arXiv preprint arXiv:2008.04481, 2020. </p><p class='uncheck'>[76] PARK D S, CHAN W, ZHANG Y, et al. Specaugment: a simple data augmentation method for automatic speech recognition[J]. arXiv preprint arXiv:1904.08779, 2019. </p><p class='uncheck'>[77] KANNAN A, WU Y, NGUYEN P, et al. An analysis of incorporating an external language </p><p class='uncheck'>model into a sequence-to-sequence model[C]. 43rd IEEE International Conference on Acoustics, Speech and Signal Processing, Calgary, Canada, 2018: 1-5828. </p><p class='uncheck'>[78] WU T W, SU R, JUANG B. A label-aware BERT attention network for zero-shot multi-intent </p><p class='uncheck'>detection in spoken language understanding[C]. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Online and Punta Cana, Dominican Republic, 2021: 4884-4896. </p><p class='uncheck'>[79] DEL PINO G E, GALAZ H. Statistical applications of the inverse gram matrix: a revisitation[J]. Brazilian Journal of Probability and Statistics, 1995: 177-196. </p><p class='uncheck'>[80] VELIČKOVIĆ P, CUCURULL G, CASANOVA A, et al. Graph attention networks[J]. arXiv preprint arXiv:1710.10903, 2017. </p><p class='uncheck'>[81] QIN L, XU X, CHE W, ET AL. AGIF: An adaptive graph-interactive framework for joint multiple intent detection and slot filling[J]. arXiv preprint arXiv:2004.10087, 2020. </p><p class='uncheck'>[82] HEMPHILL C T, GODFREY J J, DODDINGTON G R. The ATIS spoken language systems </p><p class='uncheck'>pilot corpus[C]. Speech and Natural Language: Proceedings of a Workshop, Hidden Valley, USA, 1990. </p><p class='uncheck'>[83] COUCKE A, SAADE A, BALL A, et al. Snips voice platform: an embedded spoken language </p><p class='uncheck'>understanding system for private-by-design voice interfaces[J]. arXiv preprint arXiv:1805.10190, 2018. </p><p class='uncheck'>[84] MIKOLOV T, CHEN K, CORRADO G, et al. Efficient estimation of word representations in vector space[J]. arXiv preprint arXiv:1301.3781, 2013. </p><p class='uncheck'>[85] LIU B, LANE I. Attention-based recurrent neural network models for joint intent detection and slot filling[J]. arXiv preprint arXiv:1609.01454, 2016. </p><p class='uncheck'>[86] GANGADHARAIAH R, NARAYANASWAMY B. Joint multiple intent detection and slot </p><p class='uncheck'>labeling for goal-oriented dialog[C]. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Minneapolis, USA, 2019: 564-569. </p><p class='uncheck'>[87] CHEN L, ZHOU P, ZOU Y. Joint multiple intent detection and slot filling via self-distillation[C]. </p><p class='uncheck'>47th IEEE International Conference on Acoustics, Speech and Signal Processing, Singapore, 2022: 7612-7616. </p><p class='uncheck'>重庆邮电大学硕士学位论文 </p><p class='uncheck'>[88] SÜZEN A A, DUMAN B, ŞEN B. Benchmark analysis of jetson tx2, jetson nano and raspberry </p><p class='uncheck'>pi using deep-CNN[C]. 2nd International Congress on Human-Computer Interaction, Optimization and Robotic Applications, Online, 2020: 1-5. </p><p class='uncheck'>[89] LEE C H, CHENG H, OSTENDORF M. Dialogue state tracking with a language model using </p><p class='uncheck'>schema-driven prompting[C]. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Online and Punta Cana, Dominican Republic, 2021: 4937-4949. [90] PENG B, LI X, GAO J, et al. Deep Dyna-Q: integrating planning for task-completion dialogue </p><p class='uncheck'>policy learning[C]. Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Melbourne, Australia, 2018: 2182-2192. [91] YU W, JIANG M, HU Z, et al. Knowledge-enriched natural language generation[C]. </p><p class='uncheck'>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts, Online and Punta Cana, Dominican Republic, 2021: 11-16. </p><p class='uncheck'>[92] ŁAŃCUCKI A. Fastpitch: parallel text-to-speech with pitch prediction[C]. 46th IEEE </p><p class='uncheck'>International Conference on Acoustics, Speech and Signal Processing, Toronto, Canada, 2021: 6588-6592. </p><p>致谢</p><p><em class='similar'>在即将完成学位论文之际,</em><em class='similar'>我想要借此机会表达我最深切的感激之情。</em></p><p>首先,我要感谢我的指导老师李鹏华教授,您一直是我在学术道路上最坚实的后盾。您给予我耐心指导,帮助我澄清思路,解决问题,并在研究中不断给予鼓励和支持。没有您的帮助和支持,我不可能完成这篇论文。在您的指导下,我不仅获得了专业知识和研究技能,还形成了自己的学术思想和方法论。<em class='similar'>您的教诲将对我今后的学术生涯产生深远的影响,</em><em class='similar'>我将终生铭记。</em></p><p>我也要感谢我的家人和朋友,在我整个学术过程中一直给予我鼓励和支持。没有你们的支持和理解,我不可能取得今天的成果。在我遇到困难和挫折时,你们的支持和鼓励让我能够坚持下来。</p><p>我还要感谢我的各位同门,我们一起探讨学术问题,互相帮助,共同进步。<em class='similar'>你们的意见和建议帮助我不断完善研究内容和论文质量。</em></p><p>最后,我要感谢我的母校和国家,给予我接受高等教育的机会和平台。在我接受教育的过程中,我获得了知识和技能,更加明确了自己的人生方向和责任使命。</p><p>在此,我再次向所有支持和帮助过我的人们表示由衷的感谢和敬意。</p></p>
                </div>
              </div>
             <div class="report_explain2">
              <div class="repExp_left">说明：</div>
              <div class="repExp_rig">
                <p>1.指标是由系统根据《学术论文不端行为的界定标准》自动生成的</p>
                <p>2.本报告单仅对您所选择比对资源范围内检测结果负责</p>
              </div>
            </div>
            <div class="clear"></div>
            <div class="report_footer">
                <div class="assist_tool">
                  <h2>写作辅助工具</h2>
                  <ul>
                    <li>
                      <div class="asst icons asst1"></div>
                      <div class="asstRig"> <a target="_blank" href="https://www.bigan.net/recommendtitle/">选题分析</a>
                        <p>帮您选择合适的论文题目</p>
                      </div>
                    </li>
                    <li>
                      <div class="asst icons asst2"></div>
                      <div class="asstRig"> <a target="_blank" href="https://www.bigan.net/recommenddata/">资料搜集</a>
                        <p>提供最全最好的参考文章</p>
                      </div>
                    </li>
                    <li>
                      <div class="asst icons asst3"></div>
                      <div class="asstRig"> <a target="_blank" href="https://www.bigan.net/recommendoutline/">提纲推荐</a>
                        <p>辅助生成文章大纲</p>
                      </div>
                    </li>
                    <li>
                      <div class="asst icons asst4"></div>
                      <div class="asstRig"> <a target="_blank" href="https://www.bigan.net/writing/">在线写作</a>
                        <p>规范写作，提供灵感</p>
                      </div>
                    </li>
                    <li class="bgNo">
                      <div class="asst icons asst5"></div>
                      <div class="asstRig"> <a target="_blank" href="https://www.bigan.net/reference/">参考文献</a>
                        <p>规范参考文献，查漏补缺</p>
                      </div>
                    </li>
                  </ul>
                </div>
                <div class="repFot_bot">
                  <div class="reportCopy inlineBlock">版权所有：笔杆 www.bigan.net</div>
                  <div class="shareTo inlineBlock"><span>分享到：</span> <a href="http://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?url=https%3A%2F%2Fwww.bigan.net&title=%E5%92%8C%E6%88%91%E4%B8%80%E8%B5%B7%EF%BC%8C%E6%8B%BF%E8%B5%B7%E7%AC%94%E6%9D%86%EF%BC%8C%E5%81%9A%E4%B8%80%E4%B8%AA%E5%BF%AB%E4%B9%90%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%B8%9D%E3%80%82&summary=%E7%AC%94%E6%9D%86%EF%BC%8C%E4%B8%80%E4%B8%AA%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9C%A8%E7%BA%BF%E5%86%99%E4%BD%9C%E5%B9%B3%E5%8F%B0%E3%80%82%E7%AC%94%E6%9D%86%E4%B8%BA%E4%BD%A0%E6%89%AB%E6%B8%85%E5%86%99%E4%BD%9C%E6%80%9D%E8%B7%AF%E7%9A%84%E9%98%B4%E9%9C%BE%EF%BC%8C%E6%89%BE%E5%88%B0%E6%89%8D%E6%80%9D%E6%B3%89%E6%B6%8C%EF%BC%8C%E7%81%B5%E6%84%9F%E8%BF%B8%E5%8F%91%E7%9A%84%E5%BF%AB%E6%84%9F%EF%BC%8C%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C%EF%BC%8C%E6%88%91%E4%BB%AC%E4%BF%A1%E6%89%8B%E6%8B%88%E6%9D%A5%E3%80%82www.bigan.net&pics=https%3A%2F%2Fwww.bigan.net%2Flogo_80_80.png" target="_blank" title="QQ空间" class="inlineBlock sitem1 icons pngfix"></a> <a href="#" title="微信" class="inlineBlock sitem2 icons pngfix"></a> <a href="http://service.weibo.com/share/share.php?url=https%3A%2F%2Fwww.bigan.net&title=%E5%92%8C%E6%88%91%E4%B8%80%E8%B5%B7%EF%BC%8C%E6%8B%BF%E8%B5%B7%E7%AC%94%E6%9D%86%EF%BC%8C%E5%81%9A%E4%B8%80%E4%B8%AA%E5%BF%AB%E4%B9%90%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%B8%9D%E3%80%82%EF%BC%88%E7%AC%94%E6%9D%86%EF%BC%8C%E4%B8%80%E4%B8%AA%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9C%A8%E7%BA%BF%E5%86%99%E4%BD%9C%E5%B9%B3%E5%8F%B0%E3%80%82%E7%AC%94%E6%9D%86%E4%B8%BA%E4%BD%A0%E6%89%AB%E6%B8%85%E5%86%99%E4%BD%9C%E6%80%9D%E8%B7%AF%E7%9A%84%E9%98%B4%E9%9C%BE%EF%BC%8C%E6%89%BE%E5%88%B0%E6%89%8D%E6%80%9D%E6%B3%89%E6%B6%8C%EF%BC%8C%E7%81%B5%E6%84%9F%E8%BF%B8%E5%8F%91%E7%9A%84%E5%BF%AB%E6%84%9F%EF%BC%8C%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C%EF%BC%8C%E6%88%91%E4%BB%AC%E4%BF%A1%E6%89%8B%E6%8B%88%E6%9D%A5%E3%80%82%40%E7%AC%94%E6%9D%86%E7%BD%91%EF%BC%89" target="_blank" title="新浪微博" class="inlineBlock sitem3 icons pngfix"></a> </div>
                </div>
              </div>
           </div>
       </div>
  </div>
</div>
</div>
</body>
</html>