# 面试

## PyTorch相关

### 并行训练

常用的API有：`torch.nn.DataParallel`(DP)、`torch.nn.DistributedDataParallel`(DDP)

DP的计算过程：

1. 将inputs从主GPU分发到所有GPU上；
2. 将model从主GPU分发到所有GPU上；
3. 每个GPU分别独立进行前向传播，得到outputs；
4. 将每个GPU的outputs发回主GPU；
5. 在主GPU上，通过loss function计算出loss，对loss function求导，求出损失梯度；
6. 计算得到的梯度分发到所有GPU上；
7. 反向传播计算参数梯度；
8. 将所有梯度回传到主GPU，通过梯度更新模型权重；
9. 不断重复上面的过程。

应用：

```python
torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)

net = torch.nn.DataParallel(model, device_ids=[0, 1, 2])
```

DDP的过程：
大体上的过程和DP类似，与DP的单进程控制多GPU不同，在distributed的帮助下只需要编写一份代码，torch就会自动将其分配给n个进程，分别在n个GPU上运行。

不再有主GPU，每个GPU执行相同的任务。对每个GPU的训练都是在自己的过程中进行的。每个进程都从磁盘加载其自己的数据。**分布式数据采样器可确保加载的数据在各个进程之间不重叠。损失函数的前向传播和计算在每个GPU上独立执行**。因此，不需要收集网络输出。在反向传播期间，梯度下降在所有GPU上均被执行，从而确保每个GPU在反向传播结束时最终得到平均梯度的相同副本。

DP优势：简单，一行代码。

DDP的优势：
1. 每个进程对应一个独立的训练过程，且只对梯度等少量数据进行信息交换。DDP在每次迭代中，每个进程具有自己的optimizer ，并独立完成所有的优化步骤，进程内与一般的训练无异。DDP中由于各进程中的模型，初始参数一致（初始时刻进行一次broadcast），而每次用于更新参数的梯度也一致，因此各进程的模型参数始终保持一致。相较于DP，DDP传输的数据量更少，因此速度更快，效率更高。

2. 每个进程包含独立的解释器和GIL。全局解释器锁（Global Interpreter Lock）是Python用于同步线程的工具，使得任何时刻仅有一个线程在执行。由于每个进程拥有独立的解释器和GIL，消除额外解释器开销和GIL-thrashing，因此可以减少解释器和GIL使用冲突。

### `model.train()`和`model.eval()`

前提：模型中有BN和Dropout。

`model.train()`：在训练时使用，表示开启训练模式，启用BN和Dropout。对于BN，保证BN能够用到每一批数据的均值和方差；对于Dropout，随机取一部分网络连接来训练更新参数。

`model.eval()`：在测试时使用，表示开启测试模式，不启用BN和Dropout。对于BN，保证BN能够用全部训练数据的均值和方差，即测试过程中BN的均值和方差不变；对于Dropout，利用到了所有网络连接，即不进行随机舍弃神经元。

**因此，在使用PyTorch进行训练和测试时一定注意要把实例化的model指定train/eval**。

### `DataLoader()`

```python
DataLoader(dataset, batch_size, shuffle=False, sampler=None, 
          batch_sample=None, num_workers=0, collate_fn=None, 
          pin_memory=False, drop_last=False, timeout=0,
          worker_init_fn=None)
```

- batch_size表示输入batch的大小；
- shuffle表示是否打乱输入；
- sampler是样本抽样；
- num_workers指的是开多少个worker来读数据，一个worker读一个batch的数据。

## ML、DL相关

### 归一化算法

Batch Normalization (BN)、Layer Normalization (LN)（包括Instance Normalization (IN)、Group Normalization (GN)）都属于归一化算法，它们的主要区别在于**操作的feature map维度不同**。

LN是“横”着来的，对一个样本、不同的神经元间做归一化；BN是“竖”着来的，各个维度做归一化，所以与batch size有关系。**二者提出的目的都是为了加快模型收敛，减少训练时间**。

CV和NLP任务的区别在于：图像数据是自然界客观存在的，像素的组织形式已经包含了“信息”；而自然语言相关的数据不一样，网络对其学习的真正开端是从embedding开始的，而这个embedding并不是客观存在，也是通过网络学习出来的。

LN是针对每一个样本，做特征的缩放。如果把BN应用于NLP任务，那就表示**默认了在同一个位置的单词对应的是同一种特征**，这样是违背直觉的。

### L1、L2正则化的区别

正则化的本质是在损失函数上添加正则化项，起到降低模型过拟合程度的作用。

L1正则化：

![L1](面试.assets/2018121020441991.png)

L2正则化：

![L2](面试.assets/20181210204427458.png)

L1正则化对所有参数的惩罚力度都一样，可以让一部分权重变为零，因此产生稀疏模型，能够去除某些特征（权重为0则等效于去除）；L2正则化减少了权重的固定比例，使权重平滑，不会使权重变为0（不会产生稀疏模型），所以选择了更多的特征。

- L1减少的是一个常量，L2减少的是权重的固定比例。
- L1使权重稀疏，L2使权重平滑。
- L1优点是能够获得稀疏模型，对于large-scale的问题来说这一点很重要，因为可以减少存储空间。
- L2优点是实现简单，能够起到正则化的作用。缺点就是稀疏模型。

### KL散度

相对熵又称互熵，交叉熵，鉴别信息，Kullback熵，Kullback-Leible散度（即**KL散度**）等。

https://blog.csdn.net/matrix_space/article/details/80550561

https://blog.csdn.net/ACdreamers/article/details/44657745?spm=1001.2101.3001.6650.3&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-3-44657745-blog-80550561.pc_relevant_multi_platform_whitelistv2&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-3-44657745-blog-80550561.pc_relevant_multi_platform_whitelistv2&utm_relevant_index=6



## NLP相关

### Transformer

![Transformer](面试.assets/v2-47b6b5c4fadc1ef9e7cbbb88a04a293f_1440w.jpg)

#### 结构

Transformer本身还是一个典型的encoder-decoder模型，如果从模型层面来看，Transformer实际上就像一个seq2seq with attention的模型。

##### Encoder&Decoder

Encoder端由N（原论文中**N=6**）个相同的大模块堆叠而成，其中每个大模块又由**两个子模块**构成，这两个子模块为多头self-attention模块和前馈神经网络模块。

**需要注意的是，Encoder端每个大模块接收的输入是不一样的，第一个大模块（最底下的那个）接收的输入是输入序列的embedding（embedding可以通过word2vec预训练得来），其余大模块接收的是其前一个大模块的输出，最后一个模块的输出作为整个Encoder端的输出。**

Decoder端同样由N（原论文中**N=6**）个相同的大模块堆叠而成，其中每个大模块则由**三个子模块**构成，这三个子模块分别为多头self-attention模块，**多头Encoder-Decoder attention交互模块**，以及一个前馈神经网络模块。

**同样需要注意的是，Decoder端每个大模块接收的输入也是不一样的，其中第一个大模块（最底下的那个）训练时和测试时的接收的输入是不一样的，并且每次训练时接收的输入也可能是不一样的（也就是模型总览图示中的"shifted right"），其余大模块接收的是同样是其前一个大模块的输出，最后一个模块的输出作为整个Decoder端的输出。**

对于第一个大模块，简而言之，其训练及测试时接收的输入为：

- **训练的时候每次的输入为上次的输入加上输入序列向后移一位的ground truth（例如每向后移一位就是一个新的单词，那么则加上其对应的embedding），特别地，当Decoder的time step为1时（也就是第一次接收输入），其输入为一个特殊的token，可能是目标序列开始的token（\<BOS>），也可能是源序列结尾的token（如\<EOS>），也可能是其它视任务而定的输入等等，不同源码中可能有微小的差异，其目标则是预测下一个位置的单词（token）是什么。对应到time step为1时，则是预测目标序列的第一个单词（token）是什么，以此类推；**
  - **这里需要注意的是，在实际实现中可能不会这样每次动态的输入，而是一次性把目标序列的embedding通通输入第一个大模块中，然后在多头attention模块对序列进行mask即可**。
- 而在测试的时候，是**先生成第一个位置的输出，然后有了这个之后，第二次预测时，再将其加入输入序列，以此类推直至预测结束**。

##### Encoder子模块

###### 多头self-attention模块

![自注意力模块](面试.assets/v2-d7913da8a83d6ca9d67524560b263688_1440w.png)

上述attention可以被描述为**将query和key-value键值对的一组集合映射到输出**，其中query，keys，values和输出都是向量，其中 query和keys的维度均为$d_k$，values的维度为$d_v$（论文中$d_k=d_v=d_{model}/h=64$），输出被计算为values的加权和，其中分配给每个value的权重由query与对应key的相似性函数计算得来。这种attention的形式被称为“Scaled Dot-Product Attention”，对应到公式的形式为：
$$
Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V
$$
而多头self-attention模块，则是将$Q,K,V$通过参数矩阵映射后（给它们分别接一个全连接层），然后再做self-attention，将这个过程重复，最后再将所有的结果拼接起来，再送入一个全连接层即可：

![多头自注意力模块](面试.assets/v2-03207f9148878efb9ccceedf14264932_1440w.jpg)

对应到公式：
$$
MultiHead(Q,K,V)=Concat(head_1,...,head_h)W^O\\head_i=Attention(QW{^Q_i},KW{^K_i},VW{^V_i})
$$
其中
$$
W{^Q_i}∈\mathbb{R}^{d_{model}×d_k},W{^K_i}∈\mathbb{R}^{d_{model}×d_k},W{^V_i}∈\mathbb{R}^{d_{model}×d_v},W^O∈\mathbb{R}^{hd_v×d_{model}}
$$

###### 前馈神经网络模块

前馈神经网络模块（Feed Forward）由两个线性变换组成，中间有一个ReLU激活函数，对应到公式的形式为：
$$
FFN(x)=max(0,xW_1+b_1)W_2+b_2
$$
论文中前馈神经网络模块输入和输出的维度均为$d_{model}=512$，其内层维度$d_{ff}=2048$。

##### Decoder子模块

###### 多头self-attention模块

Decoder端多头self-attention模块与Encoder端的一致，但是**需要注意的是Decoder端的多头self-attention需要做mask，因为它在预测时，是“看不到未来的序列的”，所以要将当前预测的单词（token）及其之后的单词（token）全部mask掉。**

###### 多头Encoder-Decoder attention交互模块

多头Encoder-Decoder attention交互模块的形式与多头self-attention模块一致，**唯一不同的是其$Q,K,V$矩阵的来源**，其$Q$矩阵来源于下面子模块的输出（对应到图中即为masked多头self-attention模块经过Add&Norm后的输出），而$K,V$则来源于整个Encoder端的输出。**仔细想想其实可以发现，这里的交互模块就跟seq2seq with attention中的机制一样，目的就在于让Decoder端的单词（token）给予Encoder端对应的单词（token）**“更多的关注（attention weight）”。

###### 前馈神经网络模块

该部分与Encoder端的一致。

##### 其他模块

###### Add&Norm模块

Add&Norm模块接在Encoder端和Decoder端每个子模块的后面，其中Add表示残差连接，Norm表示LayerNorm，因此Encoder端和Decoder端每个子模块实际的输出为$LayerNorm(x+Sublayer(x))$，其中$Sublayer(x)$为子模块的输出。

###### Positional Encoding模块

Positional Encoding添加到Encoder端和Decoder端最底部的输入embedding。Positional Encoding具有与embedding相同的维度$d_{model}$，因此可以对两者进行求和。具体做法是使用不同频率的正弦和余弦函数，公式如下：
$$
PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})\\PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})
$$
其中$pos$为位置，$i$为维度。

需要注意的是，Transformer中的Positional Encoding不是通过网络学习得来的，而是直接通过上述公式计算而来的，论文中也实验了利用网络学习Positional Encoding，发现结果与上述基本一致，但是论文中选择了正弦和余弦函数版本，**因为三角公式不受序列长度的限制，也就是可以对“比所遇到序列的更长的序列”进行表示。**

#### 关于self-attention

##### 是什么？计算过程？

**self-attention也叫intra-attention**，是一种通过自身和自身相关联的attention机制，从而得到一个更好的representation来表达自身，self-attention可以看成一般attention的一种特殊情况。在self-attention中，$Q=K=V$，序列中的每个单词（token）和该序列中其余单词（token）进行attention计算。self-attention的特点在于**无视词（token）之间的距离直接计算依赖关系，从而能够学习到序列的内部结构**，实现起来也比较简单。
$$
Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V
$$

##### 为什么有如此大的作用？

self-attention是一种自身和自身相关联的attention机制，这样能够得到一个更好的representation来表达自身，在多数情况下，自然会对下游任务有一定的促进作用。Self Attention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短，有利于有效地利用这些特征。除此外，Self Attention对于增加计算的并行性也有直接帮助作用。但是Transformer效果显著及其强大的特征抽取能力是否完全归功于其self-attention模块，还是存在一定争议的。

##### 为什么要使用Q、K、V？

self-attention使用$Q,K,V$，这样三个参数独立，模型的表达能力和灵活性显然会比只用$Q$、$V$或者只用$V$要好些。**其实还有个小细节，因为self-attention的范围是包括自身的（masked self-attention也是一样），因此至少是要采用Q、V或者K、V的形式，而这样“询问式”的attention方式，个人感觉Q、K、V显然合理一些。**

#### 相比于RNN/LSTM有什么优势？

RNNs模型并行计算中$T$时刻的计算依赖于$T-1$时刻的隐层计算结果，会形成序列依赖关系，所以RNNs模型并行计算能力很差；Transformer的特征提取能力比RNNs要好。

#### 如何并行化？

Transformer的并行化主要体现在self-attention模块，在Encoder端Transformer可以并行处理整个序列，并得到整个输入序列经过Encoder端的输出，在self-attention模块，对于某个序列$x_1,...,x_n$，self-attention模块可以直接计算$x_i$和$x_j$的点乘结果，而RNNs就必须按序列顺序从$x_i$计算到$x_j$。

#### 归一化（缩放）的作用？

随着$d_k$的增大，$QK^T$的结果也会随之增大，这样会将softmax函数推入梯度非常小的区域，使得收敛困难（可能出现梯度消失的情况），因此为了抵消这种影响，将点积缩放$\frac{1}{\sqrt{d_k}}$。



### BERT