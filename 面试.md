# 面试

## PyTorch相关

### 并行训练

常用的API有：`torch.nn.DataParallel`(DP)、`torch.nn.DistributedDataParallel`(DDP)

DP的计算过程：

1. 将inputs从主GPU分发到所有GPU上；
2. 将model从主GPU分发到所有GPU上；
3. 每个GPU分别独立进行前向传播，得到outputs；
4. 将每个GPU的outputs发回主GPU；
5. 在主GPU上，通过loss function计算出loss，对loss function求导，求出损失梯度；
6. 计算得到的梯度分发到所有GPU上；
7. 反向传播计算参数梯度；
8. 将所有梯度回传到主GPU，通过梯度更新模型权重；
9. 不断重复上面的过程。

应用：

```python
torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)

net = torch.nn.DataParallel(model, device_ids=[0, 1, 2])
```

DDP的过程：
大体上的过程和DP类似，与DP的单进程控制多GPU不同，在distributed的帮助下只需要编写一份代码，torch就会自动将其分配给n个进程，分别在n个GPU上运行。

不再有主GPU，每个GPU执行相同的任务。对每个GPU的训练都是在自己的过程中进行的。每个进程都从磁盘加载其自己的数据。**分布式数据采样器可确保加载的数据在各个进程之间不重叠。损失函数的前向传播和计算在每个GPU上独立执行**。因此，不需要收集网络输出。在反向传播期间，梯度下降在所有GPU上均被执行，从而确保每个GPU在反向传播结束时最终得到平均梯度的相同副本。

DP优势：简单，一行代码。

DDP的优势：
1. 每个进程对应一个独立的训练过程，且只对梯度等少量数据进行信息交换。DDP在每次迭代中，每个进程具有自己的optimizer ，并独立完成所有的优化步骤，进程内与一般的训练无异。DDP中由于各进程中的模型，初始参数一致（初始时刻进行一次broadcast），而每次用于更新参数的梯度也一致，因此各进程的模型参数始终保持一致。相较于DP，DDP传输的数据量更少，因此速度更快，效率更高。

2. 每个进程包含独立的解释器和GIL。全局解释器锁（Global Interpreter Lock）是Python用于同步线程的工具，使得任何时刻仅有一个线程在执行。由于每个进程拥有独立的解释器和GIL，消除额外解释器开销和GIL-thrashing，因此可以减少解释器和GIL使用冲突。

### `model.train()`和`model.eval()`

前提：模型中有BN和Dropout。

`model.train()`：在训练时使用，表示开启训练模式，启用BN和Dropout。对于BN，保证BN能够用到每一批数据的均值和方差；对于Dropout，随机取一部分网络连接来训练更新参数。

`model.eval()`：在测试时使用，表示开启测试模式，不启用BN和Dropout。对于BN，保证BN能够用全部训练数据的均值和方差，即测试过程中BN的均值和方差不变；对于Dropout，利用到了所有网络连接，即不进行随机舍弃神经元。

**因此，在使用PyTorch进行训练和测试时一定注意要把实例化的model指定train/eval**。

### `DataLoader()`

```python
DataLoader(dataset, batch_size, shuffle=False, sampler=None, 
          batch_sample=None, num_workers=0, collate_fn=None, 
          pin_memory=False, drop_last=False, timeout=0,
          worker_init_fn=None)
```

- batch_size表示输入batch的大小；
- shuffle表示是否打乱输入；
- sampler是样本抽样；
- num_workers指的是开多少个worker来读数据，一个worker读一个batch的数据。

## ML、DL相关

### 归一化算法

Batch Normalization (BN)、Layer Normalization (LN)（包括Instance Normalization (IN)、Group Normalization (GN)）都属于归一化算法，它们的主要区别在于**操作的feature map维度不同**。

LN是“横”着来的，对一个样本、不同的神经元间做归一化；BN是“竖”着来的，各个维度做归一化，所以与batch size有关系。**二者提出的目的都是为了加快模型收敛，减少训练时间**。

CV和NLP任务的区别在于：图像数据是自然界客观存在的，像素的组织形式已经包含了“信息”；而自然语言相关的数据不一样，网络对其学习的真正开端是从embedding开始的，而这个embedding并不是客观存在，也是通过网络学习出来的。

LN是针对每一个样本，做特征的缩放。如果把BN应用于NLP任务，那就表示**默认了在同一个位置的单词对应的是同一种特征**，这样是违背直觉的。

## NLP相关

### Transformer





### BERT